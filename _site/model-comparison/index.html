<!DOCTYPE html>
<html lang="en"><head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-N0NYCTSJCT"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-N0NYCTSJCT');
</script>
          <link rel="icon" 
                type="image/png" 
                href="/docs/assets/images/dp_icon.png">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Model comparison | Data Perspectives</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Model comparison" />
<meta name="author" content="Stippe" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Understanding good and bad aspects of your models" />
<meta property="og:description" content="Understanding good and bad aspects of your models" />
<link rel="canonical" href="http://localhost:4000/model-comparison/" />
<meta property="og:url" content="http://localhost:4000/model-comparison/" />
<meta property="og:site_name" content="Data Perspectives" />
<meta property="og:image" content="http://localhost:4000/docs/assets/images/model_comparison/loo.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-08-26T00:00:00+02:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/docs/assets/images/model_comparison/loo.jpg" />
<meta property="twitter:title" content="Model comparison" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Stippe"},"dateModified":"2023-08-26T00:00:00+02:00","datePublished":"2023-08-26T00:00:00+02:00","description":"Understanding good and bad aspects of your models","headline":"Model comparison","image":"http://localhost:4000/docs/assets/images/model_comparison/loo.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/model-comparison/"},"url":"http://localhost:4000/model-comparison/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Perspectives" />

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
</head>
<body><header class="site-header">

<div>
  <div class="wrapper" style="display:flex;"><a href="/"><img class="site-masthead" src="/docs/assets/images/logo_dp.png" alt="Data Perspectives" /></a><nav class="site-nav" style="display:flex;">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about">About me</a><a class="page-link" href="/index">Home</a><a class="page-link" href="/notes">List of the notes</a><a class="page-link" href="/links">Some useful resources</a></div>
      </nav></div>
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Model comparison</h1>
    <p class="post-meta"><time class="dt-published" datetime="2023-08-26T00:00:00+02:00" itemprop="datePublished">
        Aug 26, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In the <a href="/predictive-checks/">last</a> post we looked at how one can assess a modelâ€™s ability to reproduce the data.
In this post we will look at a related topic, which is how we can compare two or more Bayesian models.
In fact, you rarely know from the beginning what is the most appropriate model to fit your data.
Most of the times you will find yourself building different models for the same dataset,
and a crucial part of your work will be to compare them.
Comparing model sometimes may be understood as choosing the best model,
but in most cases it means to asses which model is better to describe or predict some particular aspect of your data.
Model comparison can be done analytically in some case,
but most of the time it will be done numerically or graphically, and here we will give an overview of the most important tools.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="nf">use</span><span class="p">(</span><span class="sh">"</span><span class="s">seaborn-v0_8-darkgrid</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="bayes-factors">Bayes factors</h2>

<p>Let us consider again our first example, where we had a sample of 79 yeast cells and we counted 70 alive cells and 9 death cells.
let us assume that we have two candidate models to describe our data:
model 1 has uniform prior, which mean that the prior is a beta distribution with $a=1$ and $b=1\,,$
while the second one has $a=b=10\,.$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n</span> <span class="o">=</span> <span class="mi">79</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">70</span>

<span class="n">model_1</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
<span class="n">model_2</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">:</span> <span class="mi">10</span><span class="p">}</span>

<span class="n">x_pl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">([</span><span class="n">model_1</span><span class="p">,</span> <span class="n">model_2</span><span class="p">]):</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="nf">beta</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">model</span><span class="p">[</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">],</span> <span class="n">b</span><span class="o">=</span><span class="n">model</span><span class="p">[</span><span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">]).</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x_pl</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">model </span><span class="si">{</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="n">legend</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">$\theta$</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">$p(\theta)$  </span><span class="sh">"</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/model_comparison/priors.jpg" alt="Priors" /></p>

<p>Given the two models $M_1$ and $M_2$ we may ask which one we prefer, given the data. The probability of the model given the data is given by</p>

\[p(M_k | y) = \frac{p(y | M_k)}{p(y)} p(M_k)\]

<p>where the quantity $p(y | M_k)$ is the <strong>marginal likelihood</strong> of the model. If we assign the same prior probability $p(M_k)$ to each model then we can simply replace $p(M_k | y)$ with the
marginal likelihood.</p>

<p>As usual, an analytic calculation is only possible in a very limited number of models.</p>

<p>One may think to compute $p(M_k| y)$ by starting from $p(y | \theta, M_k)$ and integrating out $\theta$ but doing this naively is generally not a good idea, as
this method is unstable and prone to numerical errors.</p>

<p>However can use the Sequential Monte Carlo to compare the two models, since it allows to estimate the (log) marginal likelihood of the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">traces</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="p">[</span><span class="n">model_1</span><span class="p">,</span> <span class="n">model_2</span><span class="p">]:</span>
    <span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Beta</span><span class="p">(</span><span class="sh">"</span><span class="s">theta</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">m</span><span class="p">[</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">],</span> <span class="n">beta</span><span class="o">=</span><span class="n">m</span><span class="p">[</span><span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">])</span>
        <span class="n">yl</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">(</span><span class="sh">"</span><span class="s">yl</span><span class="sh">"</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
        <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_smc</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">return_inferencedata</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>
        <span class="n">models</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">traces</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/model_comparison/trace_0.jpg" alt="First trace" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/model_comparison/trace_1.jpg" alt="Second trace" /></p>

<p>What one usually computes is the <strong>Bayes factor</strong> of the models, which is the ratio between the posterior probability of the model (which in this case is simply the
ratio between the marginal likelihoods).</p>

<table>
  <thead>
    <tr>
      <th>$BF = p(M_1 \vert y)/p(M_2\vert y)$</th>
      <th>interpretation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$BF&lt;10^{0}$</td>
      <td>support to $M_2$ (see reciprocal)</td>
    </tr>
    <tr>
      <td>$10^{0}\leq BF&lt;10^{1/2}$</td>
      <td>Barely worth mentioning support to $M_1$</td>
    </tr>
    <tr>
      <td>$10^{1/2}\leq BF&lt;10^2$</td>
      <td>Substantial support to $M_1$</td>
    </tr>
    <tr>
      <td>$10^{2} \leq BF&lt;10^{3/2}$</td>
      <td>Strong support to $M_1$</td>
    </tr>
    <tr>
      <td>$10^{3/2} \leq BF&lt;10^2$</td>
      <td>Very strong support to $M_1$</td>
    </tr>
    <tr>
      <td>$\geq 10^2$</td>
      <td>Decisive support to $M_1$</td>
    </tr>
  </tbody>
</table>

<p>We can now compute the Bayes factor as follows</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">BF_smc</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">sample_stats</span><span class="p">.</span><span class="n">log_marginal_likelihood</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">sample_stats</span><span class="p">.</span><span class="n">log_marginal_likelihood</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span><span class="p">))</span>
<span class="n">np</span><span class="p">.</span><span class="nf">log10</span><span class="p">(</span><span class="n">BF_smc</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>2.0487805236</p>
</blockquote>

<p>The Bayes factor is above 100, so we have a strong support for model 0.</p>

<p>We can better understand this result if we compare our estimate with the frequentist one, recalling that the confidence interval was $[0.81, 0.96]$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_forest</span><span class="p">(</span><span class="n">traces</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">rope</span><span class="o">=</span><span class="p">[</span><span class="mf">0.81</span><span class="p">,</span> <span class="mf">0.96</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/model_comparison/forest.jpg" alt="Forest plot" /></p>

<p>As we can see, our first model gives an estimate which is compatible
with the frequentist one, while the second HDI is not compatible
with the frequentist estimate.
We also have that the posterior predictive distribution of the first model is much
closer to the observed data than the one of the second model:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ppc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">models</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">m</span><span class="p">:</span>
        <span class="n">ppc</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ppc</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">yl</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">([</span><span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">]).</span><span class="n">values</span>
</code></pre></div></div>
<blockquote>
  <p>array(69.15525)</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ppc</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">yl</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">([</span><span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">]).</span><span class="n">values</span>
</code></pre></div></div>
<blockquote>
  <p>array(63.75)</p>
</blockquote>

<p>The first model predicts 69 alive cells, while the second one predicts 63.
So the first one is much closer to the observed number, which is 70.</p>

<h2 id="leave-one-out-cross-validation">Leave One Out cross-validation</h2>

<p>In the past, Bayes factor analysis was the most common method to perform
model selection.
However, according to many modern Bayesian statisticians, 
it should not be used for this purpose <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.
The main criticism to this method is that you are both using
your data to fit the data and to check your model.
A better alternative is provided by the Leave One Out (LOO)
cross-validation.
LOO cross validation consists into using some metrics to
assess the probability of a datum where that datum is not uses
to fit the model.
There are many metrics that can be used,
and the most common ones are Aikane Information Criteria, Bayesian Information Criteria
(AIC and BIC respectively).
They are respectively given, for a model with $k$ parameters fitted by using $n$
points, as</p>

\[AIC = 2k - 2 \log \hat{L}\]

\[BIC = k\log(n) - 2 \log\hat{L}\]

<p>where $\hat{L}$ is the maximized value of the likelihood function.
However, none of them is truly Bayesian, as they are defined
using the maximum value of the likelihood function, while a more consistent
approach would use the average of the likelihood function <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.
Arviz uses the Pareto Smoothed Importance Sampling (PSIS)
to estimate the LOO-Watanabe Aikane Information Criteria (WAIC), which is
the Bayesian version of the AIC.</p>

<p>Let us go back to the hurricanes dataset, and compare the following
models:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">df_hurricanes</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">data/frequency-north-atlantic-hurricanes.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">y_obs</span> <span class="o">=</span> <span class="n">df_hurricanes</span><span class="p">[</span><span class="sh">"</span><span class="s">Number of US Hurricanes (HUDRAT, NOAA)</span><span class="sh">"</span><span class="p">].</span><span class="nf">dropna</span><span class="p">().</span><span class="n">values</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_a</span><span class="p">:</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Gamma</span><span class="p">(</span><span class="sh">'</span><span class="s">mu</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Poisson</span><span class="p">(</span><span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_obs</span><span class="p">)</span>
    <span class="n">trace_a</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">return_inferencedata</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">idata_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">log_likelihood</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                       <span class="n">random_seed</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_b</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Gamma</span><span class="p">(</span><span class="sh">'</span><span class="s">mu</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Exponential</span><span class="p">(</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">NegativeBinomial</span><span class="p">(</span><span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_obs</span><span class="p">)</span>
    <span class="n">trace_b</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">return_inferencedata</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">idata_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">log_likelihood</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                       <span class="n">random_seed</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_a</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/model_comparison/trace_hurricanes_a.jpg" alt="Trace hurricanes A" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_b</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/model_comparison/trace_hurricanes_b.jpg" alt="Trace hurricanes B" /></p>

<p>We can compute the LOO-WAIC as</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loo_a</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="nf">loo</span><span class="p">(</span><span class="n">trace_a</span><span class="p">,</span> <span class="n">model_a</span><span class="p">)</span>
<span class="n">loo_b</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="nf">loo</span><span class="p">(</span><span class="n">trace_b</span><span class="p">,</span> <span class="n">model_b</span><span class="p">)</span>

<span class="n">model_compare</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="nf">compare</span><span class="p">({</span><span class="sh">'</span><span class="s">Model a</span><span class="sh">'</span><span class="p">:</span> <span class="n">loo_a</span><span class="p">,</span> <span class="sh">'</span><span class="s">Model b</span><span class="sh">'</span><span class="p">:</span> <span class="n">loo_b</span><span class="p">})</span>
<span class="n">az</span><span class="p">.</span><span class="nf">plot_compare</span><span class="p">(</span><span class="n">model_compare</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/model_comparison/loo.jpg" alt="LOO Plot" /></p>

<p>Model $a$ is slightly preferred to model $a\,,$ as it is more accurate in reproducing
the data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ppc_a</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_a</span><span class="p">,</span> <span class="n">model_a</span><span class="p">)</span>
<span class="n">ppc_b</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_b</span><span class="p">,</span> <span class="n">model_b</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="nf">plot_ppc</span><span class="p">(</span><span class="n">ppc_a</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">13</span><span class="p">)</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="nf">plot_ppc</span><span class="p">(</span><span class="n">ppc_b</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax1</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">13</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/model_comparison/ppc_hurricanes.jpg" alt="PPC Hurricanes" /></p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>See <a href="https://statmodeling.stat.columbia.edu/2019/09/10/i-hate-bayes-factors-when-theyre-used-for-null-hypothesis-significance-testing/">here</a> or <a href="https://vasishth.github.io/bayescogsci/book/ch-bf.html">here</a> and references therein.Â <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>More precisely, they can be only consistently used with regular models, which are models where the posterior distribution can be asymptotically approximated with a normal distribution. See <a href="https://www.jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf">Watanabe</a> for an in-depth discussion.Â <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/model-comparison/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Stippe</li>
          <li><a class="u-email" href="mailto:smaurizio87@protonmail.com">smaurizio87@protonmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://twitter.com/SteffPy" target="_blank" title="twitter">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://stackoverflow.com/users/11065831/stefano" target="_blank" title="stackoverflow">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#stackoverflow"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://github.com/thestippe/" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

</html>

