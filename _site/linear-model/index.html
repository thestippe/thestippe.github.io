<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>The linear model | Just another Bayesian enthusiast</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="The linear model" />
<meta name="author" content="Stippe" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The simplest regression problem" />
<meta property="og:description" content="The simplest regression problem" />
<link rel="canonical" href="http://localhost:4000/linear-model/" />
<meta property="og:url" content="http://localhost:4000/linear-model/" />
<meta property="og:site_name" content="Just another Bayesian enthusiast" />
<meta property="og:image" content="http://localhost:4000/docs/assets/images/linear_model/lung_data.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-08-28T00:00:00+02:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/docs/assets/images/linear_model/lung_data.jpg" />
<meta property="twitter:title" content="The linear model" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Stippe"},"dateModified":"2023-08-28T00:00:00+02:00","datePublished":"2023-08-28T00:00:00+02:00","description":"The simplest regression problem","headline":"The linear model","image":"http://localhost:4000/docs/assets/images/linear_model/lung_data.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/linear-model/"},"url":"http://localhost:4000/linear-model/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Just another Bayesian enthusiast" />

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Just another Bayesian enthusiast</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about">About me</a><a class="page-link" href="/index">Home</a><a class="page-link" href="/notes">List of the notes</a><a class="page-link" href="/links">Some useful resources</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">The linear model</h1>
    <p class="post-meta"><time class="dt-published" datetime="2023-08-28T00:00:00+02:00" itemprop="datePublished">
        Aug 28, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In this post we will start looking at some regression problem,
 which are models where we want to relate the behavior of the outcome
 variable $y$ to some other variable $x$ which we do not want to model.
In particular, we will look the most fundamental regression model, the linear model.
This model is so widespread that entire statistical textbooks
and academic courses has been devoted to it, and it is crucial to fully
understand both how to assess and give a correct interpretation of the uncertainties
in this model and how to report these uncertainties no non-statisticians.
Of course we will just give few examples, without any claim of completeness.
For a full immersion in this model from a frequentist
perspective I reccomend the Weisberg textbook
“Applied linear regression”, freely available
<a href="https://www.stat.purdue.edu/~qfsong/teaching/525/book/Weisberg-Applied-Linear-Regression-Wiley.pdf">here</a>.</p>

<h2 id="normal-linear-regression">Normal linear regression</h2>

<p>Consider the <a href="https://search.r-project.org/CRAN/refmans/mplot/html/fev.html">following dataset</a>,
describing the lung capacity of a set of 654 young patients with age ranging from 3 to 19, 
recorded in a series of measures performed in the 1970s.
This dataset was used in <a href="https://pubmed.ncbi.nlm.nih.gov/463860/">this</a> article to investigate the effect of having a smoking
parent on the respiratory capacity of the children.
The most relevant covariate here is the age, but there are also other possible relevant quantities, and we will consider them later.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">pymc.sampling_jax</span> <span class="k">as</span> <span class="n">pmjax</span>


<span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="nf">use</span><span class="p">(</span><span class="sh">"</span><span class="s">seaborn-v0_8-darkgrid</span><span class="sh">"</span><span class="p">)</span>

<span class="n">df_lungs</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/tkseneee/Dataset/master/LungCapdata.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">df_lungs</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">Age</th>
      <th style="text-align: right">Height</th>
      <th style="text-align: right">Gender</th>
      <th style="text-align: right">Smoke</th>
      <th style="text-align: right">FEV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">9</td>
      <td style="text-align: right">57</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">1.708</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: right">8</td>
      <td style="text-align: right">67.5</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">1.724</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">7</td>
      <td style="text-align: right">54.5</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">1.72</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">9</td>
      <td style="text-align: right">53</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">1.558</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">9</td>
      <td style="text-align: right">57</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">1.895</td>
    </tr>
  </tbody>
</table>

<p>Here FEV means Forced Expiratory Volume, and roughly measures how many liters of air a person can exhale in the first second of forced breath.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="p">.</span><span class="nf">pairplot</span><span class="p">(</span><span class="n">df_lungs</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/linear_model/lung_pairplot.jpg" alt="Lung pairplot" /></p>

<p>As we could imagine, the FEV depends on the age.
While the age seems almost normally distributed, the FEV is not,
and as well the FEV variance grows with the age.
The distribution of the FEV seems definitely different between
the smoke=0 and the smoke=1 patients, so we should also take this into account.</p>

<p>Let us give a closer look to the data we are going to fit:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_0</span> <span class="o">=</span> <span class="n">df_lungs</span><span class="p">[</span><span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">df_lungs</span><span class="p">[</span><span class="sh">'</span><span class="s">FEV</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">AGE [Y]</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">FAV [L]</span><span class="sh">'</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/linear_model/lung_data.jpg" alt="Lung data" /></p>

<p>The relation between the age and the FEV looks linear, 
so let us try and fit the data with the linear model,
where we assume a linear relation between the FEV and the age.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">linear_model_0</span><span class="p">:</span>
    <span class="n">theta_0</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">theta_0</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">theta_1</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">theta_1</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfCauchy</span><span class="p">(</span><span class="sh">'</span><span class="s">sigma</span><span class="sh">'</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">theta_0</span> <span class="o">+</span> <span class="n">theta_1</span><span class="o">*</span><span class="n">x_0</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
                  <span class="n">observed</span><span class="o">=</span><span class="n">df_lungs</span><span class="p">[</span><span class="sh">'</span><span class="s">FEV</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div>

<p>Since we do not want to use our priors to constrain too much our model, we used uninformative priors for all the parameters.
Let us verify that our prior guess includes the data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">linear_model_0</span><span class="p">:</span>
    <span class="n">lm_prior_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_prior_predictive</span><span class="p">()</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">lm_prior_pred</span><span class="p">.</span><span class="n">prior_predictive</span><span class="p">.</span><span class="n">y</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">t</span><span class="p">,:],</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">+</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">navy</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">df_lungs</span><span class="p">[</span><span class="sh">'</span><span class="s">FEV</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">AGE [Y]</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">FAV  [L]</span><span class="sh">'</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/linear_model/lung_prior_pred.jpg" alt="Lung prior predictive" /></p>

<p>There is not any observed point which is not covered by the prior predictive, so we can be confident that
our prior are generous enough to reproduce the observed data.
Now that we ensured that our prior guess looks OK, we can proceed with the next step
and perform the sampling of the posterior distribution.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">linear_model_0</span><span class="p">:</span>
    <span class="n">trace_lm0</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">return_inferencedata</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>
</code></pre></div></div>

<p>Let us now check if we can spot any problem in the sampling procedure:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_lm0</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/linear_model/lung_trace.jpg" alt="Lung trace" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_autocorr</span><span class="p">(</span><span class="n">trace_lm0</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/linear_model/lung_corrplot.jpg" alt="Lung corrplot" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_rang</span><span class="p">(</span><span class="n">trace_lm0</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/linear_model/lung_rank.jpg" alt="Lung rank" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">summary</span><span class="p">(</span><span class="n">trace_lm0</span><span class="p">)</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right">mean</th>
      <th style="text-align: right">sd</th>
      <th style="text-align: right">hdi_3%</th>
      <th style="text-align: right">hdi_97%</th>
      <th style="text-align: right">mcse_mean</th>
      <th style="text-align: right">mcse_sd</th>
      <th style="text-align: right">ess_bulk</th>
      <th style="text-align: right">ess_tail</th>
      <th style="text-align: right">r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">theta_0</td>
      <td style="text-align: right">0.428</td>
      <td style="text-align: right">0.078</td>
      <td style="text-align: right">0.285</td>
      <td style="text-align: right">0.574</td>
      <td style="text-align: right">0.001</td>
      <td style="text-align: right">0.001</td>
      <td style="text-align: right">2815</td>
      <td style="text-align: right">3553</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: left">theta_1</td>
      <td style="text-align: right">0.222</td>
      <td style="text-align: right">0.007</td>
      <td style="text-align: right">0.208</td>
      <td style="text-align: right">0.236</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">2808</td>
      <td style="text-align: right">3506</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: left">sigma</td>
      <td style="text-align: right">0.568</td>
      <td style="text-align: right">0.016</td>
      <td style="text-align: right">0.54</td>
      <td style="text-align: right">0.6</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">4048</td>
      <td style="text-align: right">3554</td>
      <td style="text-align: right">1</td>
    </tr>
  </tbody>
</table>

<p>So far so good:</p>
<ul>
  <li>The traces look fine</li>
  <li>The correlation goes to 0 after few iterations for all the variables</li>
  <li>The rank plots look almost uniform</li>
  <li>r_hat is 1 and the ESS are large enough.</li>
</ul>

<p>Of course, we don’t expect out model to be able to exactly reproduce
all the relevant features of the FEV plot, but let us check how far away is it.
In order to do this, we will take 20 random samples and compare them with the true sample:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pp_lm0</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">linear_model_0</span><span class="p">,</span> <span class="n">trace</span><span class="o">=</span><span class="n">trace_lm0</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">pp_lm0</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">.</span><span class="n">y</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">s</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">+</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">navy</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">df_lungs</span><span class="p">[</span><span class="sh">'</span><span class="s">FEV</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">AGE [Y]</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">FAV [L]</span><span class="sh">'</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/linear_model/lung_ppc.jpg" alt="Lung PPC" /></p>

<p>Our model overestimates the uncertainties for lower age values, up to 10 years or so, but apparently it catches all the other relevant features of the sample.
When we will discuss about causal inference we will see how to deal with data with
non-constant variance [^1], as it happens in the previous plot.</p>

<h3 id="interpretation">Interpretation</h3>

<p>We have three parameters in the model:</p>
<ul>
  <li>$\theta_0\,,$ which represents how our model would predict the average FEV of a newborn child</li>
  <li>$\theta_1\,,$ which represents the average slope of the FEV. Alternatively, we can think about this parameter as how does the average FEV changes when we increment the age by one.</li>
  <li>$\sigma\,,$ which is the variance of the FEV. Notice that this is assumed to be independent on the age.</li>
</ul>

<p>In our dataset there are no points for 0 years old children, so you could ask yourself if you are allowed to claim that $\theta_0$ is actually an estimate
for the FEV of a newborn child. There are many risks in doing so: does the current knowledge regarding the lung growth in babies allow you to assume that
the FEV can be extrapolated in a linear way? You should only extrapolate if your model is robust enough.
You can easily convince yourself about this by trying to extrapolate in the other direction. Since, in the linear model, the intercept $\theta_0$ becomes always less relevant
as $x$ grows, we may roughly say that the average of a 20 years old person is twice of the FEV of a 10 years old person, and this makes sense,
as our measures say that the average FEV for a 10 years old children is around 2.5, while the one of a 20 years old person is roughly 5.
But if we iterate this reasoning we would say that the average FEV of a 40 years old person is twice of the FEV of a 20 years old person,
and the FEV of an 80 years old person is four times the FEV of a 20 years old person. Of course, this simply sounds crazy, since 
we expect that elderly people will have a lower FEV than younger adults.</p>

<h2 id="robust-linear-regression">Robust linear regression</h2>

<p>In some case your data may be not good enough to provide you reliable estimates with normal linear regression,
and this is the case of the conclusions drawn from
<a href="https://www.cambridge.org/core/journals/american-political-science-review/article/abs/political-institutions-and-voter-turnout-in-the-industrial-democracies/D6725BBF93F2F90F03A69B0794728BF7">this</a> article,
where the author concludes that there is a significant correlation between the voter turnout in a country and its average income inequality.
This example is a classical example of misleading result of a regression, where the author does not provide a plot of the data,
taken from <a href="https://www.google.it/books/edition/Data_Visualization/3XOYDwAAQBAJ?hl=it&amp;gbpv=1&amp;dq=Data+visualization,+a+practical+introduction&amp;printsec=frontcover">Healy, “Data visualization, a practical introduction”</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_turnout</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">data/inequality.csv</span><span class="sh">'</span><span class="p">)</span>
<span class="n">df_turnout</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">turnout</th>
      <th style="text-align: right">inequality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0.85822</td>
      <td style="text-align: right">1.95745</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: right">0.837104</td>
      <td style="text-align: right">1.95745</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">0.822021</td>
      <td style="text-align: right">2.41135</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">0.87632</td>
      <td style="text-align: right">2.76596</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">0.901961</td>
      <td style="text-align: right">2.95035</td>
    </tr>
  </tbody>
</table>

<p>In this dataset we have on the percentage turnout against the average income inequality.
How can we decide, from a Bayesian perspective, if the conclusion hold? As already pointed
out, we cannot rely on statistical significance.
For this kind of problem we can use the ROPE, which is the Region Of Practical Equivalence:
before looking at the data, we should decide a region such that, if the HDI of a certain parameter
is included inside the region, we will conclude that the parameter is negligible. 
In our model we will be interested with the slope of the fitted model.
We decide that, if the HDI is included between $[-5, 5]$, then the slope is compatible with
0, so a change in the turnout does not lead to a large enough change into average the inequality income.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">pairplot</span><span class="p">(</span><span class="n">df_turnout</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/linear_model/inequality_pairplot.jpg" alt="Turnout pairplot" /></p>

<p>By simply plotting the data we can clearly see that there is one point, the South Africa, which is far away from the other, and this may have a huge impact on the fit.
Let us see this, and how one may avoid this kind of error.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_norm</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">'</span><span class="s">tau</span><span class="sh">'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">turnout</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">inequality</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>
    <span class="n">trace_norm</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">idata_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">log_likelihood</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>
<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_norm</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/linear_model/trace_norm.jpg" alt="Traceplot normal" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_plt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>
<span class="k">with</span> <span class="n">model_norm</span><span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">x_plt</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>
    <span class="n">ppc_norm</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_norm</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">],</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">ppc_norm</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">ppc_norm</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]),</span> <span class="n">ppc_norm</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">grey</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">turnout</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">inequality</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/linear_model/inequality_norm_ppc.jpg" alt="PPC normal" /></p>

<p>The error bands include all the points and it looks like it correctly reproduces the data. 
Does this model support the conclusions of the cited article?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_forest</span><span class="p">(</span><span class="n">trace_norm</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">],</span> <span class="n">rope</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/linear_model/inequality_forest_normal.jpg" alt="PPC normal" /></p>

<p>Yes, this model drives us to the same conclusions of the above cited article.
Let us now try with a more robust model, using both for the prior and
for the likelihood distribution with heavier tails than the normal distribution.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_robust</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Cauchy</span><span class="p">(</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Cauchy</span><span class="p">(</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfCauchy</span><span class="p">(</span><span class="sh">'</span><span class="s">tau</span><span class="sh">'</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Exponential</span><span class="p">(</span><span class="sh">'</span><span class="s">nu</span><span class="sh">'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">StudentT</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">turnout</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">inequality</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">tau</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    <span class="n">trace_robust</span> <span class="o">=</span> <span class="n">pmjax</span><span class="p">.</span><span class="nf">sample_numpyro_nuts</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">idata_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">log_likelihood</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span>
    <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_robust</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/linear_model/inequality_trace_robust.jpg" alt="Traceplot robust" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model_robust</span><span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">StudentT</span><span class="p">(</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">x_plt</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">tau</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    <span class="n">ppc_robust</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_robust</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">],</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>


<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">ppc_robust</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">median</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">ppc_robust</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]),</span> <span class="n">ppc_robust</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">grey</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">turnout</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">inequality</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/linear_model/inequality_ppc_robust.jpg" alt="PPC robust" /></p>

<p>Also in this case the model correctly reproduces the data, but the points now are located far away from the limits of the error bands
and the slope significantly decreased.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_forest</span><span class="p">(</span><span class="n">trace_robust</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">],</span> <span class="n">rope</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/linear_model/inequality_forest_robust.jpg" alt="Forest robust" /></p>

<p>In this case the ROPE is only partially compatible with the HDI, so we cannot draw any conclusion.</p>

<p>Let us now check which model is the best one. We will do this by using the “LOO - Leave One Out” metric (see the model evaluation post).
The LOO function returns many results, included the Pareto shape values for each observation.
The Pareto index ranges from $-\infty$ to $\infty$ and the bigger it is, the less likely the observation is.
One usually takes all the observations above $0.7$ as bad observations, and the ones above $1$ are considered as extremely bad,
while the ones below $0.5$ are good.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loo_normal</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="nf">loo</span><span class="p">(</span><span class="n">trace_norm</span><span class="p">,</span> <span class="n">model_norm</span><span class="p">)</span>
<span class="n">loo_robust</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="nf">loo</span><span class="p">(</span><span class="n">trace_robust</span><span class="p">,</span> <span class="n">model_robust</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loo_normal</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Pareto k</th>
      <th>Meaning</th>
      <th>Count</th>
      <th>Pct</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>(-Inf, 0.5]</td>
      <td>Good</td>
      <td>16</td>
      <td>88.9</td>
    </tr>
    <tr>
      <td>(0.5, 0.7]</td>
      <td>Ok</td>
      <td>1</td>
      <td>5.6</td>
    </tr>
    <tr>
      <td>(0.7, 1.0]</td>
      <td>Bad</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>(1, Inf)</td>
      <td>Very bad</td>
      <td>1</td>
      <td>5.6</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loo_robust</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Pareto k</th>
      <th>Meaning</th>
      <th>Count</th>
      <th>Pct</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>(-Inf, 0.5]</td>
      <td>Good</td>
      <td>17</td>
      <td>94.4</td>
    </tr>
    <tr>
      <td>(0.5, 0.7]</td>
      <td>Ok</td>
      <td>1</td>
      <td>5.6</td>
    </tr>
    <tr>
      <td>(0.7, 1.0]</td>
      <td>Bad</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>(1, Inf)</td>
      <td>Very bad</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p>If we recall what we wrote about the LOO diagnostic and we observe the summary of the normal model,
we have that there is one point in the dataset such that,
when removed from the fit, becomes highly unlikely.
On the opposite, as expected, this does not happens for the robust model, as the Student-t distribution
can easily accommodate more extreme values without being affected as much as the normal model.</p>

<p>Let us see what does the model comparison tell us.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">compare</span><span class="p">({</span><span class="sh">'</span><span class="s">Normal model</span><span class="sh">'</span><span class="p">:</span> <span class="n">trace_norm</span><span class="p">,</span> <span class="sh">'</span><span class="s">Robust model</span><span class="sh">'</span><span class="p">:</span> <span class="n">trace_robust</span><span class="p">})</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right">rank</th>
      <th style="text-align: right">elpd_loo</th>
      <th style="text-align: right">p_loo</th>
      <th style="text-align: right">elpd_diff</th>
      <th style="text-align: right">weight</th>
      <th style="text-align: right">se</th>
      <th style="text-align: right">dse</th>
      <th style="text-align: left">warning</th>
      <th style="text-align: left">scale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Normal model</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">-30.8054</td>
      <td style="text-align: right">4.73068</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0.920135</td>
      <td style="text-align: right">4.28969</td>
      <td style="text-align: right">0</td>
      <td style="text-align: left">True</td>
      <td style="text-align: left">log</td>
    </tr>
    <tr>
      <td style="text-align: left">Robust model</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">-32.7588</td>
      <td style="text-align: right">7.15545</td>
      <td style="text-align: right">1.95346</td>
      <td style="text-align: right">0.0798655</td>
      <td style="text-align: right">4.63041</td>
      <td style="text-align: right">2.12229</td>
      <td style="text-align: left">False</td>
      <td style="text-align: left">log</td>
    </tr>
  </tbody>
</table>

<p>Both our models are able to correctly reproduce the data, but there is a strong penalty for the robust model for the extra parameter.
Moreover, the comparison shows a warning, which may be due to the presence of an outlier.
At this point a careful researcher would try and remove the problematic observation and see what does it happen to the estimates of each model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_norm_red</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">'</span><span class="s">tau</span><span class="sh">'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">turnout</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">[:</span><span class="mi">17</span><span class="p">]</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">inequality</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">[:</span><span class="mi">17</span><span class="p">],</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>
    <span class="n">trace_norm_red</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">idata_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">log_likelihood</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_norm_red</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/linear_model/inequality_trace_normal_red.jpg" alt="Trace normal reduced model" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model_norm_red</span><span class="p">:</span>
    <span class="n">y_pred_red</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">x_plt</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>
    <span class="n">ppc_norm_red</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_norm_red</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">],</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">ppc_norm_red</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">ppc_norm_red</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]),</span> <span class="n">ppc_norm_red</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">grey</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">turnout</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">inequality</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/linear_model/inequality_ppc_normal_red.jpg" alt="PPC normal reduced model" /></p>

<p>By explicitly removing the South Africa point, the fit changes in a dramatic way, as beta becomes compatible with zero and the South Africa is no more 
included into the 95% error bands.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_forest</span><span class="p">(</span><span class="n">trace_norm_red</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">],</span> <span class="n">rope</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/linear_model/inequality_forest_normal_red.jpg" alt="Forest normal red" /></p>

<p>This model explicitly contradicts the first model, and tells us that there when you excludes the South Africa from the dataset
you won’s see any association between turnout and average income inequality.
By seeing this result, one should investigate why the South Africa has a behavior which is so different from the one of the other
countries, and only after a sensible answer to this question one should decide if he wants to include this
point inside the dataset.</p>

  </div><a class="u-url" href="/linear-model/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Stippe</li>
          <li><a class="u-email" href="mailto:smaurizio87@protonmail.com">smaurizio87@protonmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://twitter.com/SteffPy" target="_blank" title="twitter">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://stackoverflow.com/users/11065831/stefano" target="_blank" title="stackoverflow">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#stackoverflow"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://github.com/thestippe/" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

</html>

