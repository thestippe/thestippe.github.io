<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-06-04T18:33:59+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Data Perspectives</title><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><entry><title type="html">Splines regression</title><link href="http://localhost:4000/statistics/splines" rel="alternate" type="text/html" title="Splines regression" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>http://localhost:4000/statistics/splines</id><content type="html" xml:base="http://localhost:4000/statistics/splines"><![CDATA[<p>A common issue that any data scientist faced at some point
is how to include non-linearities into regression.
It is well known that higher order polynomials tend to overfit,
and it is therefore generally not a good idea to use this kind of
model.</p>

<p>A very flexible solution is given by the splines, which are
piecewise smooth functions.
There are many kind of splines, and recently 
<a href="https://en.wikipedia.org/wiki/B-spline">B-splines</a> gained
a lot of attention since they are very easy to implement, and they
are numerically stable.
Here we will use them to fit the “Six cities study”
of <a href="https://content.sph.harvard.edu/fitzmaur/ala2e/">this link</a>,
from the “Applied Longitudinal Analysis” textbook by Fitzmaurice, Laird
and Ware.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_stata</span><span class="p">(</span><span class="s">'fev1.dta'</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s">'id'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'id'</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'age'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'logfev1'</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/splines/data.webp" alt="Our dataset" /></p>

<p>We will try and model the relation between the Forced Expiratory Volume
(FEV) and the age.
The relation seems linear up to roughly 16 years, while above this point
it looks like the FEV saturates.
This is quite clear, since at some point the breath capacity must
saturate, since our growth stops.
Let us now implement the B-splines according to Wikipedia’s recursive algorithm</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">bspline</span><span class="p">(</span><span class="n">t</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span><span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="s">"""
    Returns a B-spline, defined as https://en.wikipedia.org/wiki/B-spline.

    Parameters:
    -----------
    t: np.array
    x: np.array
    i: int
    p: int

    Returns:
    np.array
    
    Raises:
    ------
    ValueError
       if i is not an integer between 0 and len(x)-p-1 (both included)
    """</span>
    <span class="k">if</span> <span class="n">i</span><span class="o">&gt;=</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="n">p</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">heaviside</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">heaviside</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">fac0</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">p</span><span class="p">]</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">fac1</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">t</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">fac0</span><span class="o">*</span><span class="n">bspline</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">fac1</span><span class="o">*</span><span class="n">bspline</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s">'Got i=</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">, i must be an integer between 0 and len(x)-p-1=</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="n">p</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<p>The splines are quite fast, but our dataset contains thousands of points,
and for each point we calculate the likelihood thousands of times,
so it is better to precompute the value of the splines.
We will use second order splines in order to ensure smoothness,
and we will use 15 knots.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_fit</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">p_fit</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">splines_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_fit</span><span class="p">)</span><span class="o">-</span><span class="n">p_fit</span><span class="o">-</span><span class="mi">1</span>

<span class="n">basis</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">bspline</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'age'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">x_fit</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">p_fit</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">splines_dim</span><span class="p">)])</span>
</code></pre></div></div>

<p>Before fitting the model, let us take a look at our basis functions</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_plot</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>

<span class="n">basis_plot</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">bspline</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">x_fit</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">p_fit</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">splines_dim</span><span class="p">)])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">basis_plot</span><span class="p">:</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">elem</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/splines/basis.webp" alt="Our basis functions" /></p>

<p>We are finally ready to fit our model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">spline_model</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'w'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">splines_dim</span><span class="p">))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">df</span><span class="p">[</span><span class="s">'age'</span><span class="p">]</span> <span class="o">+</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">basis</span><span class="p">)</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'yhat'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'logfev1'</span><span class="p">])</span>

<span class="k">with</span> <span class="n">spline_model</span><span class="p">:</span>
    <span class="n">idata_spline</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_spline</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/splines/trace.webp" alt="Our dataset" /></p>

<p>We can now verify if our model is able to describe the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">spline_model</span><span class="p">:</span>
    <span class="n">mu_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'mu_pred'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">x_plot</span> <span class="o">+</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">basis_plot</span><span class="p">))</span>
    <span class="n">yhat_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'yhat_pred'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu_pred</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

<span class="k">with</span> <span class="n">spline_model</span><span class="p">:</span>
    <span class="n">ppc_spline</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata_spline</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'yhat_pred'</span><span class="p">,</span> <span class="s">'mu_pred'</span><span class="p">])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span>
<span class="n">ppc_spline</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pred'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span>
<span class="n">ppc_spline</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
<span class="n">ppc_spline</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
                <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span>
               <span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'age'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'logfev1'</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="n">x_plot</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_plot</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/splines/ppc.webp" alt="Our dataset" /></p>

<p>As we can see, our model accurately reproduces our data without
overfitting it. It has some small issue above 18, and the fact
that splines are not very stable just below the boundaries
is a known issue.</p>

<h2 id="conclusions">Conclusions</h2>

<p>You should consider using splines if you need more flexibility than
ordinary linear regression, as they allow you to smoothly add non-linearity
without overfitting.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/splines_regression/" /><summary type="html"><![CDATA[Going beyond linear models]]></summary></entry><entry><title type="html">Introduction to non-parametric models</title><link href="http://localhost:4000/statistics/nonparametric" rel="alternate" type="text/html" title="Introduction to non-parametric models" /><published>2024-06-02T00:00:00+00:00</published><updated>2024-06-02T00:00:00+00:00</updated><id>http://localhost:4000/statistics/nonparametric</id><content type="html" xml:base="http://localhost:4000/statistics/nonparametric"><![CDATA[<p>Non-parametric models are becoming more and more popular in Bayesian
statistics, as they are able to accurately reproduce complex data
and patterns.
In this section we will introduce some of the most popular
non-parametric models, namely splines and Dirichlet processes-related
models.</p>

<p>When dealing with these models, one should keep in mind that
the Bernstein-Von Mises theorem does not hold, so one cannot
approximate frequentist confidence intervals with Bayesian
credible intervals.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/nonparametric_intro/" /><summary type="html"><![CDATA[Letting the number of parameters vary]]></summary></entry><entry><title type="html">How does MCMC works</title><link href="http://localhost:4000/statistics/mcmc_intro" rel="alternate" type="text/html" title="How does MCMC works" /><published>2024-06-01T00:00:00+00:00</published><updated>2024-06-01T00:00:00+00:00</updated><id>http://localhost:4000/statistics/mcmc_intro</id><content type="html" xml:base="http://localhost:4000/statistics/mcmc_intro"><![CDATA[<p>In this post I will try and give you an idea of how does PyMC works
by performing Bayesian inference from scratch.
I just want to explain the underlying working principles,
without entering too much into technical details, so I will try and keep
things as simple as possible.</p>

<h2 id="sampling-random-numbers">Sampling random numbers</h2>

<h3 id="the-linear-congruential-generator">The linear congruential generator</h3>

<p>This is the simplest generator, and it allows you to generate
random integers between $0$ and some large integer $c$
or, equivalently, to generate float numbers between $0$ and $1$.
Given three integers $a$, $b$ and $c$, a linear congruential generator
can be constructed as</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">uniform</span><span class="p">,</span> <span class="n">norm</span><span class="p">,</span> <span class="n">t</span>

<span class="k">def</span> <span class="nf">rndn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">a</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">b</span><span class="p">)</span><span class="o">%</span><span class="n">c</span>
</code></pre></div></div>

<p>This is the default random number generator for most programming
languages, and the choice of the three parameters is not unique.
On the <a href="https://en.wikipedia.org/wiki/Linear_congruential_generator">Wikipedia page</a>
you will find a large number of possible choice.
A possible good one is</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">31</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span>
<span class="n">b</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">29</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">17</span>

<span class="c1"># We initialize the sequence with a random number
</span><span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">%</span> <span class="n">c</span>

<span class="n">xtmp</span> <span class="o">=</span> <span class="p">[</span><span class="n">seed</span><span class="p">]</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">seed</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20000</span><span class="p">):</span>  
    <span class="n">x0</span> <span class="o">=</span> <span class="n">rndn</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
    <span class="n">xtmp</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">xtmp</span><span class="p">)</span><span class="o">/</span><span class="n">c</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/mcmc/uniform.webp" alt="The histogram of our linear congruential generator" /></p>

<h3 id="inverse-transform-sampling">Inverse transform sampling</h3>

<p>By using this generator we can sample any distribution such that
the inverse of the cumulative distribution function is known.
In fact, if $X$ is distributed according to the uniform distribution
over \([0, 1]\) and $F(x)$ is the cumulative distribution function
of a distribution with probability density $p(x)\,,$
then $F^{-1}(X)$ is distributed according to \(p(x)\,.\)</p>

<p>Let us take as an example the normal distribution function with mean $2$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">xpl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">).</span><span class="n">ppf</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">xtmp</span><span class="p">)</span><span class="o">/</span><span class="n">c</span><span class="p">),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xpl</span><span class="p">,</span> <span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">).</span><span class="n">pdf</span><span class="p">(</span><span class="n">xpl</span><span class="p">))</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/mcmc/normal.webp" alt="The histogram of our normally distributed random numbers" /></p>

<p>As you can see, the theoretical distribution matches the sampled one with
quite a high accuracy.
There is, of course, the issue that we are sampling
correlated numbers, while we would like to have independent
random numbers.
This is one of the central problems of any random number generator,
and the easiest way to deal with is to take a slice of the sampled array,
since the correlation between distant elements is smaller than the one
between nearby elements.</p>

<h2 id="markov-chain-monte-carlo">Markov Chain Monte Carlo</h2>

<p>The inverse transform sampling only works with distributions
with known cumulative distribution function. 
When we perform Bayesian statistics, however, we don’t know how to compute
it, so other methods are needed.
Here we will introduce the Markov Chain Monte Carlo (MCMC) techniques.
These methods rely on properties of Markov processes, and a discussion
on this topic is far away from the subject of this blog,
so we will limit ourselves to the illustration of the methods.</p>

<h3 id="the-metropolis-algorithm">The Metropolis algorithm</h3>

<p>The Metropolis algorithm allows you to sample any distribution
with known density/mass function, you only need a proposal distribution.
The algorithm can be implemented as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">prop</span><span class="p">(</span><span class="n">xold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">logpdf</span><span class="p">):</span>
    <span class="c1"># The proposal distribution
</span>    <span class="n">xtemp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
    <span class="n">log_w</span> <span class="o">=</span> <span class="n">logpdf</span><span class="p">(</span><span class="n">xtemp</span><span class="p">)</span> <span class="o">-</span> <span class="n">logpdf</span><span class="p">(</span><span class="n">xold</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">log_w</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">xtemp</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_w</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">z</span> <span class="o">&lt;</span> <span class="n">w</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">xtemp</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">xold</span>
</code></pre></div></div>

<p>Let us now see how to use it</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Our initial point
</span><span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Our target distribution
</span><span class="n">target</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">).</span><span class="n">logpdf</span>


<span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50000</span><span class="p">):</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">prop</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">res</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    
<span class="c1"># We discard the first half of the sample, since
# the initial points may be far away from the target distribution
</span><span class="n">sz</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># We only take a subsample to reduce the correlation
</span><span class="n">res</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="n">sz</span><span class="p">::</span><span class="mi">3</span><span class="p">]</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">xpl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xpl</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">target</span><span class="p">(</span><span class="n">xpl</span><span class="p">)))</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/mcmc/metropolis.webp" alt="The histogram of the random numbers generated using the Metropolis algorithm" /></p>

<h2 id="the-hamiltonian-monte-carlo-algorithm">The Hamiltonian Monte Carlo algorithm</h2>

<p>For the Metropolis algorithm, the success of the sampling crucially
depends on the proposal distribution, and this might cause
problems for strongly correlated high dimensional distributions.
For this reason, the best algorithm for high dimensional distributions
is the Hamiltonian Monte Carlo (HMC).
The underlying idea behind the HMC is that, if $x$ is distributed
according to \(f(x)\) then</p>

\[g(p, x) = \frac{1}{\sqrt{2 \pi}} e^{-p^2/2} p(x)\]

<p>has \(p(x)\) as marginal distribution. If we then observe that</p>

\[-\log(g(p, x)) = \frac{p^2}{2} - \log(p(x)) + \log{\sqrt{2\pi}}= H(p, x)\]

<p>describes the hamiltonian of a particle with potential \(-\log(p(x))\,.\)
Thanks to this, it is possible to prove that the following algorithm
produces a sample distributed according to \(p(x):\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">leapfrog</span><span class="p">(</span><span class="n">xold</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">potential</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">p</span> <span class="o">-</span> <span class="n">dt</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">potential</span><span class="p">(</span><span class="n">xold</span><span class="o">+</span><span class="n">eps</span><span class="p">)</span><span class="o">-</span><span class="n">potential</span><span class="p">(</span><span class="n">xold</span><span class="p">))</span><span class="o">/</span><span class="n">eps</span>
    <span class="n">xn</span> <span class="o">=</span> <span class="n">xold</span> <span class="o">+</span> <span class="n">dt</span><span class="o">*</span><span class="n">p1</span>
    <span class="n">p2</span> <span class="o">=</span> <span class="n">p1</span> <span class="o">-</span> <span class="n">dt</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">potential</span><span class="p">(</span><span class="n">xn</span><span class="o">+</span><span class="n">eps</span><span class="p">)</span><span class="o">-</span><span class="n">potential</span><span class="p">(</span><span class="n">xn</span><span class="p">))</span><span class="o">/</span><span class="n">eps</span>
    <span class="k">return</span> <span class="n">p2</span><span class="p">,</span> <span class="n">xn</span>

<span class="k">def</span> <span class="nf">hamiltonian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">potential</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">p</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="n">potential</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">p0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="n">L</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">dt</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-4</span>

<span class="k">def</span> <span class="nf">pot</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">target</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">xn</span> <span class="o">=</span> <span class="n">x0</span>
<span class="n">rs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50000</span><span class="p">):</span>
    <span class="n">pn</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>
        <span class="n">ptmp</span><span class="p">,</span> <span class="n">xtmp</span> <span class="o">=</span> <span class="n">leapfrog</span><span class="p">(</span><span class="n">xn</span><span class="p">,</span> <span class="n">pn</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">pot</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">hamiltonian</span><span class="p">(</span><span class="n">xtmp</span><span class="p">,</span> <span class="n">ptmp</span><span class="p">,</span> <span class="n">pot</span><span class="p">)</span><span class="o">+</span><span class="n">hamiltonian</span><span class="p">(</span><span class="n">xn</span><span class="p">,</span> <span class="n">pn</span><span class="p">,</span> <span class="n">pot</span><span class="p">))])</span>
        <span class="k">if</span> <span class="n">w</span> <span class="o">&lt;</span> <span class="n">alpha</span><span class="p">:</span>
            <span class="n">xn</span> <span class="o">=</span> <span class="n">xtmp</span>
        <span class="n">rs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">xn</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">xpl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">rs</span><span class="p">[</span><span class="mi">20000</span><span class="p">::</span><span class="mi">5</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xpl</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">target</span><span class="p">(</span><span class="n">xpl</span><span class="p">)))</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/mcmc/hmc.webp" alt="The histogram of the random numbers generated using the Metropolis algorithm" /></p>

<p>This method works much better than the Metropolis algorithm, especially
for highly correlated variables.
Up to few years ago, this method was not very popular because
implementing it requires the computation of the Jacobian matrix (the derivative
of the log pdf).
In the 2010s, however, automatic differentiation became available,
and it became possible to easily implement this algorithm
within STAN and many other frameworks to perform Bayesian statistics.</p>

<h2 id="bayesian-inference-with-hmc">Bayesian inference with HMC</h2>

<p>We can now leverage what we implemented above to compute the posterior
distribution of a one dimensional system.
Let us assume that we have some data, and we know that it is
distributed according to a Student-t distribution with 5 dof
and parameter $\sigma=1\,,$ but we don’t know its mean.
What we know is that the mean’s order of magnitude is roughly 1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sample the fake data
</span><span class="n">data</span> <span class="o">=</span> <span class="mf">0.8</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">standard_t</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>

<span class="c1"># We now implement Bayes theorem.
# We take as prior for our parameter a normal distribution with sigma=20
</span>
<span class="k">def</span> <span class="nf">post</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">logpdf</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">20</span><span class="p">).</span><span class="n">logpdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">likelihood</span><span class="p">)</span> <span class="o">-</span> <span class="n">prior</span>

<span class="n">xn</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">rs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">dtn</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20000</span><span class="p">):</span>
    <span class="n">pn</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>
        <span class="n">ptmp</span><span class="p">,</span> <span class="n">xtmp</span> <span class="o">=</span> <span class="n">leapfrog</span><span class="p">(</span><span class="n">xn</span><span class="p">,</span> <span class="n">pn</span><span class="p">,</span> <span class="n">dtn</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">hamiltonian</span><span class="p">(</span><span class="n">xtmp</span><span class="p">,</span> <span class="n">ptmp</span><span class="p">,</span> <span class="n">post</span><span class="p">)</span><span class="o">+</span><span class="n">hamiltonian</span><span class="p">(</span><span class="n">xn</span><span class="p">,</span> <span class="n">pn</span><span class="p">,</span> <span class="n">post</span><span class="p">))])</span>
        <span class="k">if</span> <span class="n">w</span> <span class="o">&lt;</span> <span class="n">alpha</span><span class="p">:</span>
            <span class="n">xn</span> <span class="o">=</span> <span class="n">xtmp</span>
        <span class="n">rs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">xn</span><span class="p">)</span>

<span class="n">trace</span> <span class="o">=</span> <span class="n">rs</span><span class="p">[</span><span class="mi">2000</span><span class="p">::</span><span class="mi">5</span><span class="p">]</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/mcmc/bayes.webp" alt="Our sample describing the posterior distribution for our unknown parameter" /></p>

<p>We can now easily compute any estimate for the parameter.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<div class="code">
0.786
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">)</span>
</code></pre></div></div>

<div class="code">
0.696
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="mf">0.96</span><span class="p">)</span>
</code></pre></div></div>

<div class="code">
0.870
</div>

<p>We can therefore conclude that our parameter
has mean 0.88 and 94% credible interval 
\([0.78, 0.97]\,.\)</p>

<h2 id="conclusion">Conclusion</h2>

<p>I hope I managed to give you an idea of how does a probabilistic
programming language works.
In the next posts we will see how to use PyMC to write down statistical models
and criticize them.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/mcmc_intro/" /><summary type="html"><![CDATA[Getting an idea of what's happening behind the scenes]]></summary></entry><entry><title type="html">Introduction to bayesian statistics</title><link href="http://localhost:4000/statistics/bayes_intro" rel="alternate" type="text/html" title="Introduction to bayesian statistics" /><published>2024-05-25T00:00:00+00:00</published><updated>2024-05-25T00:00:00+00:00</updated><id>http://localhost:4000/statistics/bayes_intro</id><content type="html" xml:base="http://localhost:4000/statistics/bayes_intro"><![CDATA[<p>The main topic of this section will be statistics, and we will mostly discuss Bayesian
models.</p>

<h2 id="why-should-you-learn-bayesian-statistics">Why should you learn Bayesian statistics</h2>

<p>Bayesian statistics attracted much interest in the last years, and in the last decades
many Bayesian models have been developed to tackle any kind of problem.</p>

<p><img src="/docs/assets/images/statistics/intro/nature_count.webp" alt="The number of Nature research article 
matching the keyword &quot;bayes&quot;" /></p>

<p>Bayesian statistics can be a valuable tool for any data scientist, as it easily allows you
to build, fit and criticize any kind of model with a minimal effort.
In Bayesian inference you don’t only get an estimate for your parameters,
but you get the entire probability distribution for them, and this implies that
you can immediately get the uncertainty for your parameters.</p>

<p>In any statistical model you must specify a likelihood, which represents
the probability distribution which generated the data, and we will refer to this quantity
as</p>

\[P(X | \theta)\]

<p>where $\theta$ represents the unknown parameter vector.
In a Bayesian model you must moreover specify a prior distribution for the parameter
set</p>

\[P(\theta)\,.\]

<p>You can therefore compute the probability distribution of your parameters given the data
by using the Bayes theorem:</p>

\[P(\theta | X) = \frac{P(X | \theta)}{P(X)} P(\theta)\]

<p>Since the denominator \(P(X)\) is a normalization constant independent on $\theta$ which can be computed by normalizing the left hand side of the equation
to 1, its dependence is usually neglected and the Bayes theorem is usually rewritten as</p>

\[P(\theta | X) \propto P(X | \theta) P(\theta)\,.\]

<h2 id="a-historical-trip-in-mcmc">A historical trip in MCMC</h2>

<p>While the normalization constant \(P(X)\) can be <strong>in principe</strong> computed for any model,
it can be analytically computed only for a very limited range of models, namely
the conjugate models, and for this
reason Bayesian models have not been so popular for a long time.
However, thanks to the development of Monte Carlo sampling techniques, it became possible
to easily sample (pseudo) random numbers according to any probability distribution.
These techniques have been developed during the WWII by the Manhattan project
group, and it soon became popular among scientists to perform numerical simulations.
Up to few years ago, this was however only possible for people with a strong background
in numerical techniques, as it was necessary to implement from scratch the sampler.
Nowadays things have changed, and Probabilistic Programming Languages
such as PyMC, STAN or JAGS allows anyone to implement and fit any
kind of model with a minimal effort.</p>

<h2 id="some-philosophical-consideration">Some philosophical consideration</h2>
<p>You may have heard about the war of statistics, which is a debate which lasted
almost one century between frequentist statisticians and Bayesian ones.
At the beginning of the last century, a group of statisticians tried and promote
Bayesian statistics as the only meaningful way to do statistical inference.
According to them, the prior should have encoded all the available information
together with any personal consideration of the researcher. In this way,
the posterior probability distribution $P(\theta | X)$ can be interpreted
as the updated version of the researcher’s beliefs once the observed data $X$
has been taken into account by a perfectly rational person.
This philosophy has been rejected by the majority of the statisticians,
as they considered this “subjective” probability meaningless.</p>

<p>Today, however, the prior probability is only considered a regularization tool,
which allows you to use Bayes theorem to compute the posterior probability.
The results should only weakly depend on the prior choice, and this dependence
must be taken into account when reporting the fit results.
In this way, since the <a href="https://en.wikipedia.org/wiki/Bernstein%E2%80%93von_Mises_theorem">Bernstein–von Mises</a> theorem ensures you that
in the long run the Bayesian inference will produce the same results of the model
with the same likelihood, one can stick to the usual frequentist interpretation.</p>

<div class="emphbox">
Nowadays Bayesian statistics is accepted by most statisticians as a tool to make inference,
and an entire workflow has been developed to ensure that the inference procedure
has been properly performed.
</div>

<h2 id="bayesian-statistics-as-a-tool-in-the-replication-crisis">Bayesian statistics as a tool in the replication crisis</h2>

<p>During the beginning of the 2010s, scientists realized that a large part
of research articles were impossible to replicate.
In these years, many unsound scientific results have been found, and claims such as
<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4706048/">the existence of paranormal phenomena</a> or that 
<a href="http://cda.psych.uiuc.edu/multivariate_fall_2013/salmon_fmri.pdf">a dead salmon was able to recognize human emotions</a> were not as rare
as one would expect <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<p>A new research field emerged in these years, namely meta-science (which means the science
which studies science), and this produced many proposals to tackle the scientific crisis.
One of the issues that came out was that <a href="https://www.nature.com/articles/d41586-019-00874-8">the “golden standard” tool of the 0.05 statistical
significance was often misused or misinterpreted</a>,
and statisticians suggested that using a broader set of tools to perform statistical
analysis would have reduced this problem.
As you might imagine, due to the simple interpretation and of
the possibility to easily implement structured models and of combining
different data sources,
Bayesian statistics has been popularized by statisticians as one of these tools,
and it has gained a lot of attention by the scientific community.</p>

<h2 id="technical-considerations-of-using-bayesian-statistics">Technical considerations of using Bayesian statistics</h2>

<p>There are also technical considerations which one should take into account
when choosing the analysis method.
For simple models there is no reason why one should prefer Bayesian inference
to frequentist-based ones,
at least as long as the sample size is large enough
to allow you using the central limit theorem.
However, for complex models, things soon change. One may naively be tempted
to simply use the maximum likelihood estimation, which is in principle a
valid way to drop out the prior dependence. However, this method relies
on finding the extreme of a function, which implies maximizing the derivative
of the likelihood. This approach soon becomes unstable as soon as the model
complexity grows, and you should spend a lot of effort in ensuring that
your numerical approximation is close enough to the true maximum.
For these reasons, statistical textbooks devoted to complex models such as
longitudinal data analysis models provide many different methods
to fit the frequentist models <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.</p>

<p>you must find its zeros, and there is no stable procedure to do so for higher dimensional problems.</p>

<p>The Bayesian method, on the other hand, does not require to compute any
derivative, as you simply need to sample the posterior distribution and, as
we will see, it is very easy to assess the goodness of such a sampling procedure.
If you then want to compute any point estimate  \(\mathbb{E}[f(\theta)]\), what you have to do is to compute
the corresponding expectation value on the sample \(\left\{\theta_i\right\}_i:\)</p>

\[\mathbb{E}[f(\theta)] \approx \frac{1}{N} \sum_{i=1}^N f(\theta_i)\]

<p>and, as we will discuss, the estimate of the error associated with this procedure is already
implemented in the sampling engine.</p>

<p>There is of course no free lunch, as every method has pros and cons.
The main drawback of using Bayesian methods is that sampling may require time,
and while having a high quality sample with few parameters
requires seconds, you may need hours or more for a good sample if you are
dealing with thousands of parameters.
You should also consider that the prior selection might take require some effort,
especially when you are dealing with a new problem and if you are new to Bayesian
inference.</p>

<h2 id="conclusions">Conclusions</h2>

<p>I hope I convinced you that Bayesian statistics is a valuable instrument in the toolbox of
any data scientist.
In the next articles I will show you how to implement, check and discuss Bayesian models
in Python.
As usual, if you have criticisms or suggestions, feel free to write me.</p>

<p>In this section of the blog we will both discuss Bayesian inference and
the Bayesian workflow.
I will use Python to perform the computation, and I will use the PyMC ecosystem.</p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>One is a joke, the other is not. Choosing which one is a joke is left to the reader as an exercise. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Even if you analytically compute the derivative, <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/bayes_intro/" /><summary type="html"><![CDATA[A little bit more about bayesian statistics]]></summary></entry><entry><title type="html">The autoregressive model</title><link href="http://localhost:4000/statistics/autoregressive" rel="alternate" type="text/html" title="The autoregressive model" /><published>2024-02-16T00:00:00+00:00</published><updated>2024-02-16T00:00:00+00:00</updated><id>http://localhost:4000/statistics/autoregressive</id><content type="html" xml:base="http://localhost:4000/statistics/autoregressive"><![CDATA[<p>In the last post we introduced the time series, and in this post
we will look more in details to the autoregressive model,
namely</p>

\[y_k = \sum_{i=1}^r y_{k-i} + \varepsilon_k\]

<p>where $\varepsilon_k$ are iid.</p>

<p>We will use the airline passengers dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">statsmodels.graphics.tsaplots</span> <span class="kn">import</span> <span class="n">plot_acf</span><span class="p">,</span> <span class="n">plot_pacf</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">from</span> <span class="nn">scipy.signal</span> <span class="kn">import</span> <span class="n">periodogram</span>
<span class="kn">import</span> <span class="nn">pymc.sampling_jax</span> <span class="k">as</span> <span class="n">pmj</span>

<span class="n">df_pass</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"https://raw.githubusercontent.com/MakrandBhandari/Time-Series-Forecasting--Airline-Passengers-in-Python/main/international-airline-passengers.csv"</span><span class="p">)</span>
<span class="n">df_pass</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">"International airline passengers: monthly totals in thousands. Jan 49 ? Dec 60"</span><span class="p">:</span> <span class="s">"Thous_pass"</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df_pass</span> <span class="o">=</span> <span class="n">df_pass</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">144</span><span class="p">]</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"Month"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"Thous_pass"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[::</span><span class="mi">24</span><span class="p">])</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/autoregressive/airline.webp" alt="" /></p>

<p>A visual inspection of the data can be very useful, but sometimes it could not be 
enough to build a starting model. 
As an example, we can clearly see that there is a (probably linear) trend in our series,
that there is a strong annual periodicity, and that the amplitude of the periodic part increases
with the time.
There are however methods which may help us in this task.
The first is the <a href="https://en.wikipedia.org/wiki/Periodogram"><strong>periodogram</strong></a>, which could help us in assessing the frequency of the
seasonal part.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f</span><span class="p">,</span> <span class="n">Pxx_spec</span> <span class="o">=</span> <span class="n">periodogram</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Thous_pass'</span><span class="p">],</span> <span class="n">detrend</span><span class="o">=</span><span class="s">'linear'</span><span class="p">,</span> <span class="n">scaling</span><span class="o">=</span><span class="s">'spectrum'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">stem</span><span class="p">(</span><span class="n">f</span><span class="o">*</span><span class="mi">12</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Pxx_spec</span><span class="p">))</span> <span class="c1"># The dataset has monthly frequency, we now convert the frequency in Y^-1
</span><span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'frequency $[Y^{-1}]$'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">35</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/autoregressive/periodogram.webp" alt="The periodogram, where the frequency is expressed in inverse years" /></p>

<p>The peak at 1 confirms us that there is a strong periodic component at one year, as well as
possibly some higher frequency components.</p>

<p>We now want to have an idea of the order of the autoregressive part, and
plotting the coefficients of the autocorrelation function can be helpful in this task.
While the periodogram automatically handles the linear component, here we must remove 
both the trend and the periodic components by hand. We can do this as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">detrended</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">diff</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Thous_pass'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
<span class="n">deperiod</span> <span class="o">=</span> <span class="n">detrended</span><span class="p">[:</span><span class="o">-</span><span class="mi">12</span><span class="p">]</span> <span class="o">-</span> <span class="n">detrended</span><span class="p">[</span><span class="mi">12</span><span class="p">:]</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">plot_acf</span><span class="p">(</span><span class="n">deperiod</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">plot_pacf</span><span class="p">(</span><span class="n">deperiod</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax1</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/autoregressive/acorr.webp" alt="The ACF and PACF plot" /></p>

<p>It looks like there is a small autoregressive component of order one.</p>

<p>We will now build a model with a trend, a periodic and a \(AR(1)\) component.
We will start by only assuming a yearly component, and in order to reduce the
trend in the amplitude of the seasonal component, we will model the logarithm
of the number of passengers.
We will only use the first 120 points to fit the model, while the remaining 2 years will
be used to assess the performances of our model for future data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pass</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Thous_pass'</span><span class="p">])).</span><span class="n">values</span>
<span class="n">x_pass</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pass</span><span class="p">))</span>

<span class="n">y_pass_fit</span> <span class="o">=</span> <span class="n">y_pass</span><span class="p">[:</span><span class="mi">120</span><span class="p">]</span>
<span class="n">x_pass_fit</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pass_fit</span><span class="p">))</span>
<span class="n">n_pred</span> <span class="o">=</span> <span class="mi">144</span> <span class="o">-</span> <span class="mi">120</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">pass_model</span><span class="p">:</span>
    <span class="n">y0</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y0'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'gamma'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">eta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'eta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_ar</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">AR</span><span class="p">(</span><span class="s">'x_ar'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="n">eta</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pass_fit</span><span class="p">))</span>
    <span class="n">muv</span> <span class="o">=</span> <span class="n">y0</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">x_pass_fit</span> <span class="o">+</span> <span class="n">x_ar</span>  <span class="o">+</span>  <span class="n">beta</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="n">x_pass_fit</span><span class="o">/</span><span class="mi">12</span><span class="p">))</span><span class="o">+</span>   <span class="n">gamma</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="n">x_pass_fit</span><span class="o">/</span><span class="mi">12</span><span class="p">))</span>
    <span class="n">yhat_pass</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'yhat_pass'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">muv</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_pass_fit</span><span class="p">)</span>
    <span class="n">trace_pass</span> <span class="o">=</span> <span class="n">pmj</span><span class="p">.</span><span class="n">sample_numpyro_nuts</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/autoregressive/trace.webp" alt="The trace plot" /></p>

<p>The trace looks OK, let us now verify if our model reproduces the fitted data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pass_model</span><span class="p">:</span>
    <span class="n">ppc_pass</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_pass</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span> <span class="n">y_pass</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pass_fit</span><span class="p">)],</span> <span class="n">ppc_pass</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'chain'</span><span class="p">,</span> <span class="s">'draw'</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pass_fit</span><span class="p">)],</span> <span class="n">ppc_pass</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'chain'</span><span class="p">,</span> <span class="s">'draw'</span><span class="p">],</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">),</span> <span class="n">ppc_pass</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'chain'</span><span class="p">,</span> <span class="s">'draw'</span><span class="p">],</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">),</span>
               <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="mi">12</span><span class="p">::</span><span class="mi">24</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/autoregressive/ppc_fitted.webp" alt="The PPC for the fit data" /></p>

<p>The agreement looks good, except for some minor issues with high frequency modes which is
however not interesting for us, since we are not interested in such a high resolution.
We can now verify is our model is also able to reproduce the observed data for the last two years.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_pass_pred</span> <span class="o">=</span> <span class="n">x_pass</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x_pass_fit</span><span class="p">):]</span>

<span class="k">with</span> <span class="n">pass_model</span><span class="p">:</span>
    <span class="n">pass_model</span><span class="p">.</span><span class="n">add_coords</span><span class="p">({</span><span class="s">"z_1"</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pass_fit</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pass</span><span class="p">),</span> <span class="mi">1</span><span class="p">)})</span>
    <span class="n">x_ar_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">AR</span><span class="p">(</span>
            <span class="s">"x_ar_pred"</span><span class="p">,</span>
            <span class="n">init_dist</span><span class="o">=</span><span class="n">pm</span><span class="p">.</span><span class="n">DiracDelta</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="n">x_ar</span><span class="p">[...,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
            <span class="n">rho</span><span class="o">=</span><span class="n">eta</span><span class="p">,</span>
            <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
            <span class="n">dims</span><span class="o">=</span><span class="s">"z_1"</span><span class="p">)</span>
    <span class="n">periodic</span> <span class="o">=</span>  <span class="n">beta</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="n">x_pass_pred</span><span class="o">/</span><span class="mi">12</span><span class="p">))</span><span class="o">+</span><span class="n">gamma</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="n">x_pass_pred</span><span class="o">/</span><span class="mi">12</span><span class="p">))</span>
    <span class="n">muv</span> <span class="o">=</span> <span class="n">y0</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">x_pass_pred</span> <span class="o">+</span> <span class="n">x_ar_pred</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>  <span class="o">+</span>  <span class="n">periodic</span>
    <span class="n">yhat_pass_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">"yhat_pass_pred"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">muv</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
    
    <span class="n">ppc_ar_pred_y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_pass</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'yhat_pass_pred'</span><span class="p">])</span>

<span class="n">ypass_av</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span>
<span class="n">ppc_pass</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span> <span class="n">ppc_ar_pred_y</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass_pred'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">])])</span>
<span class="n">ypass_m</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span>
<span class="n">ppc_pass</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span> <span class="n">ppc_ar_pred_y</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">])])</span>
<span class="n">ypass_M</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span>
<span class="n">ppc_pass</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span> <span class="n">ppc_ar_pred_y</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">])])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span> <span class="n">y_pass</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span> <span class="n">ypass_av</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span> <span class="n">ypass_m</span><span class="p">,</span> <span class="n">ypass_M</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="mi">12</span><span class="p">::</span><span class="mi">24</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pass_fit</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/autoregressive/ppc_all.webp" alt="" /></p>

<p>We observe some discrepancy for the yearly minimum, but except for that the data
are always included in the credible interval.</p>

<p>We can now inspect the contribution of each component</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pass_model</span><span class="p">:</span>
    <span class="n">ppc_ar_pred_ar</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_pass</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'x_ar_pred'</span><span class="p">])</span>

<span class="n">x_pass_all</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x_pass_fit</span><span class="p">,</span> <span class="n">x_pass_pred</span><span class="p">])</span>

<span class="n">y_trend</span> <span class="o">=</span> <span class="p">(</span><span class="n">trace_pass</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'y0'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]).</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">trace_pass</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]).</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">x_pass_all</span><span class="p">)</span>
<span class="n">y_seas</span> <span class="o">=</span> <span class="p">(</span><span class="n">trace_pass</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'beta'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]).</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="n">x_pass_all</span><span class="o">/</span><span class="mi">12</span><span class="p">))</span><span class="o">+</span><span class="n">trace_pass</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]).</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="n">x_pass_all</span><span class="o">/</span><span class="mi">12</span><span class="p">)))</span>
<span class="n">y_res</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">trace_pass</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'x_ar'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span> <span class="n">ppc_ar_pred_ar</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'x_ar_pred'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">])[</span><span class="mi">1</span><span class="p">:]])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">311</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span><span class="n">y_trend</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Trend'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="mi">12</span><span class="p">::</span><span class="mi">24</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x_pass_fit</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>

<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">312</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span><span class="n">y_seas</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Seasonal'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="mi">12</span><span class="p">::</span><span class="mi">24</span><span class="p">])</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x_pass_fit</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>

<span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">313</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span><span class="n">y_res</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Residual'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="mi">12</span><span class="p">::</span><span class="mi">24</span><span class="p">])</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x_pass_fit</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/autoregressive/components.webp" alt="" /></p>

<p>As expected, the trend component is the most relevant one. We can also see that
the residual component has the same order of magnitude of the seasonal one.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We have seen some tools which may help us choosing an appropriate model.
We have also seen how to implement a time series with an autoregressive component.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/time_series/" /><summary type="html"><![CDATA[How to model dependence on the past]]></summary></entry><entry><title type="html">Introduction to time series modelling</title><link href="http://localhost:4000/statistics/time_series" rel="alternate" type="text/html" title="Introduction to time series modelling" /><published>2024-02-15T00:00:00+00:00</published><updated>2024-02-15T00:00:00+00:00</updated><id>http://localhost:4000/statistics/time_series</id><content type="html" xml:base="http://localhost:4000/statistics/time_series"><![CDATA[<p>Up to now we assumed that our samples were iid according to some
probability distribution. We will now modify this assumption
ad assume that each observation depends on the previous
observations.
We will assume that we are dealing with discrete time process,
and leave continuous time ones to a future post.</p>

<p>Let us take a look at some example of time series.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">statsmodels</span> <span class="k">as</span> <span class="n">stats</span>
<span class="kn">import</span> <span class="nn">yfinance</span> <span class="k">as</span> <span class="n">yf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">statsmodels.graphics.tsaplots</span> <span class="kn">import</span> <span class="n">plot_acf</span><span class="p">,</span> <span class="n">plot_pacf</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="n">sm</span>
<span class="kn">import</span> <span class="nn">pyreadr</span>

<span class="n">df_co2</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"https://raw.githubusercontent.com/gahjelle/data-analysis-with-python/master/data/co2-ppm-mauna-loa-19651980.csv"</span><span class="p">)</span>
<span class="n">df_co2</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'CO2 (ppm) mauna loa, 1965-1980'</span><span class="p">:</span><span class="s">"CO2_ppm"</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df_stock</span> <span class="o">=</span> <span class="n">yf</span><span class="p">.</span><span class="n">download</span><span class="p">(</span><span class="s">'RB=F'</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="s">'2021-01-01'</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s">'2023-01-01'</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="s">'1wk'</span><span class="p">)</span>

<span class="n">df_lynx</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'http://people.whitman.edu/~hundledr/courses/M250F03/LynxHare.txt'</span><span class="p">,</span> <span class="n">delim_whitespace</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">df_lynx</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="s">'Year'</span>
<span class="n">df_lynx</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Hare'</span><span class="p">,</span> <span class="s">'Lynx'</span><span class="p">]</span>

<span class="n">df_pass</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"https://raw.githubusercontent.com/MakrandBhandari/Time-Series-Forecasting--Airline-Passengers-in-Python/main/international-airline-passengers.csv"</span><span class="p">)</span>
<span class="n">df_pass</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">"International airline passengers: monthly totals in thousands. Jan 49 ? Dec 60"</span><span class="p">:</span> <span class="s">"Thous_pass"</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_pass</span> <span class="o">=</span> <span class="n">df_pass</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">144</span><span class="p">]</span>

<span class="n">df_ssp</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://www.sidc.be/SILSO/INFO/snmtotcsv.php'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">';'</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">df_sunspot</span> <span class="o">=</span> <span class="n">df_ssp</span><span class="p">.</span><span class="n">set_axis</span><span class="p">([</span><span class="s">'YEAR'</span><span class="p">,</span> <span class="s">'month'</span><span class="p">,</span> <span class="s">'ym'</span><span class="p">,</span> <span class="s">'SUNACTIVITY'</span><span class="p">,</span> <span class="s">'activity_sd'</span><span class="p">,</span> <span class="s">'n_obs'</span><span class="p">,</span> <span class="s">'is_definitive'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df_sunspot</span> <span class="o">=</span> <span class="n">df_sunspot</span><span class="p">[</span><span class="n">df_sunspot</span><span class="p">[</span><span class="s">'YEAR'</span><span class="p">]</span><span class="o">&lt;</span><span class="mi">2024</span><span class="p">]</span>

<span class="k">with</span> <span class="n">requests</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'https://github.com/cran/TSA/raw/master/data/larain.rda'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'./larain.rda'</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">g</span><span class="p">:</span>
        <span class="n">g</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">content</span><span class="p">)</span>

<span class="n">df_rain</span> <span class="o">=</span> <span class="n">pyreadr</span><span class="p">.</span><span class="n">read_r</span><span class="p">(</span><span class="s">'./larain.rda'</span><span class="p">)</span>

<span class="n">df_larain</span> <span class="o">=</span> <span class="n">df_rain</span><span class="p">[</span><span class="s">'larain'</span><span class="p">]</span>

<span class="n">df_larain</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1878</span><span class="p">,</span><span class="mi">1993</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_co2</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"Month"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"CO2_ppm"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_co2</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[::</span><span class="mi">24</span><span class="p">])</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_stock</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"Date"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"Close"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_lynx</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"Year"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"Lynx"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"Month"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"Thous_pass"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">].</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[::</span><span class="mi">24</span><span class="p">])</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_sunspot</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"YEAR"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"SUNACTIVITY"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_larain</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"year"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"larain"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/time_series_intro/ts_examples.webp" alt="Some time series example
" /></p>

<p>Each time series has different characteristics, and each of them encodes different
features.</p>

<p>First of all, we can classify the time series depending on its <strong>trend</strong>, that is
the presence of a monotone growth or decrease of the mean.
This component is clearly visible in the airline dataset (4), in the $CO_2$ one (1) and in the stock
(number 2, where we can see a growth followed by a decrease).</p>

<p>Another kind of common component is the <strong>seasonal</strong> (or periodic) one,
which is present if there’s some recurrent pattern.
This component is evident in the $CO_2$ and in the airline dataset, but also possibly in the lynx one
as well as in the sunspot data.</p>

<p>A common approach is to decompose the time series into a sum of trend, seasonality and residual random component,
and not all of them are always needed.
As an example, the $CO_2$ time series can be well reproduced without its random component,
while the inclusion of this part will the core topic of the future posts in this section.</p>

<h2 id="the-co_2-series">The \(CO_2\) series</h2>

<p>We will only use a subset of our sample to fit the model, and use the remaining
point to check the performance of our model for this subset of points.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cut_df</span> <span class="o">=</span> <span class="mi">150</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">df_co2</span><span class="p">[</span><span class="s">'CO2_ppm'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[:</span><span class="n">cut_df</span><span class="p">]</span><span class="o">/</span><span class="mi">1000</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="n">y_all</span> <span class="o">=</span> <span class="n">df_co2</span><span class="p">[</span><span class="s">'CO2_ppm'</span><span class="p">]</span><span class="o">/</span><span class="mi">1000</span>
<span class="n">X_all</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_all</span><span class="p">))</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">co2_model</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'gamma'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">Xp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">X</span><span class="o">/</span><span class="mi">12</span><span class="p">)</span>  <span class="c1"># The regressor for the periodic component
</span>    <span class="n">mu0</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">+</span><span class="n">beta</span><span class="o">*</span><span class="n">X</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">Xp</span>
    <span class="c1"># X is the regressor for the trend component
</span>    <span class="n">y_co2</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y_co2'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="k">with</span> <span class="n">co2_model</span><span class="p">:</span>
    <span class="n">trace_co2</span> <span class="o">=</span> <span class="n">pmj</span><span class="p">.</span><span class="n">sample_numpyro_nuts</span><span class="p">(</span><span class="n">tune</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_co2</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/time_series_intro/trace_co2.webp" alt="The trace of the CO2 model
" /></p>

<p>Let us now extent the model above the fitted data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">co2_model</span><span class="p">:</span>
    <span class="n">Xp_all</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">X_all</span><span class="o">/</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">mu0_all</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">+</span><span class="n">beta</span><span class="o">*</span><span class="n">X_all</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">Xp_all</span>
    <span class="n">y_co2_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y_co2_pred'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu0_all</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
    <span class="n">ppc_co2</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_co2</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y_co2_pred'</span><span class="p">])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">df_co2</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span>
<span class="n">ppc_co2</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_co2_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
<span class="n">ppc_co2</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_co2_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
    <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span>
<span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">df_co2</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span>
<span class="n">ppc_co2</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_co2_pred'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]))</span>

<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_co2</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span> <span class="n">y_all</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_co2</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="n">cut_df</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_co2</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[::</span><span class="mi">24</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/time_series_intro/co2.webp" alt="The posterior predictive of the CO2 model" /></p>

<p>We are using only the data below the black dotted line to fit the model.
As we can see, our model reproduces with quite a good accuracy also the data above
this line for many months.
When you have enough data it can be really useful to perform this check, in order to ensure
that your model is able to reproduce the observed data for the future,
at least with the current knowledge.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We have seen a possible decomposition of a time series, and we saw that a possible
way to deal with the seasonal and trend components is to treat them as we would
do in a normal inference problem.
We saw how to split our data in order to check the performance of our model for unobserved data.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/time_series/" /><summary type="html"><![CDATA[How to model time-dependent processes]]></summary></entry><entry><title type="html">Synthetic control</title><link href="http://localhost:4000/statistics/synthetic_control" rel="alternate" type="text/html" title="Synthetic control" /><published>2024-02-12T00:00:00+00:00</published><updated>2024-02-12T00:00:00+00:00</updated><id>http://localhost:4000/statistics/synthetic_control</id><content type="html" xml:base="http://localhost:4000/statistics/synthetic_control"><![CDATA[<p>The <strong>synthetic control</strong> method recently became a very popular method
among economists (although I honestly can’t see the same enthusiasm in
the statistics community).
This method has been widely (and a little bit wildly) used to assess
the effects on a quantity \(Y^{\bar{s}}_t\) of the introduction of a new policy into a country $s$
(or other geographical region) at a time $t=t_1$.
Assuming that you have the same quantity for a set of similar countries $s_i$
as well as for the target country $\bar{s}\,,$
you assume that the time behavior of \(Y_{\bar{s}} = (Y_{t_0}^{\bar{s}}, \dots, Y^{\bar{s}}_{t_1})\) before the intervention is given by a weighted
average of $Y^{s_i}\,.$</p>

<p>You moreover assume, as control, the same weighted average
\(\bar{Y}^{\bar{s}}\) after the intervention.</p>

<p>A very detailed discussion of this method can be found on <a href="https://juanitorduz.github.io/synthetic_control_pymc/">Juan Camilo Orduz’ page</a>.
We will use the same model, but we will apply it to a different dataset.</p>

<p>While in fact he uses PyMC to reproduce <a href="https://matheusfacure.github.io/python-causality-handbook/landing-page.html">this example</a>,
we will use it to perform a simplified re-analysis of <a href="https://link.springer.com/article/10.1007/s10584-021-03111-2">this article</a>, where the authors analyze the impact of the introduction
of a policy for the reduction of the $CO_2$ emissions in the UK.
The dataset used in this work can be found <a href="https://zenodo.org/records/4566804">on Zenodo</a>.</p>

<p>The authors of the original work, in fact, performed a careful analysis
of the control set, while we will limit ourself to the set of countries
who were in the OECD organization in 2001 and who had not adopted any 
$CO_2$ reduction policy before that year.
We will assume</p>

\[Y^{\bar{s}} \sim \mathcal{N}(\mu, \sigma)\]

<p>In order to ensure that the behavior before the intervention is carefully
reproduced, we assume a small variance</p>

\[\sigma \sim \mathcal{Exp}(100)\]

<p>As anticipated, $\mu$ is given by</p>

\[\mu = \sum_{i=1}^n \omega_{i} Y^i\]

<p>We assume that the weights sum up to one, so we assume</p>

\[\omega \sim \mathcal{Dir}(1/n,\dots,1/n)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc.sampling_jax</span> <span class="k">as</span> <span class="n">pmjax</span>

<span class="n">df_dt</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./data/climate_policies.csv'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">';'</span><span class="p">)</span>
<span class="n">df_carb</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./data/nation.1751_2014.csv'</span><span class="p">)</span>

<span class="n">df_red</span> <span class="o">=</span> <span class="n">df_carb</span><span class="p">[</span><span class="n">df_carb</span><span class="p">[</span><span class="s">'Year'</span><span class="p">]</span><span class="o">&gt;=</span><span class="mi">1990</span><span class="p">][[</span><span class="s">'Nation'</span><span class="p">,</span> <span class="s">'Year'</span><span class="p">,</span> <span class="s">'Per capita CO2 emissions (metric tons of carbon)'</span><span class="p">]]</span>

<span class="n">df_co2</span> <span class="o">=</span> <span class="n">df_red</span><span class="p">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s">'Year'</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s">'Nation'</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s">'Per capita CO2 emissions (metric tons of carbon)'</span><span class="p">)</span>

<span class="c1"># Taken from the repo
</span>
<span class="n">oecd</span> <span class="o">=</span> <span class="p">[</span><span class="s">"AUSTRALIA"</span><span class="p">,</span><span class="s">"AUSTRIA"</span><span class="p">,</span><span class="s">"BELGIUM"</span><span class="p">,</span><span class="s">"CANADA"</span><span class="p">,</span><span class="s">"CZECH REPUBLIC"</span><span class="p">,</span>
        <span class="s">"DENMARK"</span><span class="p">,</span><span class="s">"FINLAND"</span><span class="p">,</span><span class="s">"FRANCE (INCLUDING MONACO)"</span><span class="p">,</span><span class="s">"GERMANY"</span><span class="p">,</span>
        <span class="s">"GREECE"</span><span class="p">,</span><span class="s">"HUNGARY"</span><span class="p">,</span><span class="s">"ICELAND"</span><span class="p">,</span><span class="s">"IRELAND"</span><span class="p">,</span><span class="s">"ITALY (INCLUDING SAN MARINO)"</span><span class="p">,</span>
        <span class="s">"JAPAN"</span><span class="p">,</span><span class="s">"LUXEMBOURG"</span><span class="p">,</span><span class="s">"MEXICO"</span><span class="p">,</span><span class="s">"NETHERLANDS"</span><span class="p">,</span><span class="s">"NEW ZEALAND"</span><span class="p">,</span><span class="s">"NORWAY"</span><span class="p">,</span>
        <span class="s">"POLAND"</span><span class="p">,</span><span class="s">"PORTUGAL"</span><span class="p">,</span><span class="s">"SLOVAKIA"</span><span class="p">,</span><span class="s">"REPUBLIC OF KOREA"</span><span class="p">,</span><span class="s">"SPAIN"</span><span class="p">,</span><span class="s">"SWEDEN"</span><span class="p">,</span>
        <span class="s">"SWITZERLAND"</span><span class="p">,</span><span class="s">"TURKEY"</span><span class="p">,</span><span class="s">"UNITED KINGDOM"</span><span class="p">,</span><span class="s">"UNITED STATES OF AMERICA"</span><span class="p">]</span>

<span class="n">to_exclude</span> <span class="o">=</span> <span class="p">[</span><span class="s">'DENMARK'</span><span class="p">,</span> <span class="s">'ESTONIA'</span><span class="p">,</span> <span class="s">'FINLAND'</span><span class="p">,</span> <span class="s">'NETHERLANDS'</span><span class="p">,</span> <span class="s">'NORWAY'</span><span class="p">,</span>
       <span class="s">'SLOVENIA'</span><span class="p">,</span> <span class="s">'SWEDEN'</span><span class="p">,</span> <span class="s">"UNITED KINGDOM"</span><span class="p">]</span>

<span class="n">donors</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">oecd</span><span class="p">)</span><span class="o">-</span><span class="nb">set</span><span class="p">(</span><span class="n">to_exclude</span><span class="p">))</span>

<span class="n">df_in</span> <span class="o">=</span> <span class="n">df_co2</span><span class="p">[</span><span class="n">donors</span><span class="p">].</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">sc_model</span><span class="p">:</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="s">'w'</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">)))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">):</span>
        <span class="n">mu</span> <span class="o">+=</span> <span class="n">w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">df_in</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">loc</span><span class="p">[</span><span class="mi">1990</span><span class="p">:</span><span class="mi">2001</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_co2</span><span class="p">[</span><span class="s">'UNITED KINGDOM'</span><span class="p">].</span><span class="n">loc</span><span class="p">[</span><span class="mi">1990</span><span class="p">:</span><span class="mi">2001</span><span class="p">])</span>

<span class="k">with</span> <span class="n">sc_model</span><span class="p">:</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pmjax</span><span class="p">.</span><span class="n">sample_numpyro_nuts</span><span class="p">(</span><span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>

</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/synthetic_control/trace.webp" alt="The trace plot" /></p>

<p>We ran quite a large number of draws as the number of parameters is quite large
and rather unconstrained. However, the trace looks fine.
An important thing that one should always verify when using
a synthetic control, is that the weights must be sparse (only few should
dominate, while the remaining should be close to 0).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'w'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/synthetic_control/weights.webp" alt="The forest plot of the weights" /></p>

<p>The requirement seems fulfilled, as only few dominate the entire fit.
We can now compute the posterior predictive before and after the intervention.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">sc_model</span><span class="p">:</span>
    <span class="n">mu1</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">):</span>
        <span class="n">mu1</span> <span class="o">+=</span> <span class="n">w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">df_in</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">loc</span><span class="p">[</span><span class="mi">2002</span><span class="p">:].</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">y1</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y1'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu1</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y'</span><span class="p">,</span> <span class="s">'y1'</span><span class="p">])</span>

<span class="n">yv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">ppc</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">20000</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
                     <span class="n">ppc</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y1'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">20000</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))],</span>
                     <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[[</span><span class="s">'right'</span><span class="p">,</span> <span class="s">'top'</span><span class="p">]].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
<span class="n">uk</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">df_co2</span><span class="p">[</span><span class="s">'UNITED KINGDOM'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'UK'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">yv</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">yv</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">synth</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">yv</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'Synthetic UK'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">2001</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Year"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">"Per capita $CO_2$ $m^3/Year$"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">'Synthetic'</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">yv</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">color</span><span class="o">=</span><span class="n">synth</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">get_color</span><span class="p">()</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">'UK'</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">df_co2</span><span class="p">[</span><span class="s">'UNITED KINGDOM'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">color</span><span class="o">=</span><span class="n">uk</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">get_color</span><span class="p">()</span> <span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'/home/stippe/thestippe.github.io/docs/assets/images/statistics/synthetic_control/posterior_predictive.webp'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/synthetic_control/posterior_predictive.webp" alt="The comparison between the true and the synthetic UK" /></p>

<p>As we can see, the behavior is very similar up to 2001, while after this date
the synthetic UK $CO_2$ consumption is larger than one of the true $UK\,.$
You can verify yourself that, by only fitting up to 2000, the result doesn’t
change, and the lines still diverge starting from 2002.
This is another important check that you should always perform when using the
synthetic control method.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We have seen how to implement the synthetic control method, together with
some of the most important checks that you should always do in order to
exclude major problems in your model.
We also re-analyzed <a href="https://link.springer.com/article/10.1007/s10584-021-03111-2">this article</a>, obtaining the same conclusions.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/discontinuity_regression/" /><summary type="html"><![CDATA[Building a doppleganger from the control group]]></summary></entry><entry><title type="html">Regression discontinuity</title><link href="http://localhost:4000/statistics/discontinuity_regression" rel="alternate" type="text/html" title="Regression discontinuity" /><published>2024-02-11T00:00:00+00:00</published><updated>2024-02-11T00:00:00+00:00</updated><id>http://localhost:4000/statistics/discontinuity_regression</id><content type="html" xml:base="http://localhost:4000/statistics/discontinuity_regression"><![CDATA[<p>Regression discontinuity recently became a popular way to assess the effect
of an intervention $I$ which depends on some confounder $X$ via</p>

\[I=
\begin{cases}
0 &amp; x &lt; x_0 \\
1 &amp; x \geq x_0
\end{cases}\]

<p>where in general the effect of $Y$ on $X$ varies smoothly.
Since the dependence can be, in principle, arbitrary, many authors
discuss both the linear as well as higher order polynomials (See Angrist’ textbook below).
However, higher order polynomial regression should in principle be avoided,
as it may lead to artificial discontinuities, as extensively explained
in <a href="http://www.stat.columbia.edu/~gelman/research/published/2018_gelman_jbes.pdf">this work by Gelman and Imbens</a>.</p>

<p>We will perform a bayesian version of <a href="https://lfoswald.github.io/2021-spring-stats2/materials/session-7/07-online-tutorial/">this analysis</a> and, for the reasons explained
above, we will limit ourself to the linear dependence.
The dataset uses is the MADD dataset, which collects the
mortality rate of young people in the USA.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">df_madd</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"https://raw.githubusercontent.com/seramirezruiz/stats-ii-lab/master/Session%206/data/mlda.csv"</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_madd</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="s">'forcing'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'outcome'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/discontinuity_regression/data.webp" alt="The input data" /></p>

<p>Here “outcome” is the mortality rate from motor vehicle (per 100000),
while “forcing” is age minus 21 (we recall that, in the US, 21 is the age
where it is legally possible to drink alcoholic beverages).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">madd_model</span><span class="p">:</span>
  <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
  <span class="n">gamma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'gamma'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
  <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
  <span class="n">mu_0</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">df_madd</span><span class="p">[</span><span class="s">'forcing'</span><span class="p">].</span><span class="n">values</span>
  <span class="n">mu</span> <span class="o">=</span> <span class="n">mu_0</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">heaviside</span><span class="p">(</span><span class="n">df_madd</span><span class="p">[</span><span class="s">'forcing'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
  <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> 
                <span class="n">observed</span><span class="o">=</span><span class="n">df_madd</span><span class="p">[</span><span class="s">'outcome'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
  <span class="n">trace_madd</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_madd</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/discontinuity_regression/trace.webp" alt="The trace plot" /></p>

<p>The trace looks good, let us verify if our model is able to reproduce the data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">madd_model</span><span class="p">:</span>
    <span class="n">x_pl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">x_pl</span><span class="o">+</span><span class="n">gamma</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">heaviside</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="n">y_pl</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y_pl'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
    <span class="n">pp_plot</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="o">=</span><span class="n">trace_madd</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y_pl'</span><span class="p">])</span>

<span class="n">pp_madd</span> <span class="o">=</span> <span class="n">pp_plot</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">.</span><span class="n">y_pl</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_pl</span><span class="p">)))</span>

<span class="n">madd_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pp_madd</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">madd_qqmax</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">pp_madd</span><span class="p">,</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">madd_qqmin</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">pp_madd</span><span class="p">,</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_madd</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="s">'forcing'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'outcome'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">madd_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">madd_qqmin</span><span class="p">,</span> <span class="n">madd_qqmax</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/discontinuity_regression/posterior_predictive.webp" alt="The posterior predictive plot" /></p>

<p>The posterior predictive seems in good agreement with the observed data, and
the threshold effect seems quite evident. In fact it is quite large:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">trace_madd</span><span class="p">,</span> <span class="n">filter_vars</span><span class="o">=</span><span class="s">'like'</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="s">'gamma'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/discontinuity_regression/effect.webp" alt="The boxplot of the effect size" /></p>

<h2 id="conclusions">Conclusions</h2>

<p>We have discussed how to implement the regression discontinuity,
together with some recommendations on how to implement it, and we applied
it to the MADD dataset.</p>

<h2 id="recommended-readings">Recommended readings</h2>
<ul>
  <li><cite>Angrist, J. D., Pischke, J. (2009). Mostly harmless econometrics : an empiricist’s companion. UK: Princeton University Press.
</cite></li>
</ul>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/discontinuity_regression/" /><summary type="html"><![CDATA[Using jumps to estimate effects]]></summary></entry><entry><title type="html">Difference in difference</title><link href="http://localhost:4000/statistics/difference_in_differences" rel="alternate" type="text/html" title="Difference in difference" /><published>2024-02-10T00:00:00+00:00</published><updated>2024-02-10T00:00:00+00:00</updated><id>http://localhost:4000/statistics/difference_in_differences</id><content type="html" xml:base="http://localhost:4000/statistics/difference_in_differences"><![CDATA[<p>Difference in differences is a very old technique,
and one of the first applications of
this method was done by John Snow, who’s also
popular due to the cholera outbreak data visualization.</p>

<p>In his study, he used the <strong>Difference in Difference</strong>
(DiD) method to provide some evidence that,
during the London cholera epidemic of 1866,
the cholera was caused by drinking from a water
pump.
This method has been more recently used <a href="https://davidcard.berkeley.edu/papers/njmin-aer.pdf">by 
Card and Krueger in this work</a>
to analyze the causal relationship between
minimum wage and employment.
In 1992, the New Jersey increased the minimum wage
from 4.25 dollars to 5.00 dollars.
They compared the employment in Pennsylvania
and New Jersey before and after the minimum wage increase
to assess if it caused a decrease in the New Jersey
occupation, as supply and demand theory would predict.</p>

<p>DiD assumes that, before the intervention $I$,
the untreated group and the treated one
both evolve linearly with the time $t$ with the
same slope,
while after the intervention the treated group
changes slope.
Assuming, that the intervention was applied at time
$t=0$</p>

\[\begin{align}
&amp;
Y_{P}^0 = \alpha_{P} 
\\
&amp;
Y_{P}^1 = \alpha_{P} +\beta
\\
&amp;
Y_{NJ}^0 = \alpha_{NJ} 
\\
&amp;
Y_{NJ}^1 = \alpha_{NJ} +\beta + \gamma
\end{align}\]

<p>In the above formulas, the intervention effect
is simply $\gamma\,.$</p>

<h2 id="the-implementation">The implementation</h2>

<p>We downloaded the dataset from <a href="https://www.kaggle.com/code/harrywang/difference-in-differences-in-python/input">this page</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">df_employment</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/employment.csv'</span><span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_employment</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'state'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/difference_in_difference/pairplot.webp" alt="The dataset pairplot" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_before</span> <span class="o">=</span> <span class="n">df_employment</span><span class="p">[[</span><span class="s">'state'</span><span class="p">,</span> <span class="s">'total_emp_feb'</span><span class="p">]]</span>
<span class="n">df_after</span> <span class="o">=</span> <span class="n">df_employment</span><span class="p">[[</span><span class="s">'state'</span><span class="p">,</span> <span class="s">'total_emp_nov'</span><span class="p">]]</span>

<span class="c1"># We will assign t=0 data before treatment and t=1 after the treatment
# Analogously g=0 will be the control group, g=1 will be the test group
</span>
<span class="n">df_before</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">df_after</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">df_before</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'total_emp_feb'</span><span class="p">:</span> <span class="s">'Y'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_after</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'total_emp_nov'</span><span class="p">:</span> <span class="s">'Y'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df_before</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'state'</span><span class="p">:</span> <span class="s">'g'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_after</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'state'</span><span class="p">:</span> <span class="s">'g'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df_reg</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_before</span><span class="p">,</span> <span class="n">df_after</span><span class="p">])</span>

<span class="c1">## Let us build the interaction term
</span>
<span class="n">df_reg</span><span class="p">[</span><span class="s">'gt'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_reg</span><span class="p">[</span><span class="s">'g'</span><span class="p">]</span><span class="o">*</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span>

<span class="n">df_reg</span> <span class="o">=</span> <span class="n">df_reg</span><span class="p">[[</span><span class="s">'g'</span><span class="p">,</span> <span class="s">'t'</span><span class="p">,</span> <span class="s">'gt'</span><span class="p">,</span> <span class="s">'Y'</span><span class="p">]]</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">did_model</span><span class="p">:</span>
    <span class="n">beta_0</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_0'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">beta_g</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_g'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">beta_t</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_t'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">beta_gt</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_gt'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s">'nu'</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="n">beta_g</span><span class="o">*</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'g'</span><span class="p">]</span><span class="o">+</span> <span class="n">beta_t</span><span class="o">*</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span><span class="o">+</span> <span class="n">beta_gt</span><span class="o">*</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'gt'</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">,</span>
                  <span class="n">observed</span><span class="o">=</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'Y'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
    <span class="n">trace_did</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_did</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/difference_in_difference/trace.webp" alt="The model trace" /></p>

<p>The trace looks fine, let us now verify the posterior
predictive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">did_model</span><span class="p">:</span>
    <span class="n">y00</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y00'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">beta_0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    <span class="n">y10</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y10'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">beta_0</span><span class="o">+</span><span class="n">beta_g</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    <span class="n">y01</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y01'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">beta_0</span><span class="o">+</span><span class="n">beta_t</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    <span class="n">y11</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y11'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">beta_0</span><span class="o">+</span><span class="n">beta_g</span><span class="o">+</span><span class="n">beta_t</span><span class="o">+</span><span class="n">beta_gt</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    
    <span class="n">ppc_check</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span>
        <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y00'</span><span class="p">,</span><span class="s">'y01'</span><span class="p">,</span><span class="s">'y10'</span><span class="p">,</span><span class="s">'y11'</span><span class="p">],</span> <span class="n">trace</span><span class="o">=</span><span class="n">trace_did</span><span class="p">)</span>


<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">].</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">80</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
            <span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">az</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">ppc_check</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s">"y</span><span class="si">{</span><span class="n">g</span><span class="si">}{</span><span class="n">t</span><span class="si">}</span><span class="s">"</span><span class="p">],</span> <span class="n">group</span><span class="o">=</span><span class="s">'posterior_predictive'</span><span class="p">,</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">],</span>
                       <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">)</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">az</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">ppc_check</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s">"y</span><span class="si">{</span><span class="n">g</span><span class="si">}{</span><span class="n">t</span><span class="si">}</span><span class="s">"</span><span class="p">],</span> <span class="n">group</span><span class="o">=</span><span class="s">'posterior_predictive'</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">])</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">df_reg</span><span class="p">[(</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'g'</span><span class="p">]</span><span class="o">==</span><span class="n">g</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span><span class="o">==</span><span class="n">t</span><span class="p">)][</span><span class="s">'Y'</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s">"g=</span><span class="si">{</span><span class="n">g</span><span class="si">}</span><span class="s">, t=</span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="n">legend</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">legend</span><span class="p">(</span><span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/difference_in_difference/posterior_predictives.webp" alt="The comparison between the predicted
and observed distributions of Y" /></p>

<p>The posterior predictive distributions agree with the observed data. We extracted some random sub-sample to
provide an estimate of the uncertainties.</p>

<p>We can finally verify if there is any effect:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">trace_did</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'beta_gt'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/difference_in_difference/effect_estimate.webp" alt="Our estimate for the minimum wage increase effect
" /></p>

<p>As you can see, the effect is compatible with 0, therefore there is no evidence
that by increasing the minimum salary there is an effect on the occupation.</p>

<p>Our model has a small issue: it allows for negative values of the occupation,
which doesn’t make sense. This problem can be easily circumvented by using 
the <a href="https://www.pymc.io/projects/docs/en/v4.4.0/api/distributions/generated/pymc.Truncated.html">truncated PyMC class</a>.</p>

<p>I suggest you to try it and verify yourself if there is any effect.
Remember that in that case $\mu$ is no more the mean for $Y$,
so you can’t use it to estimate the average effect.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We have seen how to implement the DiD method with PyMC, and we used to
re-analyze the Krueger and Card article on the relation between the minimum
salary and the occupation.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/causal_intro/" /><summary type="html"><![CDATA[Causal inference from 1850]]></summary></entry><entry><title type="html">Instrumental variable regression</title><link href="http://localhost:4000/statistics/instrumental_variable" rel="alternate" type="text/html" title="Instrumental variable regression" /><published>2024-02-09T00:00:00+00:00</published><updated>2024-02-09T00:00:00+00:00</updated><id>http://localhost:4000/statistics/instrumental_variable</id><content type="html" xml:base="http://localhost:4000/statistics/instrumental_variable"><![CDATA[<p>In many circumstances you cannot randomize, either because it is unethical
or simply because it’s too expensive.
There are however methods which, if appropriately applied, may provide
you some convincing causal evidence.</p>

<p>Let us consider the case where you cannot randomly assign the treatment $T\,,$
and in this case it could be affected by any confounder $X$
leading you to a biased estimate of the treatment effect.
However, if you have a variable $Z$ that only affects $T$
and does not affect your outcome in any other way other than via $T\,,$
than you can apply <strong>Instrumental Variable Regression</strong>.</p>

<p><img src="/docs/assets/images/statistics/instrumental_variable/causal_structure.webp" alt="The assumed causal flow" /></p>

<p>Of course, the above causal assumption is quite strong, but it holds
in quite a good approximation in some circumstance.</p>

<p>This method has been applied to analyze the effect of school years ($T$)
on earning ($Y$).
In this case the variable $Z$ was the assignment of some economical assistance
(a voucher) to go to school.</p>

<p>One would be tempted to simply use linear regression to fit this model:</p>

\[Y = \alpha + \beta T + \gamma Z + \varepsilon\]

<p>However, linear regression assumes independence between the regressors,
while in our case we have that $T$ is determined by $Z\,.$
This has an impact on the variance estimate of $Y\,,$ as we do not
correctly propagate the uncertainty due to the $T$ dependence on $Z\,.$
In fact, linear regression always predicts homoscedastic variance,
while IV can also reproduce heteroscedasticity.</p>

<h2 id="application-to-the-cigarettes-sales">Application to the cigarettes sales</h2>

<p>We will use IV to see if an increase in the cigarettes price ($T$)
causes a decrease in the cigarettes sales ($Y$), and we will use the
tobacco taxes as instrumental variable $Z$.
In order to linearize the dependence between the variables,
instead of the value of each quantity, we will consider the
difference between the 1995 log value and the 1985 log value.</p>

\[\begin{pmatrix}
T \\
Y \\
\end{pmatrix}
\sim 
\mathcal{t}
\left(
\left(
\alpha_0 + \beta_0 Z
\atop
\alpha_1 + \beta_1 T
\right),
\Sigma, \nu
\right)\]

<p>where $t$ represents the 2 dimensional Student-T distribution and $\Sigma$ is the $2\times2$ covariance matrix.
If $Z$ has a causal effect on $Y$ via $T\,,$ then the correlation
between $Y$ and $T$ is different from zero.</p>

<p>We will assume</p>

\[\alpha_i, \beta_i \sim \mathcal{N}(0, 10^3)\]

<p>and</p>

\[\nu \sim \mathcal{HalfNormal}(100)\]

<p>\(\Sigma\) must be a positive semi-defined matrix, and an easy way to
provide it a prior is using the
<a href="https://en.wikipedia.org/wiki/Lewandowski-Kurowicka-Joe_distribution">Lewandowski-Kurowicka-Joe distribution
</a>.
This distribution takes a shape parameter $\eta\,,$
and we will take $\eta=1\,,$ which implies that we will take a uniform
prior over $[-1, 1]$ for the correlation matrix.
We will moreover assume that the standard deviations are distributed according to</p>

\[\sigma_i \sim \mathcal{HalfCauchy}(20)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">pytensor.tensor.extra_ops</span> <span class="kn">import</span> <span class="n">cumprod</span>
<span class="kn">import</span> <span class="nn">pymc.sampling_jax</span> <span class="k">as</span> <span class="n">pmjx</span>

<span class="n">random_seed</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">df_iv</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/AER/CigarettesSW.csv'</span><span class="p">)</span>

<span class="n">X_iv</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1995</span><span class="p">][</span><span class="s">'price'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'price'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
       <span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'price'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
<span class="n">Y_iv</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1995</span><span class="p">][</span><span class="s">'packs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'packs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
       <span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'packs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
<span class="n">Z_iv</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1995</span><span class="p">][</span><span class="s">'taxs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'taxs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
       <span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'taxs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">instrumental_variable</span><span class="p">:</span>
    <span class="n">sd_dist</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">20.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'nu'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">100.0</span><span class="p">)</span>
    <span class="n">chol</span><span class="p">,</span> <span class="n">corr</span><span class="p">,</span> <span class="n">sigmas</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">LKJCholeskyCov</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sd_dist</span><span class="o">=</span><span class="n">sd_dist</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">Z_iv</span><span class="p">,</span> <span class="n">X_iv</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">X_iv</span><span class="p">,</span> <span class="n">Y_iv</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">w</span><span class="p">)</span>  <span class="c1"># so we will recover it easily
</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">MvStudentT</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">chol</span><span class="o">=</span><span class="n">chol</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_iv</span><span class="p">)),</span> <span class="n">observed</span><span class="o">=</span><span class="n">u</span><span class="p">)</span>
    <span class="c1"># We directly compute the posterior predictive
</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">MvStudentT</span><span class="p">(</span><span class="s">'y_pred'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">chol</span><span class="o">=</span><span class="n">chol</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>

<span class="k">with</span> <span class="n">instrumental_variable</span><span class="p">:</span>
    <span class="n">trace_instrumental_variable</span> <span class="o">=</span> <span class="n">pmjx</span><span class="p">.</span><span class="n">sample_numpyro_nuts</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_instrumental_variable</span> <span class="p">,</span>

              <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">,</span> <span class="s">'beta'</span><span class="p">,</span> <span class="s">'sigma'</span><span class="p">,</span> <span class="s">'nu'</span><span class="p">],</span>
              <span class="n">coords</span><span class="o">=</span><span class="p">{</span><span class="s">'sigma_corr_dim_0'</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s">'sigma_corr_dim_1'</span><span class="p">:</span><span class="mi">1</span><span class="p">})</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/instrumental_variable/trace.webp" alt="The trace plot of the above model" /></p>

<p>As we can see, there is no signal of problems in thee trace plot.</p>

<p>A few remarks on the above code. Since the model is not very fast,
we used the numpyro sampler, which hundred of times
faster than the standard PyMC sampler.
Moreover, we instructed arviz to only plot the off-diagonal elements
of the correlation matrix. We must do this because the diagonal elements
are always one, as they must be, but this causes an error in arviz
(which assumes a random behavior in all the variables of the trace).</p>

<p>We can now verify the posterior predictive distribution.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a0</span> <span class="o">=</span> <span class="n">trace_instrumental_variable</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">])[</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span>
<span class="n">b0</span> <span class="o">=</span> <span class="n">trace_instrumental_variable</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'beta'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">])[</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span>

<span class="n">x_min</span> <span class="o">=</span> <span class="mf">0.06</span>
<span class="n">x_max</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="n">x_pl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mf">0.0002</span><span class="p">)</span>

<span class="n">xiv_0</span> <span class="o">=</span> <span class="n">trace_instrumental_variable</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">2</span><span class="p">))[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">xiv_1</span> <span class="o">=</span> <span class="n">trace_instrumental_variable</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">2</span><span class="p">))[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">sampled_index</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">sampled_index</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">a0</span><span class="o">+</span><span class="n">b0</span><span class="o">*</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">sampled_index</span><span class="p">:</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xiv_0</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">xiv_1</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'x'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_iv</span><span class="p">,</span> <span class="n">Y_iv</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'steelblue'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'t'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/instrumental_variable/posterior_predictive.webp" alt="The posterior predictive distribution" /></p>

<p>Our model also looks capable to reproduce the observed data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">az</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">trace_instrumental_variable</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">"sigma_corr"</span><span class="p">])[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">"IV Model - Posterior Distribution Correlation"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$\sigma$'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">''</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/instrumental_variable/correlation.webp" alt="The off-diagonal component of the correlation matrix" /></p>

<h2 id="conclusions">Conclusions</h2>

<p>We have seen how IV allows us to make causal inference in absence of randomization,
but making some rather strong assumptions about the causal structure of the problem.
We have also seen how to implement it in PyMC.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/causal_intro/" /><summary type="html"><![CDATA[Making causal inference without randomization]]></summary></entry></feed>