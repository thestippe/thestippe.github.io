<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-11-15T10:54:23+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Data Perspectives</title><author><name>Stippe</name><email>smaurizio87@protonmail.com</email></author><entry><title type="html">Evolutions of the line chart</title><link href="http://localhost:4000/linechart-evolution/" rel="alternate" type="text/html" title="Evolutions of the line chart" /><published>2023-11-03T00:00:00+01:00</published><updated>2023-11-03T00:00:00+01:00</updated><id>http://localhost:4000/linechart-evolution</id><content type="html" xml:base="http://localhost:4000/linechart-evolution/"><![CDATA[<p>In a previous post we saw some of the most fundamental charts,
which are the basic building blocks for data visualization.</p>

<p>Datasets can become very complex, and you should adapt your data visualization
depending on your needs.
Here we will take a look at how we can draw more and more complex datasets
by simply changing few details of the basic visualization, and we will
do so by using the line chart as fundamental visualization.</p>

<h2 id="the-line-chart">The line chart</h2>

<p>As we have already seen, in the line chart we have</p>
<ul>
  <li>an ordered variable on the $x$ axis</li>
  <li>a quantitative variable on the $y$ axis</li>
</ul>

<p>As an example, let us take a look at the Italian GDP per capita expressed in US dollars adjusted by the US inflation, which can be found
<a href="https://github.com/thestippe/thestippe.github.io/blob/main/data/gdp_per_capita_filtered.csv">here</a>.</p>

<p>The dataset is based on <a href="https://github.com/RaafatSaleh/GDP-per-capita-and-its-effect-on-the-man-life-quality/blob/master/Data/gdppercapita_us_inflation_adjusted.csv">this</a> repo.</p>

<!-- Load d3.js -->
<script src="https://d3js.org/d3.v5.js"></script>

<div id="linechart"> </div>
<script src="/docs/assets/javascript/linechart_evolution/linechart.js"> </script>

<p>This visualization allow us to see how a quantity (the GDP per capita)
changes over time, and it does that in a decent way.</p>

<h2 id="issues-with-the-line-chart">Issues with the line chart</h2>

<p>But what does it happen when we try and visualize more than one Country in
a single plot?
Let us start by using color to code the Country</p>

<div id="multiple_linechart"> </div>
<script src="/docs/assets/javascript/linechart_evolution/multiple_linechart.js"> </script>

<p>As the number of lines grows, the graph soon becomes more and more cluttered.
Already with a small number of lines it becomes difficult to catch the behavior
of a single line.</p>

<p>We have two main alternatives to the multiple line chart:</p>
<ul>
  <li>we can put one line chart per graph and we create a small multiples</li>
  <li>we can use another channel to encode the temperature
or, of course, we can combine the two techniques.</li>
</ul>

<h2 id="small-multiples">Small multiples</h2>

<p>Broadly speaking, when you build a small multiple you draw more than
one visualization, and each visualization is indexed by a label which is not
used in any of the single images.</p>

<div id="sm_linechart"> </div>
<script src="/docs/assets/javascript/linechart_evolution/sm_linechart.js"> </script>

<p>Here we used small multiples to put one visualization on the right of the previous,
but you can also order them vertically or build a grid.</p>

<p>The main advantage of the small multiples is that they reduce clutter,
but it becomes more difficult to compare the single lines.</p>

<h2 id="stacked-area-chart">Stacked area chart</h2>

<p>Another possible solution is to stack the lines one above the other one,
and this is done in the stacked area chart.</p>

<div id="stacked_chart"> </div>
<script src="/docs/assets/javascript/linechart_evolution/stacked_chart.js"> </script>

<p>The major issue with this solution is that, for all but the lowest curve,
the baseline is not constant, and this makes difficult to quantify the values.</p>]]></content><author><name>Stippe</name><email>smaurizio87@protonmail.com</email></author><category term="course/various/" /><category term="/linechart-evolution/" /><summary type="html"><![CDATA[When a linechart is not enough]]></summary></entry><entry><title type="html">Perception</title><link href="http://localhost:4000/perception/" rel="alternate" type="text/html" title="Perception" /><published>2023-11-01T00:00:00+01:00</published><updated>2023-11-01T00:00:00+01:00</updated><id>http://localhost:4000/perception</id><content type="html" xml:base="http://localhost:4000/perception/"><![CDATA[<p>Before digging into visualizations, we must understand how do we perceive
images.
Typically the human eye can detect light in the range of wavelengths going
from 380 nanometers (violet) to 700 nanometers (red).
Our eye is rather complex, but to our aim we can only consider few components:</p>
<ul>
  <li>The <strong>cornea</strong> which acts as a convergent lens and focuses the light coming to our eye.</li>
  <li>The <strong>iris</strong> which regulates the amount of light entering into our eye.</li>
  <li>The <strong>crystalline lens</strong> changes the focal length of the eye, allowing us to focus on different objects.</li>
  <li>The <strong>retina</strong> which contains the light receptors of our eye.</li>
  <li>The <strong>optic nerve</strong> which transmits the information from the retina to the brain.</li>
</ul>

<p>Into the retina we have two kind of receptors:</p>
<ul>
  <li>The <strong>rod cells</strong> which are very sensitive in low light conditions. They are roughly 90 millions and are especially used into the peripheral vision and night vision. They are especially concentrated at the outer edge of our retina</li>
  <li>The <strong>cone cells</strong> are responsible of the color vision, and are roughly 6 millions. Human eye has three kind of cone cells, and each type is more sensitive into a specific wavelengths range, corresponding approximately to <strong>red</strong>, <strong>blue</strong> and <strong>green</strong> wavelengths and named long, medium and short wavelengths cones.</li>
</ul>

<p>The red cones are approximately ten times the green or blue ones, this is why we are better in discriminating the red tones than the blue or green ones.</p>

<p><img src="/docs/assets/images/perception/Cone-absorbance-en.svg" alt="Cone absorbance" /> The typical spectrum of our light receptors, from <a href="https://en.wikipedia.org/wiki/Rod_cell">Wikipedia</a></p>

<p>The cone density is much higher in a small region located oppositely to the iris,
namely the <strong>fovea</strong>. Approximately half the nerve fibers in the optic nerve
carry information from the fovea, while the remaining half carry information
from the rest of the retina.</p>

<p>This should suggest you that the fovea is the region with the highest resolution
of the retina, and since it is very small our eye can only clearly see within
a very small region.
In fact, our high-resolution region is limited to less than 2 degrees.</p>

<p>On the other hand, on average, we have the feeling that we can clearly see most
of what surrounds us. This is because our eye makes small movements (less than 20 degrees)
named <a href="https://en.wikipedia.org/wiki/Saccade"><strong>saccadic movements</strong></a> with an average time between two movements of 225 ms.
Our brain then elaborates the images and reconstructs a map by using many movements,
giving us the feeling of a higher resolution.</p>

<p>This has a very important impact on data visualization: 
<strong>attention plays  a major role into what we perceive</strong>.</p>

<h2 id="preattentive-features">Preattentive features</h2>

<p>For this reason the scientific community spent a lot of energy in trying and determine
what drives our attention.
According to Colin Ware’s textbooks 
<strong>Information Visualization: Perception for Design</strong> the list of features which
drives our attention, named <strong>preattentive features</strong>, can be divided into
four kind of features: <strong>form, color, motion</strong> and <strong>spatial positioning</strong>.</p>

<p><strong>Form:</strong></p>
<ul>
  <li>Line orientation</li>
  <li>Line length</li>
  <li>Line width</li>
  <li>Line collinearity</li>
  <li>Size</li>
  <li>Curvature</li>
  <li>Spatial grouping</li>
  <li>Blur</li>
  <li>Added marks</li>
  <li>Numerosity</li>
</ul>

<p><strong>Color:</strong></p>
<ul>
  <li>Color hue</li>
  <li>Color intensity</li>
</ul>

<p><strong>Motion:</strong></p>
<ul>
  <li>Flicker</li>
  <li>Direction of motion</li>
</ul>

<p><strong>Spatial positioning:</strong></p>
<ul>
  <li>2D position</li>
  <li>Stereoscopic depth</li>
  <li>Convex/concave shape from shading</li>
</ul>]]></content><author><name>Stippe</name><email>smaurizio87@protonmail.com</email></author><category term="course/various/" /><category term="/perception/" /><summary type="html"><![CDATA[How do we see what surrounds us]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/docs/assets/images/perception/eye.jpg" /><media:content medium="image" url="http://localhost:4000/docs/assets/images/perception/eye.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Fundamental charts</title><link href="http://localhost:4000/fundamental-charts/" rel="alternate" type="text/html" title="Fundamental charts" /><published>2023-10-30T00:00:00+01:00</published><updated>2023-10-30T00:00:00+01:00</updated><id>http://localhost:4000/fundamental-charts</id><content type="html" xml:base="http://localhost:4000/fundamental-charts/"><![CDATA[<p>In this post we will take a look at some of the most fundamental charts
that one encounters in data visualization <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.</p>

<h2 id="categorical-and-quantitative-data">Categorical and quantitative data</h2>

<p>The first kind of graph we will discuss is the bar chart, where we show
the how a quantitative attribute changes across a set of categories.</p>

<p>As an example, we can visualize the number of gold medals
that each country won in the 2020 Olympic games.
Here we will only plot a sub-sample of the dataset, while the full dataset
can be found <a href="https://github.com/MainakRepositor/Datasets/blob/master/Tokyo-Olympics/Medals.csv">here</a>.</p>

<!-- Load d3.js -->
<script src="https://d3js.org/d3.v5.js"></script>

<!-- Create a div where the graph will take place -->
<div id="barchart"> </div>

<script src="/docs//assets/javascript/fundamental_charts/barchart.js"> </script>

<p>In this case the categorical variable is the team, while the quantitative variable
is the number of gold medals.</p>

<p>The bar chart can be rotated by 90 degrees, but the vertical version (which we used)
allows for a larger number of categories to be shown.</p>

<p>If the categories don’t have any natural order it may be a good idea to
reorder the categories with respect to the plotted quantity to improve readability.</p>

<h2 id="relation-between-two-quantitative-variables">Relation between two quantitative variables</h2>

<p>The second kind of graph we will talk about is the
line chart, where you can visualize how does a quantitative variable changes with
respect to another quantity, which often represents time.
To better explain this graph, let us take a look at the gold
price in the period 1978-2021.</p>

<div id="linechart"> </div>
<script src="/docs//assets/javascript/fundamental_charts/linechart.js"> </script>

<p>Line chart is often abused, as the line naturally both encodes order
and a concept of distance between the values in
the x axes, so if x is not a quantitative variable one should
never use the line chart.</p>

<h2 id="scatterplot">Scatterplot</h2>

<p>In a scatterplot we show the distribution of two
quantities across the items.
As an example, here we show how the sepal length and the sepal width are
varying across the items of the well known Iris dataset.</p>

<div id="my_scatterplot"> </div>

<script src="/docs//assets/javascript/fundamental_charts/scatterplot.js"> </script>

<h2 id="matrix-chart">Matrix chart</h2>

<p>In a matrix we want to visualize how does a quantity
distributes across two categorical variables.
As an example, here we visualize how many points each team of the Six Nations Championship
performed against each opponent in the period 2016-2023.</p>

<div id="my_matrix_chart"> </div>

<script src="/docs//assets/javascript/fundamental_charts/matrix.js"> </script>

<p>As we will discuss in a future post, this representation is never optimal,
as the two spatial dimensions are already encoding the categorical
variables, so one must rely on another channel, typically area or color,
to encode the quantitative variable. 
The issue is that our perception of scale variations in both channels
are prone to errors, so one may find difficulties to correctly
decode the quantitative informations.</p>

<h2 id="symbol-map">Symbol map</h2>
<p>The fifth and last type of visualization we will discuss here is the
symbol map, where we show how a quantity varies across two spatial
coordinates.</p>

<p>As an example, here I plot some of the places where I lived, where the area is proportional to the
time I lived in each location.</p>

<script src="https://d3js.org/d3-geo-projection.v2.min.js"></script>

<div id="my_symbol_chart"></div>

<script src="/docs/assets/javascript/fundamental_charts/symbol.js"> </script>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Here we will follow Enrico Bertini’s Coursera lecture on fundamental graphs, which I suggest you to watch together with his entire <a href="https://www.coursera.org/specializations/information-visualization">specialization in data visualization</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Well, to be honest this post is also an excuse to try and see how to draw charts with <a href="https://d3js.org/">D3.js</a>, and up to the moment it looks an amazing tool! <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Stippe</name><email>smaurizio87@protonmail.com</email></author><category term="course/various/" /><category term="/fundamental_charts/" /><summary type="html"><![CDATA[An overview to some of the most common data visualizations]]></summary></entry><entry><title type="html">Beer and the Beta-Binomial model</title><link href="http://localhost:4000/beta-binom/" rel="alternate" type="text/html" title="Beer and the Beta-Binomial model" /><published>2023-10-22T00:00:00+02:00</published><updated>2023-10-22T00:00:00+02:00</updated><id>http://localhost:4000/beta-binom</id><content type="html" xml:base="http://localhost:4000/beta-binom/"><![CDATA[<p>I love beer, and whenever I have a free day I brew. As you probably know, beer is made
with water, malt, hop and yeast. One of the most important things to do in order
to produce a good beer is to have a good quality yeast, and one of the metrics
used to quantify the goodness of the yeast is the <strong>yeast viability</strong>, which corresponds to the percentage of alive cells in your yeast.
This procedure is time consuming, as you must count by hand the number of dead
and alive cells in a sample, so it is usually performed with small samples. It is therefore important to quantify the uncertainties in your estimate.</p>

<p>Unfortunately, most home-brew textbooks will only give you a way to
estimate the mean yeast viability, and you may get fooled by your count and think that
you are working with a good yeast while you simply overestimated the yeast viability.
If you want to know more about how to experimentally count the yeast cells,
you can take a look to <a href="https://escarpmentlabs.com/blogs/resources/crop-pray-count-yeast-counting-guide">this</a>
link, where the procedure to count the yeast cells is illustrated.</p>

<p>In the standard procedure, one has a $5\times 5$ grid and one counts the alive
cells and the death ones, where one can distinguish the cells thanks to the
Trypan Blu which will color the death cells.
A simulated example of what one will see is shown below:</p>

<p><img src="/docs/assets/images/beta_binom/yeast_count.jpg" alt="Alt text" /></p>

<p>Since counting all the cells would require a lot of time, one usually counts
five well separated squares, usually the four corner squares and the center one.
In the figure shown above:</p>

<table>
  <thead>
    <tr>
      <th>square</th>
      <th>alive</th>
      <th>death</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>top left</td>
      <td>15</td>
      <td>2</td>
    </tr>
    <tr>
      <td>top right</td>
      <td>11</td>
      <td>2</td>
    </tr>
    <tr>
      <td>bottom left</td>
      <td>10</td>
      <td>2</td>
    </tr>
    <tr>
      <td>bottom right</td>
      <td>14</td>
      <td>2</td>
    </tr>
    <tr>
      <td>center</td>
      <td>22</td>
      <td>1</td>
    </tr>
    <tr>
      <td><strong>total</strong></td>
      <td>72</td>
      <td>9</td>
    </tr>
  </tbody>
</table>

<p>Let us see how can we estimate the viability.
In the following, we will indicate with $n_a$ the number of alive cells (which is 72)
and with $n_d$ the number of death cells (9).
In order to do this, let us first open our Jupyter notebook, import some libraries
and define the data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>

<span class="c1"># Let us improve the graphics a little bit
</span><span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="nf">use</span><span class="p">(</span><span class="sh">"</span><span class="s">seaborn-v0_8-darkgrid</span><span class="sh">"</span><span class="p">)</span>

<span class="n">cmap</span> <span class="o">=</span> <span class="n">sns</span><span class="p">.</span><span class="nf">color_palette</span><span class="p">(</span><span class="sh">"</span><span class="s">rocket</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># For reproducibility
</span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">alive</span> <span class="o">=</span> <span class="mi">72</span>
<span class="n">death</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">total</span> <span class="o">=</span> <span class="n">alive</span> <span class="o">+</span> <span class="n">death</span>
</code></pre></div></div>

<h2 id="the-home-brewers-textbook-way">The home-brewer’s textbook way</h2>
<p>The home-brewer’s solution is fast and simple: if we have 72
alive cells out of 81 cells, then the probability of having
and alive cell is simply</p>

\[\theta_{hb} = \frac{n_a}{n_a + n_d}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">theta_hb</span> <span class="o">=</span> <span class="n">alive</span> <span class="o">/</span> <span class="n">total</span>
</code></pre></div></div>
<blockquote>
  <p>0.88889</p>
</blockquote>

<p>This is a quick solution, however we cannot associate any uncertainty to this number
for the moment.</p>

<h2 id="the-frequentist-statisticians-way">The frequentist statistician’s way</h2>

<p>A frequentist statistician would first of all setup a model for this problem.
The state of each cell can take two values:</p>

\[y_i = 
\begin{cases}
1 \text{ (alive) } &amp; \text{ with probability } \theta \\
0 \text{ (death) } &amp; \text{ with probability } 1-\theta
\end{cases}\]

<p>If we assume that the probability of being alive of each cell is independent on the probability of the remaining cells
of being alive and that the probability is the same for each cell, we have that the probability of finding $y$ alive cells out of $n$ total counted cells must follow a binomial distribution:</p>

\[p(y|p, n) \propto \theta^{y} (1-\theta)^{n-y}\]

<p>which can be written as</p>

\[y \sim Binomial(\theta, n)\]

<p>where the binomial distribution has probability mass</p>

\[p(y | p, n) = \binom{n}{y} \theta^y (1-\theta)^{n-y}\]

<p>and</p>

\[y = \sum_{i=1}^n y_i\]

<p>and $ \binom{n}{y} = \frac{n!}{y!(n-y)!}$ is a multiplicative normalization factor.
Once the model is built, we want to find $p$ such that the $p(y | \theta, n)$ is maximum, namely the <em>Maximum Likelihood Estimator</em> or MLE for the
sake of brevity.
$p(y | p, n)$ is a positive quantity for $\theta \in (0, 1)$, and this allows us to take its logarithm, which is a monotone increasing function, and 
this implies that the maximum of $\log p$ is the maximum of $p\,.$</p>

\[\log p(y | \theta, n) \propto y \log \theta + (n-y) \log(1-\theta)\]

\[\frac{\partial \log p(y | \theta, n)}{\partial \theta} = \frac{y}{\theta} + \frac{n-y}{\theta-1}\]

\[\left. \frac{\partial \log p(y | \theta, n)}{\partial \theta}\right|_{\theta=\hat{\theta}} = 0 \Rightarrow \frac{y}{\hat{\theta}} = \frac{n-y}{1-\hat{\theta}} \Rightarrow \hat{\theta}(n-y) = (1-\hat{\theta}) y
\Rightarrow \hat{\theta} n = y\]

<p>Which gives us, again, $\hat{\theta} = \frac{y}{n}\,,$ which is the same value that we got by using the home-brewer textbook’s way.</p>

<p>We can easily verify that it is a maximum:</p>

\[\frac{\partial^2 \log p(y | \theta, n)}{\partial \theta^2} = -(n - y)/(\theta - 1)^2 - y/\theta^2\]

\[\left. \frac{\partial^2 \log p(y | \theta, n)}{\partial \theta^2}\right|_{\theta=\hat{\theta}} =
-\frac{n^3}{y (n - y) }\]

<p>and the last quantity is always negative, for $0&lt;y&lt;n\,.$</p>

<p>The frequentist statistician, however, knows that his estimate for the alive cell
fraction is not exact, and he would like to provide an uncertainty interval
associated to the estimate.
He can use the central limit theorem, which says that, if $n$ is large, then the binomial distribution can be approximated with the normal distribution
with the same mean and variance of the binomial distribution, which corresponds to $\mu = n\hat{\theta}$ and $\sigma^2= n\hat{\theta}(1-\hat{\theta})\,.$
He would use this theorem to provide the $95\%$ Confidence Interval for this distribution.</p>

<p>For a normal distribution with mean $\mu$ and variance $\sigma$ the $95\%$ CI
is given by</p>

\[\mu \pm z_{1-0.05/2}\sigma\]

<p>where $z_{1-0.05/2}=1.96$ is the $0.975$ normal quantile.
So we can easily obtain the $95\%$ confidence interval for $\theta$ as</p>

\[\frac{\mu \pm \sigma}{n} = \hat{\theta} \pm  z_{1-0.05/2} \sqrt{\frac{\hat{\theta}(1-\hat{\theta})}{n}} 
 = \hat{\theta} \pm  1.96 \sqrt{\frac{\hat{\theta}(1-\hat{\theta})}{n}}  = [0.84, 0.94]\]

<p>The calculation is quite straightforward, but one should pay a lot of attention in giving the correct interpretation to this interval.
In the frequentist paradigm, one imagines to repeat the experiment many times, and what one can say is that, by doing this,
if the confidence interval is constructed with the procedure given above, in the $95\%$ of the repetitions it will contain the true
fraction of alive cells.
However, it doesn’t tell us anything about the confidence we have that the fraction of alive cells is in the interval $[0.84, 0.94]\,.$</p>

<p><strong>For the frequentist statistician, the probability that the true value lies inside [0.84, 0.94] is either 1 if it is inside 
or 0 if it is not inside, but he cannot say which one is correct!</strong></p>

<p>This fact is often misinterpreted, even by many researchers and data scientists.</p>

<h2 id="the-bayesian-rookies-way">The Bayesian rookie’s way</h2>

<p>The Bayesian statistician would take the same likelihood for the model, however in his framework the parameter $\theta$ is
simply another random variable, and it is described by some other probability distribution $p(\theta)$ namely by the <strong>prior</strong> associated
to the parameter $\theta\,.$</p>

<p>$\theta$ can take any value between 0 and 1, but he has no preference about any value, so he assumes that $\theta$ is distributed
according to the uniform distribution over $[0, 1]\,.$</p>

\[\theta \sim Uniform(0, 1)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">beta_binom_model</span><span class="p">:</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Uniform</span><span class="p">(</span><span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">total</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">alive</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/beta_binom/trace_bb.jpg" alt="Alt text" /></p>

<p>He used PyMC to sample $p$ many times according to its posterior probability distribution,
obtained by using the Bayes theorem</p>

\[p(\theta | y, n) \propto p(y | \theta, n) p(\theta)\]

<p>and the sampled values are those shown in the figure.
The details about how does PyMC’s sampler works will be explained in a future post,
as well as the main methods to exclude problems in the sampling procedure.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_posterior</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/beta_binom/posterior_bb.jpg" alt="Alt text" /></p>

<p>We can see that the mean is very close to the MLE value, and the (Bayesian)
$95\%$ CI (which corresponds to the two printed numbers) is close to
 the frequentist one too.
However in this case the interpretation is straightforward:</p>

<p><strong>the Bayesian statistic simply updated his/her initial guess for $p$ by means of Bayes’ theorem.</strong></p>

<p>For the Bayesian statistician there is the $95\%$ of chance that the true
value of $p$ lies inside the $95\%$ CI associated to $\theta$.</p>

<p>Another major advantage of the Bayesian approach is that we did not had to rely
on the Central Limit Theorem, which only holds if the sample is large enough.
The Bayesian approach is always valid, regardless on the size of the sample.</p>

<h2 id="the-wise-bayesians-way">The wise Bayesian’s way</h2>

<p>The wise Bayesian would follow an analogous procedure, he would however
take the less informative prior as possible, where a strongly informative
prior is a prior which influences a lot the posterior probability distribution.
The uniform distribution is not a very informative distribution.
However, as we will show, we can even choose a less informative prior, namely
the <strong>Jeffreys’ prior</strong> for the binomial distribution</p>

\[\theta \sim Beta(1/2, 1/2)\]

<p>where the Beta has pdf</p>

\[p(\theta | \alpha, \beta) = \frac{1}{B(\alpha, \beta) } \theta^\alpha (1-\theta)^\beta\]

<p>and $B(x, y)$ is the Beta function.
However, he knows he knows he must pay a lot of attention, as often -but not
in this case- the Jeffreys’ prior is not a proper prior
(it cannot be integrated to one) <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">beta_binom_model_wise</span><span class="p">:</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Beta</span><span class="p">(</span><span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">total</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">alive</span><span class="p">)</span>
    <span class="n">trace_wise</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_wise</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/beta_binom/trace_bb_wise.jpg" alt="Alt text" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_posterior</span><span class="p">(</span><span class="n">trace_wise</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/beta_binom/posterior_bb_wise.jpg" alt="Alt text" /></p>

<p>As we can see, this result is almost identical to the previous one.
In the Bayesian framework one can, and should, investigate the goodness
of his results by trying out different priors and assess how
much does the results on his/her inference depend on the choice of the priors.</p>

<h2 id="conclusions-and-take-home-message">Conclusions and take-home message</h2>

<ul>
  <li>PyMC allows you to easily implement Bayesian models.</li>
  <li>In many cases Bayesian statistics offers results which are more transparent than their frequentist counterparts. We have seen this for a very simple model, but this becomes even more evident as the complexity of the model grows.</li>
  <li>You can apply Bayesian statistics to any kind of problem, even home-brewing!</li>
</ul>

<p>In the <a href="/count/">next</a> example we will apply Bayesian statistics to study
data which can take more than two values.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>This topic will be discussed in a future post. For the moment, if you are curious, you can take a look at the <a href="https://en.wikipedia.org/wiki/Jeffreys_prior#">Wikipedia</a> page. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Stippe</name><email>smaurizio87@protonmail.com</email></author><category term="course/intro/" /><category term="/beta-binom/" /><summary type="html"><![CDATA[How to describe dichotomous variables]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/docs/assets/images/beta_binom/bar-209148_960_720.jpg" /><media:content medium="image" url="http://localhost:4000/docs/assets/images/beta_binom/bar-209148_960_720.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Why (and when) should you go for Bayesian</title><link href="http://localhost:4000/intro-when-go-bayesian/" rel="alternate" type="text/html" title="Why (and when) should you go for Bayesian" /><published>2023-10-20T00:00:00+02:00</published><updated>2023-10-20T00:00:00+02:00</updated><id>http://localhost:4000/intro-when-go-bayesian</id><content type="html" xml:base="http://localhost:4000/intro-when-go-bayesian/"><![CDATA[<p>I feel I am quite a pragmatic person, so I prefer choosing my tools depending on my needs rather than by relying on some personal believes.
Bayesian statistics allows you to build custom and structured models by simply specifying the data generating process.
The model can be divided into two parts:</p>
<ul>
  <li>The likelihood $p(y \vert \theta)$, which determines how the data you want to model $y$ are generated given the parameter(s) $\theta$.</li>
  <li>The priors $p(\theta)$, which specifies your initial hypothesis about the distribution of the parameters of the model.</li>
</ul>

<p>The only mathematical requirements for both the likelihood and for the priors is that they are non-negative and sum up to one.
There is a huge literature about the model building, and you can easily start by using one of the already available models and adapt it
to your problem.</p>

<p>Once that the model is specified you can use $PyMC$ or any other Probabilistic Programming Language
sample the entire posterior probability distribution,
which is determined by means of Bayes theorem.</p>

\[p(\theta \vert y) = \frac{p(y \vert \theta) p(\theta)}{p(y)} \propto  p(y \vert \theta) p(\theta)\]

<p>Here $\propto$ means proportional to, which means equal up to some multiplicative positive constant,
where by constant we mean independent on $\theta$.
The constant $p(y)$ can be fixed by requiring that $p(\theta \vert y)$ is normalized to one:</p>

\[1 = \int d\theta p(\theta | y) = \frac{1}{p(y)}\int d\theta p(y \vert \theta) p(\theta)\]

<p>so</p>

\[p(y) = \int d\theta p(y|\theta)p(\theta)\,.\]

<p>The fact that you sample the entire probability distribution $p(\theta \vert y)$
makes Bayesian statistics very attractive if you are building a statistical
model to make a decision, as you can easily make inference about any kind of quantity
regarding your model.
This is rarely possible if you only have a point estimate or an interval estimate, as it happens in Machine Learning.</p>

<p>Moreover, Bayesian statistics is easily interpretable: what you are doing
is simply to use the data to update your initial believes.
In fact, in the Bayesian interpretation, $p(\theta)$ represents your opinion about the possible
values that $\theta$ may take before you make an experiment and observe $y\,.$
On the other hand $p(\theta \vert y)$ represents your updated opinion about the value of $\theta$
after the experiment.</p>

<p>So why is not everyone using it? In my experience there are multiple reasons, some of them
are historical, others are more pragmatic.</p>

<p>First of all, the possibility to easily implement a numerical simulation
and to run it within a reasonable amount of time is relatively recent and not
yet spread outside the statistical community.
Bayesian statistics was the only available framework up to the end of the nineteenth 
century, and in has been largely abandoned at the beginning of the last century,
when Fisher and his collaborators developed frequentist statistics.
People in fact considered Bayesian statistics very difficult,
as the normalization factor $p(y)$
can only be computed for a very limited number of models
(the so-called <em>conjugate</em> models).
De Finetti, Savage, Jeffreys and others tried to convince
people to abandon the frequentist framework as they did not considered the
frequentist interpretation satisfactory, but they never managed to convince
the majority of the community.
Things changed when, during the Manhattan project, Metropolis, Von Neumann, Ulam and
others invented the
Markov Chain Monte Carlo methods, and this allowed physicists and later statisticians
too to 
draw random samples from an arbitrary probability distribution.
Moreover, nowadays, the misuse and misinterpretation of tools of frequentist statistics is considered 
one of the main reason for the so-called <em>reproducibility crisis in science</em>
and a <strong>proper</strong> use of Bayesian methods is considered a valid alternative to those
tools <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> <sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>.
Of course, Bayesian statistics can be misused too, but there are few very clear guidelines from the academic community which will make this less likely to happen.
Moreover, in most cases, a problem in your model will show up in a problem in your simulation, and this makes Bayesian inference less error-prone than frequentist inference.
In fact, when talking about frequentist statistics or machine learning, most of the time what you are computing
is either an optimization problem or the average of some quantity.
Since what you obtain from this kind of procedure is a number rather than a sample,
in this kind of task may be quite hard to spot.</p>

<p>However, the major practical drawback of Bayesian statistics is that you need to run your
simulation thousands of times, and this may take some time if the number
of parameters in your model is large.
Thus, I do not reccomend you to use Bayesian statistics if your task is a simple
and fast fit-predict problem.</p>

<p>There is another drawback, and this is where these notes come into play:
building a model without a basic knowledge about model building and assessment
is not an easy task. There are many beautiful online courses about Bayesian
statistics in R, which is the most common programming language between statisticians.
The choice of the programming language implies that either you already know R, or you need to learn Bayesian
statistics <em>and</em> R.
This blog is written to make this task simpler for anyone who has a basic knowledge
about Python, and since Python is the most widely spread programming language
in the World, I hope I will help a lot of people.</p>

<p>I hope you enjoyed,</p>

<p>Stefano</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>This is somehow a misleading name, as this crisis is not only affecting academia, but it is a problem in industry too. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>See <a href="https://www.nature.com/articles/533452a">this article on Nature</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>See <a href="https://www.nature.com/articles/520612a">this other article of Nature</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Stippe</name><email>smaurizio87@protonmail.com</email></author><summary type="html"><![CDATA[Because there is no silver bullet]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/docs/assets/images/intro/doubt.jpg" /><media:content medium="image" url="http://localhost:4000/docs/assets/images/intro/doubt.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Marks and channels</title><link href="http://localhost:4000/marks-channels/" rel="alternate" type="text/html" title="Marks and channels" /><published>2023-10-14T00:00:00+02:00</published><updated>2023-10-14T00:00:00+02:00</updated><id>http://localhost:4000/marks-channels</id><content type="html" xml:base="http://localhost:4000/marks-channels/"><![CDATA[<p>When we build a data visualization we are building vocabulary
to translate our data into a message, and this vocabulary can be decomposed in 
a certain number of elements.
We always do that, even if you may not conscious about it.
Being aware of this helps us to think about how to better build this vocabulary
and, ultimately, how to make your message more effective.</p>

<p>Here we will discuss these components, together with the
fundamental design principles of data visualization, which are a set of 
guidelines to help us finding the most appropriate representation for our data.</p>

<h2 id="marks-and-channels">Marks and channels</h2>

<p>Marks and channels are the first two fundamental building blocks of a
data visualization.</p>

<h3 id="marks">Marks</h3>
<p>Marks are the geometric elements that we use to identify the items
of our dataset <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.
Since we are plotting on a two dimensional surface, we can use any geometric entity
with dimensionality less or equal to two to represent our items, so we can use:</p>

<svg height="180" width="95%" style="margin:2%; font-weight: bold">
<text x="30" y="15"> POINTS </text>
  <circle cx="20" cy="40" r="10" fill="steelblue" />
  <circle cx="45" cy="45" r="10" fill="steelblue" />
  <circle cx="70" cy="55" r="10" fill="steelblue" />
  <rect x="100" y="25" width="20" height="20" fill="steelblue" />
  <rect x="100" y="50" width="20" height="20" fill="steelblue" />

<text x="315" y="15"> LINES </text>
<line x1="270" y1="40" x2="420" y2="40" style="color:black; width:20px; stroke:black; stroke-width:3px" />
<line x1="270" y1="80" x2="420" y2="60" style="color:black; width:20px; stroke:black; stroke-width:3px" />
<line x1="270" y1="100" x2="420" y2="120" style="color:black; width:20px; stroke:black; stroke-width:3px" />


<text x="630" y="15"> AREAS </text>
<rect x="600" y="25" width="120" height="80" fill="steelblue" style="stroke:black; stroke-width:3px" />
<rect x="600" y="25" width="80" height="80" fill="steelblue" style="stroke:black; stroke-width:3px" />
<rect x="600" y="25" width="80" height="40" fill="steelblue" style="stroke:black; stroke-width:3px" />
</svg>

<div class="emphbox">
Someone also uses three dimensional objects as markers.
Please, don't! 
<br />
Their encoding implies that we must model the perspective,
and this causes a distortion into our perceived values, making them not suitable
for data visualization.
</div>

<h3 id="channels">Channels</h3>
<p>We then have the channels, and they encode the values of the data associated to our items.</p>

<p>The most commonly channels used to encode quantitative information are</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right">Channel</th>
      <th style="text-align: left">Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">Position on aligned axis</td>
      <td style="text-align: left"><svg height="50" width="200"><line x1="10" y1="10" x2="200" y2="10" style="color:black; width:20px; stroke:black; stroke-width:3px"></line> <circle cx="45" cy="10" r="10" fill="grey"></circle> <line x1="10" y1="30" x2="200" y2="30" style="color:black; width:20px; stroke:black; stroke-width:3px"></line> <circle cx="125" cy="30" r="10" fill="grey"></circle></svg></td>
    </tr>
    <tr>
      <td style="text-align: right">Position on unaligned axis</td>
      <td style="text-align: left"><svg height="50" width="200"><line x1="10" y1="10" x2="160" y2="10" style="color:black; width:20px; stroke:black; stroke-width:3px"></line> <circle cx="45" cy="10" r="10" fill="grey"></circle> <line x1="50" y1="30" x2="200" y2="30" style="color:black; width:20px; stroke:black; stroke-width:3px"></line> <circle cx="125" cy="30" r="10" fill="grey"></circle></svg></td>
    </tr>
    <tr>
      <td style="text-align: right">Length</td>
      <td style="text-align: left"><svg height="50" width="200"><line x1="10" y1="10" x2="200" y2="10" style="color:black; width:20px; stroke:black; stroke-width:3px"></line>  <line x1="10" y1="30" x2="150" y2="30" style="color:black; width:20px; stroke:black; stroke-width:3px"></line> </svg></td>
    </tr>
    <tr>
      <td style="text-align: right">Width</td>
      <td style="text-align: left"><svg height="50" width="200"><line x1="10" y1="10" x2="200" y2="10" style="color:black; width:20px; stroke:black; stroke-width:3px"></line>  <line x1="10" y1="30" x2="200" y2="30" style="color:black; width:30px; stroke:black; stroke-width:5px"></line> </svg></td>
    </tr>
    <tr>
      <td style="text-align: right">Angle/Slope</td>
      <td style="text-align: left"><svg height="50" width="200"><line x1="10" y1="10" x2="70" y2="10" style="color:black; width:20px; stroke:black; stroke-width:3px"></line>  <line x1="10" y1="10" x2="70" y2="30" style="color:black; width:20px; stroke:black; stroke-width:3px"></line> <line x1="150" y1="10" x2="200" y2="10" style="color:black; width:20px; stroke:black; stroke-width:3px"></line>  <line x1="150" y1="10" x2="200" y2="40" style="color:black; width:20px; stroke:black; stroke-width:3px"></line></svg></td>
    </tr>
    <tr>
      <td style="text-align: right">Area</td>
      <td style="text-align: left"><svg height="50" width="200"> <circle cx="20" cy="20" r="10" fill="grey"></circle> <circle cx="100" cy="20" r="15" fill="grey"></circle> <circle cx="180" cy="20" r="20" fill="grey"></circle> </svg></td>
    </tr>
    <tr>
      <td style="text-align: right">Color luminance</td>
      <td style="text-align: left"><svg height="50" width="200"> <rect x="5" y="5" height="40" width="40" fill="lightgray"></rect> <rect x="85" y="5" height="40" width="40" fill="gray"></rect><rect x="160" y="5" height="40" width="40" fill="black"></rect>  </svg></td>
    </tr>
    <tr>
      <td style="text-align: right">Color saturation</td>
      <td style="text-align: left"><svg height="50" width="200"> <rect x="5" y="5" height="40" width="40" fill="#61679e"></rect> <rect x="85" y="5" height="40" width="40" fill="#3644c9"></rect><rect x="160" y="5" height="40" width="40" fill="#0019ff"></rect>  </svg></td>
    </tr>
  </tbody>
</table>

<p>On the other hand, if we want to encode a categorical information, we have the following
channels:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right">Channel</th>
      <th style="text-align: left">Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">Spatial region</td>
      <td style="text-align: left"><svg height="60" width="200"> <rect x="5" y="5" height="40" width="40" fill="gray"></rect> <rect x="85" y="15" height="40" width="40" fill="gray"></rect><rect x="160" y="5" height="30" width="30" fill="gray"></rect>  </svg></td>
    </tr>
    <tr>
      <td style="text-align: right">Color hue</td>
      <td style="text-align: left"><svg height="50" width="200"> <rect x="5" y="5" height="40" width="40" fill="#a31919"></rect> <rect x="85" y="5" height="40" width="40" fill="#19a319"></rect><rect x="160" y="5" height="40" width="40" fill="#1919a3"></rect>  </svg></td>
    </tr>
    <tr>
      <td style="text-align: right">Shape/Texture</td>
      <td style="text-align: left"><svg height="50" width="200"> <rect x="5" y="5" height="40" width="40" fill="grey"></rect> <circle cx="105" cy="25" r="20" fill="grey"></circle> <polygon points="180,45 160,5 200,5" fill="grey"></polygon> </svg></td>
    </tr>
  </tbody>
</table>

<p>The order of the items in the above tables is not random, but it reflects how easily we
translate the visual information either into a quantity or into different categories,
and this property is called <strong>effectiveness</strong>.</p>

<h3 id="other-components">Other components</h3>

<p>Marks and channels are the components which encode information about our data, but
they are not the only components which constitute a data visualization.
Other components are the ones which allow us to interpret, compare and give context
to our data.</p>

<ul>
  <li>axis</li>
  <li>grids</li>
  <li>annotations</li>
  <li>legends</li>
  <li>labels</li>
  <li>ticks</li>
  <li>reference lines</li>
</ul>

<div class="emphbox"> Annotations may also be used to draw attention to patterns of interest.</div>

<p>Use this components only if they really help your reader.
Remember that, most of the time, you want the reader to easily compare the values of your data, not to be able to assess the exact value of
your data.
You should keep your visualization as clean as possible, and in order to do so:</p>

<ul>
  <li>Avoid useless boxes.</li>
  <li>Don’t use grids if they don’t help understanding the data.</li>
  <li>Avoid too many ticks <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.</li>
</ul>

<h2 id="design-principles">Design principles</h2>

<p>The above mentioned concept should be applied, as much as possible, according to the
principles of data visualization design.
These principles, that we will discuss in the rest of this post, will allow the reader
to easily understand and decode the visualization, reducing the risk of a misinterpretation and so making the communication between you and your reader clearer.</p>

<h3 id="expressiveness-principle">Expressiveness principle</h3>
<p>The expressiveness principle essentially states that we should make the message
as clear as possible, without neglecting information and, probably most important,
without adding, either implicitly or explicitly, information.</p>

<div class="emphbox">
The visual representation should represent all and
only the relations that exist in the data.
</div>

<p>Some examples of violation to this principle are:</p>
<ul>
  <li>Line chart used to represent categorical data</li>
  <li>Different color intensities to encode different categories</li>
  <li>A diverging color map to represent a quantity that does not have an origin (a zero).</li>
  <li>Using a channel that don’t encode any data.</li>
</ul>

<h3 id="effectiveness-principle">Effectiveness principle</h3>
<p>The effectiveness principle is another principle that
helps us finding the most appropriate channel for each variable.</p>

<div class="emphbox">
The relevance of the information should match the effectiveness of the channel.
</div>

<p>The first obvious consequence of this principle is that the most important variable
should always be encoded by using a spatial dimension.
On the other hand, one should not rely on color to effectively communicate
relevant quantities.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We have first seen what are the main components of any data visualization.
We have then mentioned two important recommendations on how to translate our dataset into
a graph.</p>

<p>You should always keep them in mind, but you should also balance them by taking
into account your audience and your message.</p>

<!-- 
Train!

Establish visual hierarchy (play with contrast, brightness)
data pops, legends and rest of frame supports
put the accent on the relevant data points
don's put boxes. Work with alignments and grids
choose right font

Let you go
Use your hands/easy sketching software (at the beginning you want speed, not perfection)

Review critically
-->

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>If we are representing a network rather than a tabular dataset, they are used to represents the links too. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>The ticks are the graphical components that are used to mark the values of the axis. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Stippe</name><email>smaurizio87@protonmail.com</email></author><category term="course/various/" /><category term="/marks-channels/" /><summary type="html"><![CDATA[The building blocks of data visualization]]></summary></entry><entry><title type="html">Data visualization</title><link href="http://localhost:4000/dataviz/" rel="alternate" type="text/html" title="Data visualization" /><published>2023-10-11T00:00:00+02:00</published><updated>2023-10-11T00:00:00+02:00</updated><id>http://localhost:4000/dataviz</id><content type="html" xml:base="http://localhost:4000/dataviz/"><![CDATA[<p>I started to plot data since many years, but only few months ago I really went into
data visualization (or dataviz, if you prefer).
I attended some Coursera course of prof. Enrico Bertini, who also has a very interesting
podcast named <a href="https://datastori.es/">Data Stories</a> together with Moritz Stefaner (who is the author, among the other things,
of some of my favourite illustrations on Scientific American).
The course allows you design and decide what is the best data visualization by
starting from what is currently known about our brain’s processes in visual perception.</p>

<p>Since I discovered this research field I started reading books talking about this field,
and I began with the ones written written by the data visualization pioneer <a href="https://it.wikipedia.org/wiki/Edward_Tufte">Edward Tufte</a>.</p>

<p>I would like to share some (hopefully intelligent) though about data visualization.
Moreover, as Tufte himself wrote in his book Beautiful Evidence, “as teachers know, a very good way to learn something is to teach it”, so let’s try.</p>

<h2 id="data-visualization-in-the-scientific-process">Data visualization in the scientific process</h2>
<p>Data visualization plays a major role in most phases of the scientific processes:</p>
<ul>
  <li>In the exploratory phase, when one wants to understand the structure of the data and find recurrent patterns</li>
  <li>During the model building, when one wants to compare the model with the data</li>
  <li>When presenting the results to the audience</li>
</ul>

<p>Let us first clarify that, although you will be tempted to use the figures from the first phases in the last phase, you shouldn’t
do that.
In the first phases what you are trying to do is to understand something by scraping the data and looking at the data from different
perspectives.
On the other hand, the aim of the last phase is to summarize what you found by using data visualization.
Since you have two different objectives, you should consider using different tools and languages to do that, and the (graphical)
language you should use in the last phase strongly depends on your audience.</p>

<p>When trying to understand the structure of your data, any quantitative assessment will be helpful, while you won’t likely
put annotations on your plots.
If you want your audience to easily catch some particular message from your data, annotations may be helpful,
and maybe the audience you are presenting your data to is not able to give a broader context to the numerical values of the data,
so adding the ticks may be superfluous.</p>

<p>Moving the axis away from zero in the exploratory phase is common if one wants to clearly spot any kind of pattern in the data,
but doing so in the explanatory context might be dangerous, as this could be seen as an attempt to trick your audience.</p>

<p>In this blog, when talking about data visualization, unless otherwise specified we will usually refer to explanatory data visualization.</p>

<h2 id="why-you-should-use-it">Why you should use it</h2>

<p><img src="/docs/assets/images/visualization/cat.png" alt="An image from a Computerized Axial Tomography" /></p>

<p>Data visualization became a part of everyday life, but by using it
in the wrong way one may encounter many risk.
The above image is taken from the Wikipedia page of the CT scan,
and it uses a very common color palette, the well known rainbow color map.
Unfortunately, in the dataviz community, it is well known that this is
one of the worst color map you could use when you want your
audience to be able and quickly quantify the numerical value
associated with the color.</p>

<p>If your audience is not able to assess the numerical variation associated
to a color variation, they may take wrong decisions such as giving the wrong
therapy or making the wrong investment as they cannot properly
quantify the risk.</p>

<p>This is simply an example, but the more data visualization enters in our
life, the more are the risks associated to a miscommunication problems
due to improper visualizations.</p>

<p>Data visualization is about assessing the quality of a visualization
from a scientific point of view, and it relies on what is known about
how our brain perceives images.</p>

<p>In the following we will give an overview about this topic, but the reader
should keep in mind that this is not a blog about neuroscience, so we will
limit our discussion to what is necessary in order to achieve our
goal.</p>

<!--

Before doing so it's better to put here some vocabulary.

A data visualization is first of all made by **markers**, namely the graphical
objects that we use to represent our items.
The most common markers are:
- points
- lines
- bars
- areas

For each item we will represent some quantity, and we will do so by using one or more
**visual channels** like:
- position
- size (length/width/area)
- angle/slope
- color hue
- color intensity
- shape and textures

Other fundamental components of the visualization are the components which allow
us to contextualize and interpret the visualization.
Those components can be geometric components like axes, grids, reference lines
but also textual components such as labels and annotations.

A visual representation is a combination of such components, and you will find a huge variety visual representations,
as there is a potentially infinite number of ways to combine these ingredients.
So how to choose one? Which is the best?

As often happens, there is not **the best** representation, as we already said elsewhere in this blog, there is no silver bullet.
A better question is

> What is the most appropriate way to visualize this aspect of the data?

This of course depends on many factors, and you will often find yourself in a situation where you simply
have to decide how to balance your needs.
Data visualization is the discipline which wants to address to this question from a scientific perspective.

I will try and share some resources about this topic in some future post, as well as to share some hopefully interesting personal thoughts.

<div id="tester" style="width:900px;height:900px;"></div>
<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
<script>
	TESTER = document.getElementById('tester');
	Plotly.newPlot( TESTER, [{
	y: ["Albania","Bosnia and Herzegovina","Bulgaria","Croatia","Czechia","Estonia","Hungary","Kosovo","Latvia","Lithuania","North Macedonia","Montenegro","Poland","Romania","Serbia","Slovakia","Slovenia","Armenia","Azerbaijan","Belarus","Georgia","Moldova","Russia","Ukraine","Austria","Belgium","Cyprus","Denmark","Finland","France","Germany","Greece","Iceland","Ireland","Italy","Luxembourg","Malta","Netherlands","Norway","Portugal","Spain","Sweden","Switzerland","United Kingdom"],
	x: [296.6,187.7,1336.5,1341.2,3707.1,753.5,2774.0,108.0,819.5,1656.1,230.9,97.7,16818.9,5161.1,1443.5,2003.0,758.9,634.3,2664.8,792.2,292.7,40.1,71981.1,43983.2,3783.9,7045.0,514.6,5737.7,5089.5,56999.7,57807.7,8347.5,0.0,1207.7,34627.5,585.7,92.4,15670.8,8960.3,3647.6,20979.2,8491.5,6241.2,69998.7],
    type: "bar",
    orientation: "h",
    transforms: [{
    type: 'sort',
    target: 'x',
    order: "ascending"
    }]
    }], {
	margin: { t: 0 } } );
</script>
-->]]></content><author><name>Stippe</name><email>smaurizio87@protonmail.com</email></author><category term="course/various/" /><category term="/dataviz/" /><summary type="html"><![CDATA[How to help people understanding you]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/docs/assets/images/visualization/London_Tube_Map.png" /><media:content medium="image" url="http://localhost:4000/docs/assets/images/visualization/London_Tube_Map.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The frequentist perspective</title><link href="http://localhost:4000/estimation/" rel="alternate" type="text/html" title="The frequentist perspective" /><published>2023-10-01T00:00:00+02:00</published><updated>2023-10-01T00:00:00+02:00</updated><id>http://localhost:4000/estimation</id><content type="html" xml:base="http://localhost:4000/estimation/"><![CDATA[<p>While most of this blog is about Bayesian statistics, in this post
we will try and give an overview to some of the most relevant concepts
about frequentist statistics.</p>

<p>We generally assume that we are trying and make some statements about the properties
of a population $\mathbb{P}$.
In <strong>parametric inferential statistics</strong> we assume that our population
is distributed according to some family of distributions.
As an example, we could assume that our population is distributed according to
a normal distribution with unknown mean and known variance, and we would like to determine
if the mean of the distribution is somehow compatible with zero.</p>

<p>There are many ways to build those family of distributions, and one of the
most useful is the <strong>exponential family form</strong>.
For this kind of families we assume that the probability distribution
function takes the form</p>

\[p(x \vert \theta) = h(x) g(\theta)e^{T(x)\eta(\theta)}\,.\]

<p>We define a <strong>sufficient statistic</strong> a statistic (which is nothing
but a quantity that can be computed from the data) such that no other statistic
can provide additional informations about our distribution.
As an example, in our previous example, a sufficient statistics
for the mean of the distribution $\mu$ is the arithmetic mean of the population:</p>

\[T(x) = \frac{1}{N}\sum_{j=1}^N x_i\,.\]

<p>A necessary and sufficient condition for a distribution family to admit a sufficient statistics is to be an exponential family distribution,
and in this case the sufficient statistics is $T(x)$.</p>

<p>In this context, $\eta$ is called the <strong>natural parameter</strong> of the distribution family,
while one refers to $e^{A(\eta)}$ as the partition function.</p>

<p>In the jargon one often refers to the distribution family as the distribution
itself. This is not very precise, as the distribution is an element
of the distribution family, but it is commonly accepted.</p>

<p>In most case we won’t deal with the entire population, but only with a (possibly random)
sub-sample of it \(\{X_1,...,X_n\}\).
In this case we cannot exactly calculate the parameter of interest, but we can only
<strong>estimate</strong> it, and this is why we refer to the parameter as the <strong>estimand</strong>.
We call an <strong>estimator</strong> a map from the sample space to the space of the estimates.</p>

<p>In our usual example, the arithmetic mean of the sample is an estimator of the 
parameter $\mu$.</p>

<p>Given an estimator \(\hat{\theta}\) of a parameter $\theta$ we define its bias as</p>

\[E[\hat{\theta}-\theta]\]

<p>We say that an estimator is unbiased it the bias is zero.</p>

<p>Let us consider a sample of $n$ iid normally distributed observations
with mean $\mu$ and variance $\sigma^2$.</p>

<p>The arithmetic mean of the sample,
defined as
\(\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i\)
is an unbiased estimator of the population mean $\mu\,,$ w</p>

<p>On the other hand, the uncorrected estimator for the sample variance,
defined as</p>

\[S^2 = \frac{1}{n}\sum_{i=1}^n (X_i - \bar{X})^2\]

<p>is a biased estimator, while the unbiased estimator is $\frac{n}{n-1}S^2$.</p>

<p>There are two kind of estimates: <strong>point estimates</strong> and <strong>interval estimates</strong>.
In the case of point estimate, the estimate space has the same dimension
of the parameter space, as the estimate provides a point in the parameter
space.
The main issue of point estimates is that they provide no information
about the uncertainty that is associated with the estimate itself.</p>

<p>Confidence interval/region estimates, on the other hand, provide an interval/region
(depending if we are working with a one dimensional parameter space or with a
higher dimensional space).
More precisely, a confidence interval for $\theta$ with confidence level
$\gamma = 1-\alpha$ is an interval $(u(X), v(X))$ such that</p>

\[P(u(X)\leq \theta \leq v(X)) = \gamma\]

<p>We should always keep in mind that our parameter $\theta$ is given,
and our confidence interval does not tell us anything about the probability
for $\theta$ of being inside the interval.
All we can say is that, if we repeat an experiment many times and each time
we compute the confidence interval for the sample, a fraction
of times $\gamma$ our confidence interval will contain the true parameter $\theta$.</p>]]></content><author><name>Stippe</name><email>smaurizio87@protonmail.com</email></author><category term="course/appendices/" /><category term="/estimation/" /><summary type="html"><![CDATA[Some common misconceptions]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/docs/assets/images/frequentist/freq_xkcd.jpg" /><media:content medium="image" url="http://localhost:4000/docs/assets/images/frequentist/freq_xkcd.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Introduction to causal graphs</title><link href="http://localhost:4000/causal-graphs/" rel="alternate" type="text/html" title="Introduction to causal graphs" /><published>2023-09-04T00:00:00+02:00</published><updated>2023-09-04T00:00:00+02:00</updated><id>http://localhost:4000/causal-graphs</id><content type="html" xml:base="http://localhost:4000/causal-graphs/"><![CDATA[<p>Causality is a relation between events, and one of the easiest to interpret
representations of items and relations is by using graphs.
In our discussion we will roughly follow (in a slightly less rigorous and precise way) chapter 3 of <a href="https://www.bradyneal.com/Introduction_to_Causal_Inference-Dec17_2020-Neal.pdf">Brady Neal’s notes</a>.
Since our relation, causality, is directional (either $A$ causes $B$ or $B$ causes $A$)
we must use directed graph or digraphs.</p>

<p>Formally a digraph $\mathcal{G}$ is a pair $(V, E)$ where $V$ represents
the collection of the vertices</p>

\[V=\{x_1, x_2,...,x_N\}\]

<p>The elements of $V$ are also called nodes or points.</p>

<p>$E$ is the collection of the edges</p>

\[E \subseteq \{(x, y) \in V \times V | x \neq y\}\]

<p>We must make a further requirement: we must forbid cases where
$A$ causes $B$, $B$ causes $C$ and $C$ causes $A$, as it doesn’t make sense
to have a circular causality relation. We thus must represent the causality
relation as a <strong>Directed Acyclic Graph</strong> or DAG.</p>

<p>Of course, if we only have 2 elements, either there is a relation between them
or there isn’t, so the only possible set of edges between $x$ and $y$
are</p>

\[\{\}\]

\[\{(x, y)\}\]

\[\{(y, x)\}\]

<p>In our representation, we assume that <strong>every parent is direct cause of its children</strong>,
so in the first case we have that $x$ and $y$ are independent,
in the second one we have that $x$ causes $y$, while in the last one we have that
$y$ causes $x$.</p>

<p>In terms of probabilities, we have that the probability can be represented
 as $p(x)p(y)$, $p(x)p(y \vert x)$ or $p(y)p(x \vert y)$ respectively.</p>

<h2 id="building-blocks">Building blocks</h2>

<p>Let us take a look at what we might have with three vertices and two arrows:</p>

<p>The first possible case is called the <strong>chain</strong>, such that $p(x, y, z)=p(x)p(y\vert x)p(z\vert y)$</p>

<p style="text-align: center;"><img src="/docs/assets/images/causal_graphs/test-1.svg" alt="The chain" width="450" /></p>

<p>We then have the <strong>fork</strong>, where $p(x, y, z) = p(y) p(x \vert y) p(z \vert y)$</p>

<p style="text-align: center;"><img src="/docs/assets/images/causal_graphs/test-2.svg" alt="The chain" width="280" /></p>

<p>And we finally have the <strong>immorality</strong> $p(x, y, z) = p(x)p(z)p(y\vert x, z)$</p>

<p style="text-align: center;"><img src="/docs/assets/images/causal_graphs/test-3.svg" alt="The chain" width="280" /></p>

<h2 id="association-flow">Association flow</h2>

<p>Let us check the flow of association for the three graphs.
For the chain we have that, when $x$ changes we have a change in $y$, and a change
in $y$ causes a change in $z$, so we will see some association flow between $x$ and $z$.</p>

<p>For the fork, analogously, we have that a change in $y$ will cause both
a change in $x$ and in $z$, so they will change together and we will see some
flow of association.</p>

<p>Vice versa, in the case of immorality, $x$ and $z$ will vary in a totally independent
way, and we will generally not see an association between them.</p>

<p>Let us see this mathematically.</p>

<p>For the chain:</p>

\[\begin{align}
&amp;
p(x, y, z) = p(x) p(y\vert x) p(z \vert y)
\\
&amp;
p(x, z) = \int dy p(x, y, z) = p(x) \int dy p(y \vert x) p(z \vert y) = p(x) p(z \vert x) \neq p(x)p(z)
\end{align}\]

<p>So if we simply ignore the intermediate variable $y$, we see an association between $x$ and $z$.
Analogously, for the fork:</p>

\[\begin{align}
&amp;
p(x, y, z) = p(y) p(x\vert y) p(z \vert y)
\\
&amp;
p(x, z) = \int dy p(x, y, z) = \int dy p(y) p(x \vert y) p(z \vert y)  = \int dy p(x) p(y \vert x) p(z \vert y) = p(x) \int dy p(y \vert x) p(z \vert y) = p(x) p(z \vert x) \neq p(x) p(z)
\end{align}\]

<p>For the immorality, on the other hand:</p>

\[\begin{align}
&amp;
p(x, y, z) = p(x) p(z) p(y \vert x, z)
\\
&amp;
p(x, z) = \int dy  p(x) p(z) p(y \vert x, z) =  p(x) p(z) \int dy p(y \vert x, z) = p(x) p(z)
\end{align}\]

<h2 id="blocking">Blocking</h2>

<p>Let us now see what happens when we block for (or control) $y$:</p>

\[\begin{align}
&amp;
p(x, y, z) = p(x) p(y\vert x) p(z \vert y)
\\
&amp;
p(x, z | y) = \frac{ p(x) p(y \vert x)  }{p(y)} p(z \vert y) = p(x \vert y) p(z \vert y)
\end{align}\]

<p>The last equality follows from Bayes theorem $p(y \vert x) p(x) = p(x \vert y) p(y)\,.$
We can now prove unconfoundedness:</p>

\[p(z \vert x, y) = \frac{p(x, z \vert y)}{p(x \vert y)} = p(z \vert y)\]

<p>Let us now check the same for the fork:</p>

\[\begin{align}
&amp;
p(x, y, z) = p(y) p(x\vert y) p(z \vert y)
\\
&amp;
p(x, z \vert y) = \frac{p(x, y, z)}{p(y)} = p(x \vert y) p(z \vert y)
\\
&amp;
p(z \vert x, y) = \frac{p(x, z \vert y)}{p(x \vert y)} = p(z \vert y)
\end{align}\]

<p>This cannot be done for the immorality:</p>

\[\begin{align}
&amp;
p(x, y, z) = p(x) p(z) p(y\vert x, z)
\\
&amp;
p(x, z \vert y) = p(x) \frac{p(y \vert x, z)p(z)}{p(y)} = p(z) p(z\vert x, y)
\end{align}\]

<p>We can now compute</p>

\[p(z \vert y) = \int dx p(x, z \vert y) = p(z) \int dx p(z \vert x, y) \neq p(z \vert x, y)\]

<p>This implies that controlling for an immorality introduces an association between
the variables.</p>

<h2 id="types-of-paths-and-d-separation">Types of paths and d-separation</h2>

<p>We can now consider an arbitrary path \(\{x_1, x_2, ..., x_N\}\),
where $x_2,…,x_{N-1}$ are the central vertices of either forks or chains or
inverted chains.
We say that the path is <strong>d-separated</strong> by a set of blocked nodes $C$
if</p>
<ul>
  <li>the path contains a chain (or fork), and the middle vertex of the chain (fork) is in $C$</li>
  <li>or if the path contains an inverted chain, and nor the middle vertex of the inverted fork $Z$ neither any descendant of $Z$ is in $C$.</li>
</ul>

<p>We say that two vertices $A$ and $B$ are <strong>blocked</strong> by a set of nodes $C$
if all the paths from $A$ to $B$ are d-separated by $C$.</p>]]></content><author><name>Stippe</name><email>smaurizio87@protonmail.com</email></author><category term="course/various/" /><category term="/causal-graphs/" /><summary type="html"><![CDATA[Representing causality flows]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/docs/assets/images/causal_graphs/covariates.png" /><media:content medium="image" url="http://localhost:4000/docs/assets/images/causal_graphs/covariates.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Causal inference: a general introduction</title><link href="http://localhost:4000/causal-intro/" rel="alternate" type="text/html" title="Causal inference: a general introduction" /><published>2023-09-02T00:00:00+02:00</published><updated>2023-09-02T00:00:00+02:00</updated><id>http://localhost:4000/causal-intro</id><content type="html" xml:base="http://localhost:4000/causal-intro/"><![CDATA[<h2 id="causality-as-counterfactual-evidence">Causality as counterfactual evidence</h2>

<p>In the last years causal inference gained a lot of attention, both from academia and outside from it.
Of course, you heard that <em>association does not imply causation</em>, as you will find an association
between ice-cream consumption and the number of deaths by drowing.
However, outside from the classes for statisticians, the very important question <em>when does association imply causation</em>
is rarely discussed.
We should first try and clarify what do we mean by causation, as the popular concept of causation
is too vague for any rigorous discussion.
This topic has been debated is philosophy since centuries, and if you are interested about this aspect
I found <a href="https://iep.utm.edu/causation/">this</a> introductory article very helpful.</p>

<p>We won’t dig into the philosophical debate, and simply use the <strong>counterfactual</strong> approach:
we say that an action $T$, which is usually called treatment or intervention,
causes $Y$ if, $T$ changes, then $Y$ changes, so then when the cause $T$ disappears, then the effect $Y$ disappears too.
Our definition implies that we must switch off $T$ so,
in order for our definition to be meaningful, we must be able <em>at least hypothetically</em>
to manipulate $T$, and in this case we say that $T$ is <strong>manipulable</strong>.</p>

<p>A meaningful question is if a medicine cures the illness, as we may or may not take the medicine,
but we cannot ask whether age causes heart attack, as we can hardly imagine to change one person’s age.
The concept of manipulability depends on the context, as we may ask if increasing age causes a reduction
in the chances to be considered for a certain working position. In this case we may manipulate the age by simply
changing it on the CV and check if the company calls for a job interview.</p>

<p>The counterfactual definition is not precise enough,
as it may happen that $Y$ appears only when both $T$ and $Z$ appears, so in this
case an obvious question is whether $T$ or $Z$ is the cause of $Y$.
We will always assume that we want to investigate only one cause at time, so either we want to determine
if, given $T$, then $Z$ is a cause of $Y$ or vice versa if, given $Z$, then $T$ causes $Y$.
This imply that, when we investigate causality, we must change the hypothetical cause
by keeping everything else unchanged.</p>

<p>A very common source of confusion is the question causal inference tries and answer:
as explained in Gelman’s preprint <a href="https://arxiv.org/pdf/1003.2619.pdf">Causality and Statistical Learning</a>
when talking about causality, there are two main questions one could ask:</p>
<ul>
  <li>backward causal inference: what are the causes of a given effect?</li>
  <li>forward causal inference: what are the effects of a given cause?</li>
</ul>

<p>While there are many accepted methods to investigate the forward causal inference,
backward causal inference is a slippery terrain, as one could also 
say that the cause of the cause is the cause.
In fact, it is not uncommon to have that $A$ causes $B$, $B$ causes $C$ and $C$ causes $D$,
and in this case we will consider both $A$, $B$ and $C$ as causes of $D$.</p>

<h2 id="the-fundamental-problem-of-causal-inference">The fundamental problem of causal inference</h2>

<p>Let us assume for now that $T$ is a binary quantity with values $0$ and $1$, and let us indicate
the value of $Y$ when $T=0$ as $y_0$ while $y_1$ is the value of $Y$ whet $T=1$.
In order to assess whether $T$ causes $Y$ we must compare $y_1$ with $y_0$.</p>

<p>The exact way we want to compare these two quantities depends on the context.
Most of the time what one wants to quantify is the so called effect, defined as $\delta = y_1-y_0$, but in some cases one may prefer to
obtain informations about the relative risk $y_1/y_0$.
In any case, what we want to do is to compare both quantities and verify if they differ.</p>

<p>In most textbooks one defines the function</p>

\[Y(\tau) = \tau y_1 + (1-\tau) y_0\]

<p>so</p>

\[\delta = Y(1) - Y(0)\]

<p>One generally refers to the quantities $Y(0)$ and $Y(1)$ as the potential outcomes.
More precisely, the potential outcome is represented by the previous quantities before the experiment,
while during the experiment one measures the observed outcome, and the remaining quantity is the counterfactual outcome.
Since we assume that these quantities are the same, we will always refer to the potential outcomes.</p>

<p>As we previously stated, $y_1$ and $y_0$ represent $Y$ when $T=1$ or $0$ respectively, but everything else is
unchanged. This makes always impossible to measure the causal effect, as we cannot simultaneously realize $T=0$
and $T=1$ by keeping everything else, included the moment and the individual, unchanged,
and this is called the <strong>fundamental problem of causal inference</strong>.</p>

<p>In order to better understand why this is a problem, let us assume that we have a population,
and that we somehow split the population into two subpopulation.
The first subpopulation is then treated, so they are assigned to the $T=1$ group,
while the second one is not, and for them $T=0$.
We then take the average on each subpopulation what we are estimating is
$\mathbb{E}[Y | T=1]$ and $\mathbb{E}[Y | T=0]$ respectively.
On the other hand, what we really want to quantify in order to assess the average effect over the entire population is</p>

\[\mathbb{E}[\delta] = \mathbb{E}[Y(1) - Y(0)] = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]\]

<p>This quantity is called the <strong>average treatment effect</strong> (ATE).</p>

<p>Let us indicate with $y^i_\tau$ the observed outcome on the individual $i$ of the population
when this undergoes to treatment $\tau$.
We generally have that the outcome will both on the treatment and on some set of
relevant covariate (auxiliary quantities) $X$, that we assumed we measured for each individual.</p>

<table>
  <thead>
    <tr>
      <th>i</th>
      <th>T</th>
      <th>Y</th>
      <th>Y(0)</th>
      <th>Y(1)</th>
      <th>X</th>
      <th>Y(1) - Y(0)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>$y^1_0$</td>
      <td>$y^1_0$</td>
      <td>$y^1_1=?$</td>
      <td>$x^1$</td>
      <td>$y^1_1-y^1_0=?$</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>$y^2_0$</td>
      <td>$y^2_0$</td>
      <td>$y^2_1=?$</td>
      <td>$x^2$</td>
      <td>$y^2_1-y^2_0=?$</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>$y^3_0$</td>
      <td>$y^3_0$</td>
      <td>$y^3_1=?$</td>
      <td>$x^3$</td>
      <td>$y^3_1-y^3_0=?$</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1</td>
      <td>$y^4_1$</td>
      <td>$y^4_0=?$</td>
      <td>$y^4_1$</td>
      <td>$x^4$</td>
      <td>$y^4_1-y^4_0=?$</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1</td>
      <td>$y^5_1$</td>
      <td>$y^5_0=?$</td>
      <td>$y^5_1$</td>
      <td>$x^5$</td>
      <td>$y^5_1-y^5_0=?$</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1</td>
      <td>$y^6_1$</td>
      <td>$y^6_0=?$</td>
      <td>$y^6_1$</td>
      <td>$x^6$</td>
      <td>$y^6_1-y^6_0=?$</td>
    </tr>
  </tbody>
</table>

<p>Let us stuck for a moment to the frequentist framework, we have that</p>

\[\begin{aligned}
&amp;
\mathbb{E}[Y | T=0] = \frac{y^1_0 + y^2_0 + y^3_0}{3}
\\
&amp;
\mathbb{E}[Y | T=1] = \frac{y^4_1 + y^5_1 + y^6_1}{3}
\end{aligned}\]

<p>on the other hand</p>

\[\begin{aligned}
&amp;
\mathbb{E}[Y(0)] = \frac{y^1_0 + y^2_0 + y^3_0 + y^4_0 + y^5_0 + y^6_0}{6}
\\
&amp;
\mathbb{E}[Y(1)] = \frac{y^1_1 + y^2_1 + y^3_1 + y^4_1 + y^5_1 + y^6_1}{6}
\end{aligned}\]

<p>We cannot measure the quantities marked with the question mark,
so the fundamental problem of causal inference is a missing value problem.
The different terms entering into the two expressions don’t allow us
to simply substitute the associational quantities with the causal ones,
and this is why association is not causation.</p>

<p>As we will show briefly, however, when a set of rather stringent condition
holds, we are allowed to replace the causal quantities with the associational ones.</p>

<p>The stronger condition that might hold is <strong>ignorability</strong>, also called <strong>exchangeability</strong></p>

\[Y(0), Y(1) \perp\!\!\!\!\perp T\]

<p>The same requirement can be stated as:</p>

\[p(T | Y(0), Y(1)) = p(T)\]

<p>We are thus assuming that the probability of being treated, given the potential outcomes, is both independent on the potential outcomes and on any other quantity.
This is of course a very strong assumption, and the fact that in most observational
studies this condition is not met implies a wrong estimation of the effect.
As an example, if we are performing an observational study on a medicine, usually only people which
are sick and so will benefit by the medicine, will take the medicine and, so, will be included in the
treated group, while in the untreated group we may have sick people as well as healthy people.</p>

<p>The ignorability assumption states that we must be allowed to exchange the two groups without affecting the outcome.
Under this condition we have that</p>

\[\mathbb{E}[Y | T=1] - \mathbb{E}[Y | T=0] = \mathbb{E}[Y(1) | T=1] - \mathbb{E}[Y(0) | T=0] = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)] = \mathbb{E}[Y(1)-Y(0)] = \mathbb{E}[\delta]\]

<p>In the previous equation we used another important assumption:</p>

\[\mathbb{E}[Y | T=t] = \mathbb{E}[Y(t) | T=t]\]

<p>This only holds if we assume that, if $T=t$, then $Y = Y(T=t)$.
This assumption goes under the name on <strong>consistency</strong>, and it is not only a mathematical
requirement, but an operational one.
Consistency requires that the treatment must be well specified:
the treatment must not be “get some medicine” but should rather be “take 15 mg of medicine every 8 hours for 7 days”.</p>

<p>A slightly weaker condition with respect to ignorability is <strong>conditional ignorability</strong> or <strong>unconfoundedness</strong></p>

\[Y(0), Y(1) \perp\!\!\!\!\perp T | X\]

<p>So given the confounders $X$, the treatment probability $T$ is independent on the
potential outcome.</p>

\[p(T | Y(0), Y(1), X) = p(T | X)\]

<p>Let us assume we want to quantify the blood pressure reduction of a medicine.
It is more likely that people with a high blood pressure will take it.
We furthermore assume that the effect is higher on people with a high blood pressure.</p>

<p>Since it is more likely that people with high blood pressure will take the treatment,
ignorability doesn’t generally hold in observational studies.
We can randomly sample from the population to overcome this, but it will be very hard to obtain
a representative sample of the population.
An easier way is to stratify by initial blood pressure,
and for each stratum randomly assign with a given probability
to the treatment group or to the test group, and in this way we are fulfilling conditional
ignorability.</p>

<p>Thus, unconfoundedness states that we are “controlling for” all the relevant quantities which
may affect the outcome, except the treatment.
This may only approximately hold: if the outcome depends on some genetic aspect of the individual
which is more common in a particular ethnic group, controlling for ethnicity would partially fulfill 
unconfoundedness.</p>

<p>When we assign the population we must be sure that, for each stratum,
both groups have at least one individual:</p>

\[0 &lt; P(T=t | X) &lt; 1 \, \forall t\]

<p>The previous hypothesis is named the positivity assumption.
Positivity implies that we can compare the treatment effect with the control for each value of the
covariates, since
for each subgroup we both have units which receive the treatment and units which
does not receive it.</p>

<p>If conditional ignorability holds:</p>

\[\begin{aligned}
 \mathbb{E}[Y(1)-Y(0)|X] 
 &amp; = \mathbb{E}[Y(1)|X] - \mathbb{E}[Y(0)|X] \\
 &amp; = \mathbb{E}[Y(1)| T=1, X] - \mathbb{E}[Y(0)|T=0, X] \\
 &amp; = \mathbb{E}[Y| T=1, X] - \mathbb{E}[Y|T=0, X] \\
\end{aligned}\]

<p>By taking the average over $X$</p>

\[\mathbb{E}[\delta] = \mathbb{E}[Y(1) - Y(0)] = \mathbb{E}_X[ \mathbb{E}[Y(1) - Y(0) | X] ] 
 = \mathbb{E}_X[ \mathbb{E}[Y |T=1, X] ] - \mathbb{E}_X[ \mathbb{E}[Y |T=0, X] ]\]

<p>The equality between the first and the last term of this equation is called the <strong>adjustment formula</strong>.</p>

<p>Let us now write explicitly the adjustment formula for $X$ discrete:</p>

\[\begin{align}
&amp;
\mathbb{E}_X[ \mathbb{E}[Y |T=1, X] ] - \mathbb{E}_X[ \mathbb{E}[Y |T=0, X] ]  
\\
&amp; =  \sum_{x}P(X=x) \sum_{y} y \left(P(Y=y|T=1, X=x) - P(Y=y| T=0, X=x) \right) \\
&amp; =   \sum_{x}P(X=x) \sum_{y} y \left(\frac{P(Y=y,T=1, X=x)}{P(T=1, X=x)} - \frac{P(Y=y,T=0, X=x)}{P(T=0, X=x)}\right) \\
= &amp; \sum_{x}P(X=x) \sum_{y} y \left(\frac{P(Y=y,T=1, X=x)}{P(T=1| X=x) P(X=x)} - \frac{P(Y=y,T=0, X=x)}{P(T=0| X=x) P(X=x)}\right) 
\\
&amp; = 
\sum_{x}\sum_{y} y \left(\frac{P(Y=y,T=1, X=x)}{P(T=1| X=x)} - \frac{P(Y=y,T=0, X=x)}{P(T=0| X=x)}\right) 
\end{align}\]

<p>The first equivalence comes from the definition of conditional probability,
the second one from the hypothesis that $P(T, X) = P(T | X) P(X) $ so that $T$ causally depends on $X\,.$
You should notice that the denominators are finite thanks to the positivity hypothesis.</p>

<p>There is one more hypothesis that we have hidden into our discussion:
we have been assuming all the time that the outcome of the i-th unit only depend on the i-th treatment unit,
and does not depends on the other treatment’s unit.
This requirement is of course not always satisfied, and it’s called the <strong>no interference</strong> assumptions:</p>

\[Y_i(t_1, t_2, ..., t_{i-1}, t_i, t_{i+1}, ..., t_n) = Y_i(t_i)\]

<p>So each individual’s outcome only depends on his own treatment and not on the treatment of other individuals.
This implies that, if we are checking the effect of a product in some tomato field, we must be sure that the product does not goes in another studied field by mistake.
Another case can be a study where we are studying an experimental study program in a class.
If a student is selected in the treatment group and a friend of his is not, the latter could be sad for not being selected and his outcome could be lowered.
Generally, a good strategy to enforce this requirement is to take well separated units and isolating each unit from the other units during the experiment.</p>

<h2 id="conclusion-and-take-home-message">Conclusion and take home message</h2>

<p>As we can see, under some strict assumptions we can perform causal inference in observational studies as well as in randomized studies.
However, quoting Cochran:</p>

<p><strong><em>observational studies are are interesting and challenging field which demands a good deal of humility, since our claim are groping toward the truth.</em></strong></p>

<h2 id="additional-readings">Additional readings</h2>

<p><a href="https://www.hsph.harvard.edu/wp-content/uploads/sites/1268/2022/11/hernanrobins_WhatIf_13nov22.pdf">Hernàn, Robins; <strong>Causal inference, what if</strong>, Chapman &amp; Hall/CRC (2020)</a></p>]]></content><author><name>Stippe</name><email>smaurizio87@protonmail.com</email></author><category term="course/various/" /><category term="/causal-intro/" /><summary type="html"><![CDATA[When association implies causation]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/docs/assets/images/causal_graphs/dominoes-4020617_960_720.jpg" /><media:content medium="image" url="http://localhost:4000/docs/assets/images/causal_graphs/dominoes-4020617_960_720.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>