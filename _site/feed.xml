<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-11-27T17:51:42+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Data Perspectives</title><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><entry><title type="html">Random models and mixed models</title><link href="http://localhost:4000/statistics/random_models" rel="alternate" type="text/html" title="Random models and mixed models" /><published>2024-11-27T00:00:00+00:00</published><updated>2024-11-27T00:00:00+00:00</updated><id>http://localhost:4000/statistics/random_models</id><content type="html" xml:base="http://localhost:4000/statistics/random_models"><![CDATA[There are many situations where you want to understand the relation between two
variables at the subgroup level rather than at the level of the entire sample.
Random effect models are linear models where each subgroup has its own
slope and intercept, while in mixed effect models you either assign
a slope to each subgroup and a unique intercept or vice-versa.

![](/docs/assets/images/statistics/random_models/models.webp)

Let us see what are the differences between a random effect model and a fixed
effect model by looking at the data of [this study](https://www.key2stats.com/data-set/view/1040), where the authors restricted the number of hours of
a set of participants for ten days and analyzed the reaction time of the participants
to a test. The dataset only includes the set of participants who only slept
for three hours per night.

```python
import pandas as pd
import numpy as np
import jax.numpy as jnp
import seaborn as sns
from matplotlib import pyplot as plt
import pymc as pm
import arviz as az
import pymc.sampling_jax as pmj

df = pd.read_csv('data/Reaction.csv')

rng = np.random.default_rng(42)
df.head()
```

|    |   Unnamed: 0 |   X |   Reaction |   Days |   Subject |
|---:|-------------:|----:|-----------:|-------:|----------:|
|  0 |            1 |   1 |    249.56  |      0 |       308 |
|  1 |            2 |   2 |    258.705 |      1 |       308 |
|  2 |            3 |   3 |    250.801 |      2 |       308 |
|  3 |            4 |   4 |    321.44  |      3 |       308 |
|  4 |            5 |   5 |    356.852 |      4 |       308 |

Let us normalize the data before analyzing them,
and let us assume

$$
y_i = \alpha + \beta X_{i} + \varepsilon_i
$$

```python
df['y'] = (df['Reaction'] - mean)/df['Reaction'].std()
df['subj_id']=df['Subject'].map({elem: k for k, elem in enumerate(df['Subject'].drop_duplicates())})
df['intercept'] = 1
X_v = df[['intercept','Days']]
X_s = pd.DataFrame({'subj':df['subj_id'].drop_duplicates()})
coords = {'cols': X_v.columns, 'obs_id': X_v.index, 'subj_id': X_s.index, 'subj_col': X_s.columns}


with pm.Model() as model_fixed:
    alpha = pm.Normal('alpha', mu=0, sigma=500)
    beta = pm.Normal('beta', mu=0, sigma=500)
    sigma = pm.HalfNormal('sigma', sigma=500)
    yhat = pm.Normal('y', mu=alpha + beta*df['Days'], sigma=sigma, observed=df['y'])

with model_fixed:
    idata_fixed = pm.sample(nuts_sampler='numpyro', random_seed=rng)

az.plot_trace(idata_fixed)
fig = plt.gcf()
fig.tight_layout()
```

![](/docs/assets/images/statistics/random_models/trace_fixed.webp)

```python
yfit = jnp.outer(idata_fixed.posterior['alpha'].values.reshape(-1), jnp.ones(len(df['Days'].drop_duplicates().values)))+jnp.outer(
    idata_fixed.posterior['beta'].values.reshape(-1), jnp.arange(len(df['Days'].drop_duplicates().values)))

fig = plt.figure()
ax = fig.add_subplot(111)
ax.fill_between(df['Days'].drop_duplicates().values, jnp.quantile(yfit, q=0.03, axis=0),jnp.quantile(yfit, q=0.97, axis=0),
               color='lightgray', alpha=0.8)
ax.plot(df['Days'].drop_duplicates().values, jnp.mean(yfit, axis=0))
sns.scatterplot(df, x='Days', y='y', hue='Subject', ax=ax)
ax.get_legend().remove()
fig.tight_layout()
```

![](/docs/assets/images/statistics/random_models/ppc_fixed.webp)

The estimated trend agrees with the observed one, but our model
fails to reproduce the observed variability despite the large prior variance.
Let us now compare the previous model with a random effect model.
In this model we will assume that each individual has his/her own intercept,
which represents the response without sleeping restrictions, as well as 
his/her own slope, which represents the average change in performance
after one day of sleep restrictions.
We will fully leverage bayesian statistics, and assume that the parameters
have a common prior. Another possible choice would have been to assume that
each of them had a separate prior, but in this way we can share information across the individuals. We will also assume that the slope and the intercept
are correlated.

$$
\begin{align}
y_i &= \alpha_{[j]i} + \beta_{[j]i} X_{i} + \varepsilon_i
\\
\varepsilon_i & \sim \mathcal{N}(0, \sigma)
\\
\begin{pmatrix}
\alpha_{[j]i} \\
\beta_{[j]i}
\end{pmatrix}
& \sim \mathcal{N}(\mu, \Sigma)
\\
\mu_i & \sim \mathcal{N}(0, 10)
\\
\Sigma & \sim \mathcal{LKJ}(1)
\end{align}
$$

```python
with pm.Model(coords=coords) as model:
    mu = pm.Normal('mu', sigma=10, dims=['cols'])
    X = pm.Data('X', X_v, dims=['obs_id', 'cols'])
    sd_dist = pm.HalfNormal.dist(sigma=5, size=X_v.shape[1])
    chol, corr, sig = pm.LKJCholeskyCov('sig', n=X_v.shape[1], eta=1.0, sd_dist=sd_dist)
    sigma = pm.HalfNormal('sigma', sigma=5)
    # sig = pm.HalfNormal('sig', 5, dims=['cols'])
    alpha = pm.MvNormal(f'alpha', mu=mu, chol=chol, dims=['subj_id', 'cols'], shape=(len(df['Subject'].drop_duplicates()), X_v.shape[1]))
    tau = pm.Deterministic('tau', pm.math.sum([alpha[df['subj_id'], k]*X.T[k,:] for k in range(X_v.shape[1])], axis=0),
                          dims=['obs_id'])
    yhat = pm.Normal(f'yhat', mu=tau, sigma=sigma, observed=df['y'], dims=['obs_id'])

with model:
    idata = pm.sample(nuts_sampler='numpyro',
                     random_seed=rng)

az.plot_trace(idata, 
              coords={"sig_corr_dim_0": 0, "sig_corr_dim_1": 1})
fig = plt.gcf()
fig.tight_layout()
```

![](/docs/assets/images/statistics/random_models/trace.webp)

The trace looks fine, let us now look at the posterior predictive.

```python
with model:
    ppc = pm.sample_posterior_predictive(idata, random_seed=rng)
    
sub_dict_inv = {k: elem for k, elem in enumerate(df['Subject'].drop_duplicates())}

x_pl = np.arange(10)
fig, ax=plt.subplots(nrows=6, ncols=3, figsize=(9, 8))
df_subs = pd.DataFrame({'Subject': df['Subject'], 
                        'Days': df['Days'],
                        'y': df['y'],
                        'mean': ppc.posterior_predictive['yhat'].mean(dim=['draw', 'chain']),
                       'low': ppc.posterior_predictive['yhat'].quantile(q=0.03, dim=['draw', 'chain']),
                       'high': ppc.posterior_predictive['yhat'].quantile(q=0.97, dim=['draw', 'chain']),
})
for i in range(6):
    for j in range(3):
        k =3*i + j
        df_red = df_subs[df_subs['Subject']==sub_dict_inv[k]]
        y_pl = df_red['mean']
        y_m = df_red['low']
        y_M = df_red['high']
        ax[i][j].fill_between(x_pl, y_m, y_M, alpha=0.8, color='lightgray')
        ax[i][j].plot(x_pl, y_pl)
        ax[i][j].scatter(df_red['Days'], df_red['y'])
        ax[i][j].set_ylim(-4, 4)
        ax[i][j].set_title(f"i={k}")
        ax[i][j].set_yticks([-4, 0, 4])
fig.tight_layout()
```

![](/docs/assets/images/statistics/random_models/ppc_mixed.webp)

The performances of the new model are way better than the previous one,
and this is not surprising since we have many more parameters.

We can now verify how much does the average performance degradation changes
with the participant.

```python
az.plot_forest(idata, var_names="alpha",
              coords={"cols": ["Days"]})
```

![](/docs/assets/images/statistics/random_models/forest_pooled.webp)

We can also predict how will a new participant perform

```python
with model:
    alpha_new = pm.MvNormal('alpha_new', mu=mu, chol=chol, shape=(2), dims=['cols'])
    tau_new = pm.Deterministic('tau_new', alpha_new[0]+alpha_new[1]*df['Days'].drop_duplicates())
    y_new = pm.Normal(f'yhat_new', mu=tau_new, sigma=sigma)

with model:
    ppc_new = pm.sample_posterior_predictive(idata, var_names=['yhat_new', 'alpha_new', 'tau_new'])

fig = plt.figure()
ax = fig.add_subplot(111)

y_pl = ppc_new.posterior_predictive[f'yhat_new'].mean(dim=['draw', 'chain'])
y_m = ppc_new.posterior_predictive[f'yhat_new'].quantile(q=0.025, dim=['draw', 'chain'])
y_M = ppc_new.posterior_predictive[f'yhat_new'].quantile(q=0.975, dim=['draw', 'chain'])
ax.plot(np.arange(10), y_pl)
ax.fill_between(np.arange(10), y_m, y_M, color='lightgray', alpha=0.8)

```

![](/docs/assets/images/statistics/random_models/pp_new.webp)

Another advantage of the hierarchical model is that we can estimate
the distribution of the slope and intercept for a new participant

```python
az.plot_pair(ppc_new, group='posterior_predictive', var_names=['alpha_new'], kind='kde')
fig = plt.gcf()
fig.tight_layout()
```

![](/docs/assets/images/statistics/random_models/alpha_new.webp)

## Conclusions

We discussed the main features of random models, and we discussed when 
it may be appropriate to use them.
We have also seen what are the advantages of implementing a hierarchical
structure on random effect models.

## Suggested readings
- <cite>Gelman, A., Hill, J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models. CUP.
</cite>

```python
%load_ext watermark
```

```python
%watermark -n -u -v -iv -w -p xarray,pytensor,numpyro,jax,jaxlib
```

<div class="code">
Last updated: Mon Jul 22 2024
<br>

<br>
Python implementation: CPython
<br>
Python version       : 3.12.4
<br>
IPython version      : 8.24.0
<br>

<br>
xarray  : 2024.5.0
<br>
pytensor: 2.20.0
<br>
numpyro : 0.15.0
<br>
jax     : 0.4.28
<br>
jaxlib  : 0.4.28
<br>

<br>
pandas    : 2.2.2
<br>
arviz     : 0.18.0
<br>
jax       : 0.4.28
<br>
numpy     : 1.26.4
<br>
matplotlib: 3.9.0
<br>
seaborn   : 0.13.2
<br>
pymc      : 5.15.0
<br>

<br>
Watermark: 2.4.3
<br>
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/random_models_intro/" /><summary type="html"><![CDATA[Making inference on subgroups]]></summary></entry><entry><title type="html">Hierarchical models and meta-analysis</title><link href="http://localhost:4000/statistics/hierarchical_metaanalysis" rel="alternate" type="text/html" title="Hierarchical models and meta-analysis" /><published>2024-11-26T00:00:00+00:00</published><updated>2024-11-26T00:00:00+00:00</updated><id>http://localhost:4000/statistics/hierarchical_metaanalysis</id><content type="html" xml:base="http://localhost:4000/statistics/hierarchical_metaanalysis"><![CDATA[In the last post we discussed how to build a hierarchical model.
These models are often used in meta-analysis and reviews,
*i.e.* in academic publications where the results of many studies are collected,
criticized and combined.
In this kind of study using a full pooling would not be appropriate,
as each study is performed at its own conditions,
so a hierarchical model is much more appropriate to combine the results together.
This topic is extensively discussed in Gelman's textbook,
and we will limit ourselves to show an application of hierarchical model
to the meta-analysis.

We will use this method to re-analyze an old meta-analysis by Daryl Bem,
a well known researcher who published the famous 
["Bem meta-analysis"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4706048/)
where he suggested that he found some evidence of precognition.
This paper became very famous both because of the conclusions
and because researchers failed to replicate its findings.

By analyzing the methodological flaws used in that paper,
other scientist started proposing many methods to improve
the robustness of the research results.

We won't analyze that article, since it combined many kinds of experiments,
and this makes the overall analysis more involved. We will instead use
another paper by the same author which can be found
on [this page of the CIA website](https://www.cia.gov/readingroom/docs/CIA-RDP96-00789R003200110001-4.pdf) (yes, CIA has been really interested into paranormal
activity).
We choose this article as it simply involves a binomial likelihood.
The article, in fact, summarizes the results of 11 experiments
which has equi-probable binary outcome, so by random guessing one
would expect, on average, a success percentage of the $50\%\,.$

In the analysis we will use the **Region Of Practical Equivalence** (ROPE)
to assess if the effect is practically equivalent with absent.
We will conclude that there is no evidence of precognition if the
$94\%$ Highest Density Region for the average success ratio
is entirely included in the region $[0, 0.75]\,.$
We will instead conclude that there is evidence of precognition
if the $94\%$ HDI is entirely inside the $[0.75, 1]$ region.
If the $94\%$ HDI crosses 0.75 we will conclude that the analysis is inconclusive.

The limit 0.75 seems quite strong, but since a positive finding
would contradict the current scientific knowledge, we require a strong evidence
in order to get a positive result.
Notice that the ROPE choice must be done *before* the dataset is seen,
otherwise we could tune the choice on the data.


As before, we will take

$$
\begin{align}
\alpha \sim & \mathcal{HN}(10)
\\
\beta \sim & \mathcal{HN}(10)
\\
\theta_i \sim & \mathcal{Beta}(\alpha, \beta)
\\
y_i \sim & \mathcal{Binom}(\theta_i, n_i)
\end{align}
$$

where $\mathcal{HN}(\sigma)$ denotes the Half Normal distribution.

Our dataset will consist into the 4th and 5th columns of table 1, which we provide
here

|   n |   y |
|----:|:----|
|  22 |   8 |
|   9 |   3 |
|  35 |  10 |
|  50 |  12 |
|  50 |  18 |
|  50 |  15 |
|  36 |  12 |
|  20 |  10 |
|   7 |   3 |
|  50 |  15 |
|  25 |  16 |

```python
import pandas as pd
import numpy as np
import seaborn as sns
import pymc as pm
import arviz as az
import scipy.stats as st
from matplotlib import pyplot as plt

rope_reg = [0, 0.75]

df = pd.DataFrame({"n": [22, 9, 35, 50, 50, 50, 36, 20, 7, 50, 25], 
                   "y": [8, 3, 10, 12, 18, 15, 12, 10, 3, 15, 16]})

rng = np.random.default_rng(42)

with pm.Model() as binom_meta:
    alpha = pm.HalfNormal("alpha", sigma=10)
    beta = pm.HalfNormal("beta", sigma=10)
    theta = pm.Beta('theta', alpha=alpha, beta=beta, shape=len(df['n'].values))
    y = pm.Binomial('y', p=theta, n=df['n'].values,
                  observed=df['y'].values)

with binom_meta:
    idata_meta = pm.sample(5000, tune=5000, chains=4, target_accept=0.98,
                           random_seed=rng, nuts_sampler='numpyro')

az.plot_trace(idata_meta)
fig = plt.gcf()
fig.tight_layout()
```

![The trace of the hierarchical model](/docs/assets/images/statistics/hierarchical_meta/trace.webp)

```python
az.summary(idata_meta)
```

|           |   mean |    sd |   hdi_3% |   hdi_97% |   mcse_mean |   mcse_sd |   ess_bulk |   ess_tail |   r_hat |
|:----------|-------:|------:|---------:|----------:|------------:|----------:|-----------:|-----------:|--------:|
| alpha     |  8.234 | 3.079 |    2.857 |    13.904 |       0.03  |     0.021 |       9818 |      11696 |       1 |
| beta      | 14.226 | 5.351 |    4.903 |    24.217 |       0.051 |     0.036 |      10128 |      10807 |       1 |
| theta[0]  |  0.366 | 0.074 |    0.231 |     0.507 |       0     |     0     |      23085 |      15421 |       1 |
| theta[1]  |  0.357 | 0.09  |    0.191 |     0.528 |       0.001 |     0     |      22702 |      13380 |       1 |
| theta[2]  |  0.316 | 0.063 |    0.196 |     0.433 |       0     |     0     |      23299 |      13816 |       1 |
| theta[3]  |  0.279 | 0.054 |    0.178 |     0.381 |       0     |     0     |      23776 |      14155 |       1 |
| theta[4]  |  0.362 | 0.058 |    0.255 |     0.471 |       0     |     0     |      23117 |      14252 |       1 |
| theta[5]  |  0.32  | 0.056 |    0.216 |     0.427 |       0     |     0     |      24735 |      14446 |       1 |
| theta[6]  |  0.346 | 0.064 |    0.226 |     0.464 |       0     |     0     |      25809 |      14362 |       1 |
| theta[7]  |  0.431 | 0.08  |    0.284 |     0.584 |       0.001 |     0     |      22630 |      14849 |       1 |
| theta[8]  |  0.384 | 0.097 |    0.202 |     0.564 |       0.001 |     0     |      24538 |      13129 |       1 |
| theta[9]  |  0.32  | 0.056 |    0.218 |     0.428 |       0     |     0     |      23604 |      15030 |       1 |
| theta[10] |  0.515 | 0.081 |    0.369 |     0.67  |       0.001 |     0     |      20264 |      14867 |       1 |

There is no evident issue in the sampling procedure.

```python
az.plot_forest(idata_meta, var_names=['theta'], rope=rope_reg)
```

![The forest plot of the hierarchical model](/docs/assets/images/statistics/hierarchical_meta/forest.webp)


None of the studies suggests that there is any evidence of precognition.
We can also estimate the overall average as well as the effective sample
size.

```python
with binom_meta:
    logit_mu = pm.Deterministic("logit_mu", pm.math.log(alpha/beta))
    log_neff = pm.Deterministic("log_neff", pm.math.log(alpha+beta))

with binom_meta:
    ppc = pm.sample_posterior_predictive(idata_meta, var_names=['logit_mu', 'log_neff'])
    
    
fig = plt.figure()
ax = fig.add_subplot(111)
az.plot_pair(ppc.posterior_predictive, var_names=["logit_mu", "log_neff"], kind="kde", ax=ax)
ax.set_xlim([-1.5, 0.5])
ax.set_ylim([2, 4])
fig.tight_layout()
```

![The kernel density estimate for mu and for the effective sample size of the hierarchical model](/docs/assets/images/statistics/hierarchical_meta/kde.webp)

## Conclusions

We applied the beta binomial hierarchical model to a meta-analysis on
precognition. We also introduced the Region Of Practical Equivalence (ROPE).


## Suggested readings
- <cite><a href="http://www.stat.columbia.edu/~gelman/book/BDA3.pdf">Gelman, A. (2014). Bayesian Data Analysis, Third Edition. Taylor & Francis.</a></cite>

```python
%load_ext watermark
```

```python
%watermark -n -u -v -iv -w -p xarray,pytensor,numpyro,jax,jaxlib
```

<div class="code">
Last updated: Mon Jul 22 2024
<br>

<br>
Python implementation: CPython
<br>
Python version       : 3.12.4
<br>
IPython version      : 8.24.0
<br>

<br>
xarray  : 2024.5.0
<br>
pytensor: 2.20.0
<br>
numpyro : 0.15.0
<br>
jax     : 0.4.28
<br>
jaxlib  : 0.4.28
<br>

<br>
numpy     : 1.26.4
<br>
arviz     : 0.18.0
<br>
matplotlib: 3.9.0
<br>
pandas    : 2.2.2
<br>
pymc      : 5.15.0
<br>
seaborn   : 0.13.2
<br>
scipy     : 1.13.1
<br>

<br>
Watermark: 2.4.3
<br>
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/hierarchical_metaanalysis/" /><summary type="html"><![CDATA[How hierarchical models can be used to analyze scientific literature]]></summary></entry><entry><title type="html">Hierarchical models</title><link href="http://localhost:4000/statistics/hierarchical_models" rel="alternate" type="text/html" title="Hierarchical models" /><published>2024-11-25T00:00:00+00:00</published><updated>2024-11-25T00:00:00+00:00</updated><id>http://localhost:4000/statistics/hierarchical_models</id><content type="html" xml:base="http://localhost:4000/statistics/hierarchical_models"><![CDATA[There are many circumstances where your data are somehow connected,
but cannot be treated as iid.
As an example, when you compare clinical studies on the effectiveness
of a medicine, data originated from
different studies will likely involve different inclusion policies as
well as different hospital setup.
However, it is reasonable to assume that parameters describing
different studies are related. 

<div class='emphbox'>
In Bayesian statistics, a common approach
in these situations is to assume that the parameters are sampled from
a common prior distribution.
</div>

Mathematically speaking, you assume that

$$
Y_i \sim P(\theta_i)
$$

so each observation will be described by its own parameter. However,
the parameters are considered as iid

$$
\theta_i \sim P'(\phi)
$$

This kind of model strictly belongs to the Bayesian framework, as in the frequentist
one the parameters $\theta_i$ are numbers, so you can either treat them
as different (unpooled) or they are sampled from the same distribution (pooled).
Hierarchical models are one of the major components of the modern statistics,
so it is worthwhile spending some time to deeply understand this model family.

## A little digression on pooling

In statistics, pooling refers to the practice of grouping different
observations assuming that they are identically distributed.
In order to better explain this concept, let us assume that
we have $N$ observations $y_1,y_2,\dots,y_N\,.$

In the **unpooled** model you assume that each observation is associated
to a probability distribution, and that every distribution is independent
on the other distributions.
Assuming an unpooled model means associating a different parameter
to each observation.

<p align="center">
<img src="/docs/assets/images/statistics/hierarchical/unpooled.webp" 
alt="The structure of the unpooled model" width="300"> 
</p>

In the **pooled** model, you assume that all the observations are associated
to the same probability distribution.
In this case, all the observations are related to the same parameter.

<p align="center">
<img src="/docs/assets/images/statistics/hierarchical/pooled.webp" 
alt="The structure of the pooled model" width="300"> 
</p>


In the **hierarchical** model each observation has its own parameter,
but the parameters are all sampled from the same distribution.


<p align="center">
<img src="/docs/assets/images/statistics/hierarchical/hierarchical.webp" 
alt="The structure of the hierarchical model" width="300"> 
</p>

Let us take a look at these different kind of models with a
simple example.

## SpaceX analysis

In the following we will consider the launches from the four
main launch vehicles: Falcon 1, Falcon 9, Falcon Heavy and Starship [^1].
For the sake of simplicity, we will treat as identical
rockets of different variants within the same family.
Below we provide the relevant statistics for the different launchers.

[^1]: These data are a little bit outdated, but it's not relevant for our purposes.

|    | Mission      |   Number |   successes |
|---:|:-------------|----:|----:|
|  0 | Falcon 1     |   5 |   2 |
|  1 | Falcon 9     | 304 | 301 |
|  2 | Falcon Heavy |   9 |   9 |
|  3 | Starship     |   9 |   5 |


```python
import pandas as pd
import numpy as np
import seaborn as sns
import pymc as pm
import arviz as az
import scipy.stats as st
from matplotlib import pyplot as plt

df_spacex = pd.DataFrame({'Mission': ['Falcon 1', 'Falcon 9', 'Falcon Heavy', 'Starship'], 'N': [5, 304, 9, 9], 'y': [2, 301, 9, 5]})

df_spacex['mu'] = df_spacex['y']/df_spacex['N']

rng = np.random.default_rng(42)

coords = {'obs_id': df_spacex['Index']}
```


### No pooling

Let us first consider the no pooling model, where we assume
that the failure probability of each vehicle is independent
on the other vehicles.

```python
with pm.Model(coords=coords) as spacex_model_no_pooling:  
  theta = pm.Beta('theta', alpha=1/2, beta=1/2, dims=['obs_id'])
  y = pm.Binomial('y', p=theta, n=df_spacex['N'].values,
                  observed=df_spacex['y'].values, dims=['obs_id'])

pm.model_to_graphviz(spacex_model_no_pooling)
```


<img src="/docs/assets/images/statistics/hierarchical/model_unpooled.webp" 
alt="The structure of the unpooled model" width="300"> 

In the above diagram we see that each of the four vehicle has its own
parameter.

```python
with spacex_model_no_pooling:
    idata_spacex_no_pooling = pm.sample(5000, tune=5000, chains=4,
                                        random_seed=rng, target_accept=0.95,
                                       nuts_sampler='numpyro')

az.plot_trace(idata_spacex_no_pooling)
fig = plt.gcf()
fig.tight_layout()
```

![The trace of the unpooled model](/docs/assets/images/statistics/hierarchical/trace_unpooled.webp)

In the above block, we increased the "target_accept" parameter in order to avoid
numerical issues.
The trace looks fine, but let us now take a better look at the estimated parameters.

```python
az.plot_forest(idata_spacex_no_pooling)
```

![The forest plot of the unpooled model](/docs/assets/images/statistics/hierarchical/forest_unpooled.webp)

The parameters make sense, and the Falcon 1 as well as the Starship are very
unconstrained, due to the small number of launches.
Let us now take a look at the pooled model.

### Full pooling

On the other hand, in the full-pooling method we assume that
the same probability describes all the vehicles.

```python
with pm.Model(coords=coords) as spacex_model_full_pooling:  
  theta = pm.Beta('theta', alpha=1/2, beta=1/2)
  y = pm.Binomial('y', p=theta, n=df_spacex['N'].values,
                  observed=df_spacex['y'].values, dims=['obs_id'])

pm.model_to_graphviz(spacex_model_full_pooling)
```


<img src="/docs/assets/images/statistics/hierarchical/model_pooled.webp" alt="The structure of the pooled model" width="300"> 

In this model, all the launches are treated as sampled from a common
iid, and we therefore have only one parameter for all the launch vehicle.

```python
with spacex_model_full_pooling:
  idata_spacex_full_pooling = pm.sample(5000, tune=5000, chains=4,
                                        random_seed=rng, target_accept=0.95,
                                       nuts_sampler='numpyro')

az.plot_trace(idata_spacex_full_pooling)
fig = plt.gcf()
fig.tight_layout()
```

![The trace of the pooled model](/docs/assets/images/statistics/hierarchical/trace_pooled.webp)

```python
az.plot_forest(idata_spacex_full_pooling)
```

![The forest plot of the pooled model](/docs/assets/images/statistics/hierarchical/forest_pooled.webp)

In this case the single parameter is almost entirely above 0.94,
so any launch should succeed with a probability higher than the 94%.
Would you to bet that a Falcon 1 would succeed? I honestly wouldn't,
but this is what you should do according to this model.

### Hierarchical model

The hierarchical model has both the above models as
special cases. 

```python
with pm.Model(coords=coords) as spacex_model_hierarchical:
    alpha = pm.HalfNormal("alpha", sigma=10)
    beta = pm.HalfNormal("beta", sigma=10)
    mu = pm.Deterministic("mu", alpha/(alpha+beta))
    theta = pm.Beta('theta', alpha=alpha, beta=beta, dims=['obs_id'])
    y = pm.Binomial('y', p=theta, n=df_spacex['N'].values,
                  observed=df_spacex['y'].values, dims=['obs_id'])

pm.model_to_graphviz(spacex_model_hierarchical)
```

<img src="/docs/assets/images/statistics/hierarchical/model_hierarchical.webp" 
alt="The structure of the hierarchical model" width="450"> 

In this case, each vehicle has its own parameter. The parameters are
however sampled according to a Beta distribution, with priors
$\alpha$ and $\beta\,.$

```python
with spacex_model_hierarchical:
    idata_spacex_hierarchical = pm.sample(5000, tune=5000, chains=4,
                                        random_seed=rng, target_accept=0.95,
                                       nuts_sampler='numpyro')
```

![The trace of the hierarchical model](/docs/assets/images/statistics/hierarchical/trace_hierarchical.webp)

```python
az.plot_forest(idata_spacex_hierarchical)
```

![The forest plot of the hierarchical model](/docs/assets/images/statistics/hierarchical/forest_hierarchical.webp)

These estimates are similar to the unpooled model, they are however
closer one to the other ones.
This can be easily seen by comparing the two forest plot

```python
fig = plt.figure()
ax = fig.add_subplot(111)
az.plot_forest([idata_spacex_no_pooling, idata_spacex_hierarchical], model_names=['no pooling', 'hierarchical'],
               var_names=['theta'], ax=ax, combined=True)
fig.tight_layout()
```

![The forest plot of the hierarchical model compared to the unpooled model](
/docs/assets/images/statistics/hierarchical/forest_compare.webp)


This happens because the hierarchical model allow us to share information
across the variables.

One of the most relevant features of hierarchical models, is that they allow us to make predictions for unobserved variables
with unknown parameters.
Let us assume that SpaceX produces a new launcher, this model allows us
to estimate its success probability.

```python
with spacex_model_hierarchical:
    theta_new = pm.Beta('th_new', alpha=alpha, beta=beta)
    ppc_new = pm.sample_posterior_predictive(idata_spacex_hierarchical, var_names=['th_new'])

az.plot_posterior(ppc_new.posterior_predictive['th_new'].values.reshape(-1))
```

![The probability distribution for a new theta](/docs/assets/images/statistics/hierarchical/hierarchical_new_theta.webp)

We can moreover estimate the average success rate for any SpaceX
vehicle.

```python
map_mu = st.mode(np.digitize(idata_spacex_hierarchical.posterior["mu"].values.reshape(-1), bins=np.linspace(0, 1, 100))).mode/100

fig = plt.figure()
ax = fig.add_subplot(111)
az.plot_posterior(idata_spacex_hierarchical, var_names=["mu"], ax=ax)
ax.text(0.3, 3, f'MAP: {map_mu}', fontsize=15)
```

![The probability distribution of a success](/docs/assets/images/statistics/hierarchical/hierarchical_mu.webp)

This estimate is much more generous than the pooled one,
as it properly takes into account the failure rates of less successful
models.

We also reported the Maximum A Posteriori (MAP) estimate for the above parameter,
as it is a better point estimate than the mean for non-symmetric distributions
as the one above.

## Conclusions

We discussed how hierarchical models allow us to share information
across the variables and to make predictions for new, unobserved, variables.
In the next post we will discuss a very important application
of hierarchical models to meta-analysis.


## Suggested readings
- <cite><a href="http://www.stat.columbia.edu/~gelman/book/BDA3.pdf">Gelman, A. (2014). Bayesian Data Analysis, Third Edition. Taylor & Francis.</a></cite>


```python
%load_ext watermark
```

```python
%watermark -n -u -v -iv -w -p xarray,pytensor,numpyro,jax,jaxlib
```

<div class="code">
Last updated: Thu Nov 21 2024
<br>

<br>
Python implementation: CPython
<br>
Python version       : 3.12.7
<br>
IPython version      : 8.24.0
<br>

<br>
xarray  : 2024.9.0
<br>
pytensor: 2.25.5
<br>
numpyro : 0.15.0
<br>

<br>
scipy     : 1.14.1
<br>
matplotlib: 3.9.2
<br>
pandas    : 2.2.3
<br>
pymc      : 5.17.0
<br>
arviz     : 0.20.0
<br>
numpy     : 1.26.4
<br>
seaborn   : 0.13.2
<br>

<br>
Watermark: 2.4.3
<br>
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/hierarchical_models/" /><summary type="html"><![CDATA[How to implement hierarchies]]></summary></entry><entry><title type="html">Poisson regression</title><link href="http://localhost:4000/statistics/poisson_regression" rel="alternate" type="text/html" title="Poisson regression" /><published>2024-11-20T00:00:00+00:00</published><updated>2024-11-20T00:00:00+00:00</updated><id>http://localhost:4000/statistics/poisson_regression</id><content type="html" xml:base="http://localhost:4000/statistics/poisson_regression"><![CDATA[In the last post we introduced the Generalized Linear Models,
and we explained how to perform regression on data types which are
not appropriate for a Gaussian likelihood.
We also saw a concrete example of logistic regression, and here we will
discuss another type of GLM, the Poisson regression.

## Poisson regression

In the Poisson regression one assumes that

$$
Y_i \sim \mathcal{Poisson}(\theta_i)
$$

where $\theta_i$ must be a non-negative variable. One can 
use the exponential function to map any real number on the
positive axis, we therefore assume that

$$
\theta_i = \exp\left(\alpha + \beta X_i\right)
$$

We will use this model to estimate the average number of
bear attacks in North America.
The original data can be found on this [data.world
](https://data.world/ajsanne/north-america-bear-killings/workspace/file?filename=north_america_bear_killings.csv)
page, where there are listed all human killing by a black, brown, or polar bear from 1900-2018 in North America.
We will limit ourself to black and brown bears, as attacks by polar bears are very rare.
We will also limit our dataset to the years after 1999, as we want to assume that the attack probability
is constant within the entire time range, and we will neglect attacks by captive animals.
We want to assess the attack probability by bear type, and to do this we will use the bear type
as a regressor.

```python
import pandas as pd
import pymc as pm
import arviz as az
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt

df = pd.read_csv('./data/north_america_bear_killings.csv')

rng = np.random.default_rng(seed=42)

df_red = df.groupby(['Year', 'Type', 'Type of bear']).count().reset_index()[['Year', 'Type', 'Type of bear', 'Hikers']]

df_clean = df_red[(df_red['Year']>=2000) & (df_red['Type of bear'] != 'Polar Bear')& (df_red['Type'] != 'Captive')]

df_fit = df_clean.set_index(['Year', 'Type of bear']).unstack(fill_value=0).stack().reset_index()[['Year', 'Type of bear', 'Hikers']]

df_fit.rename(columns={'Hikers': 'Count'}, inplace=True)

df_fit['is_black'] = df_fit['Type of bear'].str.contains('Black').astype(int)

with pm.Model() as poisson_regr:
    alpha = pm.Normal('alpha', mu=0, sigma=2)
    beta = pm.Normal('beta', mu=0, sigma=2)
    z = pm.math.exp(alpha + beta*df_fit['is_black'])
    y = pm.Poisson('y', mu=z, observed=df_fit['Count'])

with poisson_regr:
    idata = pm.sample(draws=2000, tune=2000, random_seed=rng,
                     nuts_sampler='numpyro')

az.plot_trace(idata)
fig = plt.gcf()
fig.tight_layout()
```

![The trace of the Poisson model](/docs/assets/images/statistics/poisson_glm/trace.webp)

The trace seems fine, we can now verify if the model is compatible with the data.

```python
with poisson_regr:
    y_brown = pm.Poisson('y_brown', mu=pm.math.exp(alpha))
    y_black = pm.Poisson('y_black', mu=pm.math.exp(alpha+beta))
    
with poisson_regr:
    idata.extend(pm.sample_posterior_predictive(idata, var_names=['y', 'y_brown', 'y_black'], random_seed=rng))
# Let us estimate the probability that, in one year, there are k brown/black bear attacks 

n_brown = np.array([np.count_nonzero(idata.posterior_predictive['y_brown'].values.reshape(-1)==k) for k in range(10)])
n_black = np.array([np.count_nonzero(idata.posterior_predictive['y_black'].values.reshape(-1)==k) for k in range(10)])

fig = plt.figure()
ax = fig.add_subplot(211)
ax.hist(df_fit[df_fit['is_black']==0]['Count'], alpha=0.5, color='crimson', density=True, bins=np.arange(10), width=0.8)
ax.bar(np.arange(10)+0.3, n_brown/n_brown.sum(), alpha=0.7, width=0.8)
ax1 = fig.add_subplot(212)
ax1.hist(df_fit[df_fit['is_black']==1]['Count'], alpha=0.5, color='crimson', density=True, bins=np.arange(10), width=0.8)
ax1.bar(np.arange(10)+0.3, n_black/n_black.sum(), alpha=0.7, width=0.8)
```

![The posterior predictive of the Poisson model](/docs/assets/images/statistics/poisson_glm/posterior_predictive.webp)

The data seems compatible with the average estimate of our model.
We can now verify if the average number of attacks by black bears is statistically
compatible with the average number of attacks by brown bears.

```python
az.plot_forest(trace, var_names=['alpha', 'beta'])
```

![The forest plot of our parameters](/docs/assets/images/statistics/poisson_glm/posterior.webp)

As we can see, $\beta$ is compatible with 0, so we can consider the average attack number
by black bears is compatible with the average attack number by brown bears.

## Conclusions

We discussed a second kind of GLM, namely the Poisson regression,
and we applied this model to estimate the average number of lethal
attacks by wild bears in North America.


## Suggested readings
- <cite><a href="http://www.stat.columbia.edu/~gelman/book/BDA3.pdf">Gelman, A. (2014). Bayesian Data Analysis, Third Edition. Taylor & Francis.</a></cite>


```python
%load_ext watermark
```

```python
%watermark -n -u -v -iv -w -p xarray,pytensor
```

<div class="code">
Last updated: Wed Nov 20 2024
<br>

<br>
Python implementation: CPython
<br>
Python version       : 3.12.7
<br>
IPython version      : 8.24.0
<br>

<br>
xarray  : 2024.9.0
<br>
pytensor: 2.25.5
<br>

<br>
numpy     : 1.26.4
<br>
arviz     : 0.20.0
<br>
pymc      : 5.17.0
<br>
matplotlib: 3.9.2
<br>
pandas    : 2.2.3
<br>
seaborn   : 0.13.2
<br>

<br>
Watermark: 2.4.3
<br>
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/logistic_regression/" /><summary type="html"><![CDATA[Regression on count data]]></summary></entry><entry><title type="html">Logistic regression</title><link href="http://localhost:4000/statistics/logistic_regression" rel="alternate" type="text/html" title="Logistic regression" /><published>2024-11-13T00:00:00+00:00</published><updated>2024-11-13T00:00:00+00:00</updated><id>http://localhost:4000/statistics/logistic_regression</id><content type="html" xml:base="http://localhost:4000/statistics/logistic_regression"><![CDATA[In the last posts we discussed how to build
the simplest regression model for a real variable
with the linear model.
This model can be used as a starting block
to perform regression on many other types of data,
and this can be done by building
a **Generalized Linear Model** (GLM).

GLMs can be constructed by starting
from any likelihood for the
data $$P(y | \theta)\,.$$

The parameter $\theta$ usually is bounded to
some specific range $$D$$: we have
$$\theta \in [0, 1]$$ for the Binomial likelihood,
while we have $\theta > 0$ for the Poisson model.
On the other hand, the variable

$$Z \sim \alpha + \beta X$$

can generally take any real value.
However, by choosing a suitable function

$$ f : \mathbb{R} \rightarrow D$$

we can map our random variable $$Z$$ to
the desired domain $$D\,.$$

The general GLM can therefore be written as

$$
\begin{align}
Y & \sim P(\theta) \\
\theta & = f\left(\alpha + \beta X\right)
\end{align}
$$

Of course $\alpha$ and $\beta$ and any other parameter
$\phi$ will be described by a suitable prior distribution.

Let us now see how to do this in practice.

## The logistic model

The logistic model can be applied when there is a single binary dependent variable
which depends on one or more independent variables, which can be binary, integer or continuous.
In the logistic model the likelihood is taken as the binomial one,
while the mapping function $f$ is taken as the logistic function, plotted below:

$$ f(x) = \frac{1}{1+e^{-x}}$$

![The logistic function](/docs/assets/images/statistics/logistic/logistic.webp)

We will apply the logistic regression to the Challenger O-ring dataset. 
On January 28th 1986 the shuttle broke during the launch, killing several people,
and the USA president formed a commission to investigate on the causes of the incident.
One of the member of the commission was the physicist Richard Feynman,
who proved that the incident was caused by a loss of flexibility of the shuttle
O-rings caused by the low temperature 
(see the [Wikipedia page](https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disaster))
Here we will take the data on the number of O-rings damaged in each mission of the Challenger,
and we will provide an estimate on the probability that one o-ring becomes damaged as a function of the temperature.
The original data can be found [here](https://archive.ics.uci.edu/dataset/92/challenger+usa+space+shuttle+o+ring),
and we provide here the dataset grouped by temperature for completeness (the temperature is expressed in &deg;F).

|    |   temperature |   damaged |   undamaged |   count |
|---:|--------------:|----------:|------------:|--------:|
|  0 |            53 |         5 |           1 |       6 |
|  1 |            57 |         1 |           5 |       6 |
|  2 |            58 |         1 |           5 |       6 |
|  3 |            63 |         1 |           5 |       6 |
|  4 |            66 |         0 |           6 |       6 |
|  5 |            67 |         0 |          18 |      18 |
|  6 |            68 |         0 |           6 |       6 |
|  7 |            69 |         0 |           6 |       6 |
|  8 |            70 |         2 |          22 |      24 |
|  9 |            72 |         0 |           6 |       6 |
| 10 |            73 |         0 |           6 |       6 |
| 11 |            75 |         1 |          11 |      12 |
| 12 |            76 |         0 |          12 |      12 |
| 13 |            78 |         0 |           6 |       6 |
| 14 |            79 |         0 |           6 |       6 |
| 15 |            81 |         0 |           6 |       6 |

The dataset contains all the information collected before the Challenger disaster.
The logistic model is already implemented into PyMC, but to see how it works we will implement it from scratch.

```python
import pandas as pd
import pymc as pm
import arviz as az
import numpy as np
from matplotlib import pyplot as plt
import pymc.sampling_jax as pmjax
import seaborn as sns


rng = np.random.default_rng(42)

df_oring= pd.read_csv('./data/orings.csv')

# Convert it to Celsius

df_oring['deg'] = (df_oring['temperature']-32)*5/9
```

I converted the temperature to Celsius degree because it is easier for me to 
reason in terms of Celsius degree.
Let us write down our model

$$
\begin{align}
Y_i \sim & \mathcal{Binom}(p_i, n_i)\\
p_i = & 
\frac{1}{1+e^{-\alpha - \beta X_i}} 
\\
\end{align}
$$

The **odds ratio** is defined as 

$$
\begin{align}
\frac{p}{1-p} 
&
=
\frac{1}{1+e^{-\alpha - \beta X}}\frac{1}{1-\frac{1}{1+e^{-\alpha - \beta X}}}
\\
&
=
\frac{1}{1+e^{-\alpha - \beta X}}\frac{ 1+e^{-\alpha - \beta X} }{e^{-\alpha - \beta X}}
\\
&
= e^{\alpha + \beta X}
\end{align}
$$

therefore

$$
\log\left(\frac{p}{1-p}\right) = \alpha + \beta X
$$

We can therefore identify $\alpha$ with the log odds at $T=0&deg;C$
It doesn't really make sense to assume either a too big number or a too small one,
so we will take

$$
\alpha \sim \mathcal{N}(0, 15)
$$

On the other hand, $\beta$ represents the variation of the log odds with an increase of $1&deg;C\,.$
We do expect a meaningful variation on a scale of $10&deg;C\,,$ 
so we can generously take

$$
\beta \sim \mathcal{N}(0, 2)
$$

We are now ready to implement our model

```python
with pm.Model() as logistic:
    alpha = pm.Normal('alpha', mu=0, sigma=15)
    beta = pm.Normal('beta', mu=0, sigma=2)
    log_theta = alpha + beta*df_oring['deg']
    theta = 1/(1+pm.math.exp(-log_theta))
    y = pm.Binomial('y', p=theta, n=df_oring['count'], observed=df_oring['undamaged'])


with logistic:
    idata_logistic = pm.sample(draws=5000, tune=5000, chains=4,
                               random_seed=rng, nuts_sampler='numpyro')


az.plot_trace(idata_logistic)
fig = plt.gcf()
fig.tight_layout()
```

![The trace of the logistic model](/docs/assets/images/statistics/logistic/trace.webp)

The trace looks fine, we can now take a look at the posterior predictive.

```python
x_pl = np.arange(0, 30, 0.1)

with logistic:
    mu = pm.Deterministic('mu', alpha + beta*x_pl)
    p = pm.Deterministic('p', 1/(1+pm.math.exp(-mu)))

with logistic:
    idata_logistic.extend(pm.sample_posterior_predictive(idata_logistic, var_names=['y', 'mu', 'p'],
                                                        random_seed=rng))

fig = plt.figure()
ax = fig.add_subplot(111)
ax.fill_between(x_pl, 1-
                idata_logistic.posterior_predictive.p.quantile(q=0.025, dim=['draw', 'chain']),
                1-
                idata_logistic.posterior_predictive.p.quantile(q=0.975, dim=['draw', 'chain']),
                color='lightgray', alpha=0.7)

ax.plot(x_pl, 1-
        idata_logistic.posterior_predictive.p.mean(dim=['draw', 'chain']),
        color='k')

ax.scatter(df_oring['deg'], df_oring['damaged']/df_oring['count'],
           marker='x', label='raw data estimate')
ax.set_xlim([0, 30])
ax.set_ylim([-0.01, 1.01])
ax.set_xlabel(r"T $\degree$C")
ax.set_title(r"Fraction of damaged O-rings")
```

![The trace of the logistic model](/docs/assets/images/statistics/logistic/posterior_predictive.webp)

As we can see, the more we approach $0&deg;\,,$ the more it is likely that an O-ring gets damaged.
The forecasted temperature for the launch day was $26-29 &deg;F\,,$ corresponding to a range between
$-1.6$ &deg;C and $-3.3$ &deg;C.

We must however consider that one broken O-ring is not enough to create serious issues.
We can therefore estimate the probability as a function of the number of undamaged rings.

```python
tm = (26-32)*5/9
tM = (29-32)*5/9

with logistic:
    theta_m = 1/(1+np.exp(-(alpha + tm*beta)))
    ym = pm.Binomial('ym', p=theta_m, n=6)
    theta_M = 1/(1+np.exp(-(alpha + tm*beta)))
    yM = pm.Binomial('yM', p=theta_M, n=6)

with logistic:
    ppc_t = pm.sample_posterior_predictive(trace_logistic, var_names=['ym', 'yM'])

# We count how many O-rings are undamaged for each draw

hm = [(ppc_t.posterior_predictive['ym'].values.reshape(-1)==k).astype(int).sum() for k in range(7)]
hM = [(ppc_t.posterior_predictive['yM'].values.reshape(-1)==k).astype(int).sum() for k in range(7)]
h_0 = [k for k in range(7)]

# And we now estimate the corresponding probability

df_h = pd.DataFrame({'n': h_0, 'count_m': hm, 'count_M': hM})
df_h['prob_m'] = df_h['count_m']/df_h['count_m'].sum()
df_h['prob_M'] = df_h['count_M']/df_h['count_M'].sum()

df_h = pd.DataFrame({'n': h_0, 'count_m': hm, 'count_M': hM})

# Let us take a look at the best-case scenario
sns.barplot(df_h, x='n', y='prob_M')
```

![The probaility as a function of the undamaged rings](/docs/assets/images/statistics/logistic/best_case.webp)

```python
df_h[df_h['n']==0]['prob_M']
```

<div class=code>
0    0.9469
<br>
Name: prob_M, dtype: float64
</div>
The most probable scenario is that all O-rings get damaged, and this 
is scenario has, according to our model, the $95\%$ or probability to happen.

We can conclude that, with the available information,
it was not safe to perform the launch.
This is however a *post-hoc* analysis (an analysis performed on some
data once the outcome is known), and one should be really careful
to draw conclusions based on this kind of analysis, as this easily results
into false positive errors (see [the Wikipedia page on this topic](https://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data)).

## Conclusions

We introduced the Generalized Linear Model, and we analyzed the Challenger dataset
by means of a logistic regression.
We have seen how, by means of the GLM, we can easily extend the linear regression
to binary data.
In the next post we will discuss the Poisson regression.

## Suggested readings
- <cite><a href="http://www.stat.columbia.edu/~gelman/book/BDA3.pdf">Gelman, A. (2014). Bayesian Data Analysis, Third Edition. Taylor & Francis.</a></cite>

```python
%load_ext watermark
```

```python
%watermark -n -u -v -iv -w -p xarray,pytensor
```
<div class="code">
Last updated: Wed Nov 20 2024
<br>

<br>
Python implementation: CPython
<br>
Python version       : 3.12.7
<br>
IPython version      : 8.24.0
<br>

<br>
xarray  : 2024.9.0
<br>
pytensor: 2.25.5
<br>

<br>
pandas    : 2.2.3
<br>
numpy     : 1.26.4
<br>
seaborn   : 0.13.2
<br>
arviz     : 0.20.0
<br>
pymc      : 5.17.0
<br>
matplotlib: 3.9.2
<br>

<br>
Watermark: 2.4.3
<br>
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/logistic_regression/" /><summary type="html"><![CDATA[How to perform regression on binary data]]></summary></entry><entry><title type="html">Robust linear regression</title><link href="http://localhost:4000/statistics/robust_regression" rel="alternate" type="text/html" title="Robust linear regression" /><published>2024-11-06T00:00:00+00:00</published><updated>2024-11-06T00:00:00+00:00</updated><id>http://localhost:4000/statistics/robust_regression</id><content type="html" xml:base="http://localhost:4000/statistics/robust_regression"><![CDATA[In some case your data may be not good enough to provide you reliable estimates with normal linear regression,
and this is the case of the conclusions drawn from
[this](https://www.cambridge.org/core/journals/american-political-science-review/article/abs/political-institutions-and-voter-turnout-in-the-industrial-democracies/D6725BBF93F2F90F03A69B0794728BF7)
article, where the author concludes that there is a significant correlation between
the voter turnout in a country and its average income inequality.
This example is a classical example of misleading result of a regression,
where the author does not provide a plot of the data, taken from
[Healy, "Data visualization, a practical introduction"](
https://www.google.it/books/edition/Data_Visualization/3XOYDwAAQBAJ?hl=it&gbpv=1&dq=Data+visualization,+a+practical+introduction&printsec=frontcover).
The data below is extracted the data from the figure of Healy's book.
South Africa corresponds to the last point.

|    |   turnout |   inequality |
|---:|----------:|-------------:|
|  0 |  0.85822  |      1.95745 |
|  1 |  0.837104 |      1.95745 |
|  2 |  0.822021 |      2.41135 |
|  3 |  0.87632  |      2.76596 |
|  4 |  0.901961 |      2.95035 |
|  5 |  0.776772 |      3.21986 |
|  6 |  0.72549  |      3.14894 |
|  7 |  0.72549  |      2.92199 |
|  8 |  0.61991  |      2.93617 |
|  9 |  0.574661 |      2.31206 |
| 10 |  0.880845 |      3.60284 |
| 11 |  0.803922 |      3.5461  |
| 12 |  0.778281 |      3.47518 |
| 13 |  0.739065 |      3.68794 |
| 14 |  0.819005 |      4.41135 |
| 15 |  0.645551 |      3.91489 |
| 16 |  0.669683 |      5.64539 |
| 17 |  0.14178  |      9.30496 |

```python
import pandas as pd
import pymc as pm
import arviz as az
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
import pymc.sampling_jax as pmjax

df_turnout = pd.read_csv('data/inequality.csv')
rng = np.random.default_rng(42)
sns.pairplot(df_turnout)
```

![The dataset pairplot](/docs/assets/images/statistics/robust_regression/inequality_pairplot.webp)

By simply plotting the data we can clearly see that there is one point,
the South Africa, which is far away from the other,
and this may have a huge impact on the fit.
Let us see this, and how one may avoid this kind of error.

## The normal linear regression

Let us start by assuming that the inequality is distributed
according to a normal linear model,
analogous to the one already discussed in the [regression post](/linear_regression).

$$
Y \sim \mathcal{N}(\mu, \sigma)
$$

where

$$
\mu = \alpha + \beta X
$$

We will assume that the precision $\tau = 1/\sigma$ is distributed according to a Half Normal
distribution. Since the inequality goes from 0 to 10, assuming a
standard deviation of $5$ for $\tau$ should be sufficient.
On the other hand, we will make the quite generous assumption that

$$\alpha \sim \mathcal{N}(0, 20)$$

$$\beta \sim \mathcal{N}(0, 20)$$

```python
with pm.Model() as model_norm:
    alpha = pm.Normal('alpha', mu=0, sigma=20)
    beta = pm.Normal('beta', mu=0, sigma=20)
    tau = pm.HalfNormal('tau', sigma=5)
    y = pm.Normal('y', mu=alpha+df_turnout['turnout'].values*beta, observed=df_turnout['inequality'].values, tau=tau)

with model_norm:
    idata_norm = pm.sample(draws=4000, chains=4, tune=4000, nuts_sampler='numpyro',
                           idata_kwargs = {'log_likelihood': True}, random_seed=rng)

az.plot_trace(idata_norm)
```

![The trace of the normal model](/docs/assets/images/statistics/robust_regression/trace_norm.webp)

The trace doesn't show any relevant issue, and for our purposes it is
sufficient this check.
Let us check our fit

```python
x_plt = np.arange(0, 1, 0.001)

with model_norm:
    y_pred = pm.Normal('y_pred', mu=alpha+x_plt*beta, tau=tau)
    

with model_norm:
    idata_norm.extend(pm.sample_posterior_predictive(idata_norm, var_names=['y', 'y_pred'], random_seed=rng))

    
    
fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(x_plt, idata_norm.posterior_predictive['y_pred'].mean(dim=['draw', 'chain']))
ax.fill_between(x_plt, idata_norm.posterior_predictive['y_pred'].quantile(q=0.025, dim=['draw', 'chain']),
                idata_norm.posterior_predictive['y_pred'].quantile(q=0.975, dim=['draw', 'chain']), alpha=0.5, color='grey')
ax.scatter(df_turnout['turnout'].values, df_turnout['inequality'].values)
```

![The posterior preditcive distribution of our model](/docs/assets/images/statistics/robust_regression/ppc_norm.webp)

The error bands correctly reproduce almost all the data. However,
since the South Africa is far away from the other countries,
it may happen that its behavior strongly influences the fit.

Let us now use a more robust model.
In order to make it more robust, which in this context means
less sensitive to isolated data, let us take a t-Student likelihood
instead of a normal one.

We will leave the parameters $\alpha\,, \beta$ and $\tau = \frac{1}{\sigma}$
unchanged, but we must choose a prior for the number of degrees of
freedom $\nu\,.$

We want a robust estimate, so we want a prior with a small
number of degrees of freedom. However, $\nu \approx 0$
can be hard to handle from a numeric perspective,
since the resulting distribution decreases very slowly 
as one steps away from the peak.
For the above reason, we choose a Gamma prior with $\alpha=4$
and $\beta=2\,.$

```python
with pm.Model() as model_robust:
    alpha = pm.Normal('alpha', mu=0, sigma=20)
    beta = pm.Normal('beta', mu=0, sigma=20)
    tau = pm.HalfNormal('tau', sigma=5)
    nu = pm.Gamma('nu', alpha=4, beta=2)
    y = pm.StudentT('y', mu=alpha+df_turnout['turnout'].values*beta, observed=df_turnout['inequality'].values, sigma=1/tau, nu=nu)

with model_robust:
    idata_robust = pm.sample(draws=4000, chains=4, tune=4000, 
                             idata_kwargs = {'log_likelihood': True}, 
                             nuts_sampler='numpyro', random_seed=rng)

az.plot_trace(idata_robust)
```

![The trace of the robust model](/docs/assets/images/statistics/robust_regression/trace_robust.webp)

The trace doesn't show relevant issues, so we can compute the posterior predictive.

```python
with model_robust:
    y_pred = pm.StudentT('y_pred', mu=alpha+x_plt*beta, sigma=1/tau, nu=nu)

with model_robust:
    idata_robust.extend(pm.sample_posterior_predictive(idata_robust, var_names=['y', 'y_pred'], random_seed=rng))


fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(x_plt, idata_robust.posterior_predictive['y_pred'].median(dim=['draw', 'chain']))
ax.fill_between(x_plt, idata_robust.posterior_predictive['y_pred'].quantile(q=0.025, dim=['draw', 'chain']),
                idata_robust.posterior_predictive['y_pred'].quantile(q=0.975, dim=['draw', 'chain']), alpha=0.5, color='grey')
ax.scatter(df_turnout['turnout'].values, df_turnout['inequality'].values)
```

![The PPC of the robust model](/docs/assets/images/statistics/robust_regression/ppc_robust.webp)

This distribution does a better job in reproducing the data, but
it tells a very different story from the normal model.

While in fact in the above model an increase of the turnout
translated into a reduction of the average inequality,
with this robust model this conclusion does not appear so clearly.

Let us try and see what does the LOO can tell us.

```python
df_compare = az.compare({'Normal model': idata_norm, 'Robust model': idata_robust})

az.plot_compare(df_compare)
```

![The plot of the LOO cross-validation](/docs/assets/images/statistics/robust_regression/loo.webp)

The LOO is slightly better for the normal model,
they are however very similar. Let us try and understand why.

```python
df_compare
```

|              |   rank |   elpd_loo |   p_loo |   elpd_diff |   weight |      se |   dse | warning   | scale   |
|:-------------|-------:|-----------:|--------:|------------:|---------:|--------:|------:|:----------|:--------|
| Normal model |      0 |   -30.9967 | 4.92972 |     0       | 0.889385 | 4.39811 | 0     | True      | log     |
| Robust model |      1 |   -32.3574 | 6.88334 |     1.36077 | 0.110615 | 4.66233 | 1.855 | False     | log     |

The difference is $1.65\,,$ and the difference due to the number
of the degrees of freedom is the difference of the $p_{loo}\,,$
which is approximately 2.2, so the entire preference is due
to the lower number of degrees of freedom of the normal distribution.

We can see, however, that the LOO estimate for the normal
model has a warning. This generally happens because the ELPD
estimate is not exact, and it's only reliable when 
removing one point does not affect too much log predictive density.

```python
loo_normal = az.loo(idata_norm)

loo_normal
```

<div class=code>
Computed from 16000 posterior samples and 18 observations log-likelihood matrix.
<br>

<br>
         Estimate       SE
<br>
elpd_loo   -31.00     4.40
<br>
p_loo        4.93        -
<br>

<br>
There has been a warning during the calculation. Please check the results.
<br>
------
<br>

<br>
Pareto k diagnostic values:
<br>
                         Count   Pct.
<br>
(-Inf, 0.5]   (good)       16   88.9%
<br>
 (0.5, 0.7]   (ok)          1    5.6%
<br>
   (0.7, 1]   (bad)         0    0.0%
<br>
   (1, Inf)   (very bad)    1    5.6%
<br>
</div>

There are two points which strongly affect our parameters,
and one reasonable assumption is that one of those is the South Africa.

Let us try and see what does it happen once we remove it.

```python
with pm.Model() as model_norm_red:
    alpha = pm.Normal('alpha', mu=0, sigma=20)
    beta = pm.Normal('beta', mu=0, sigma=20)
    tau = pm.HalfNormal('tau', sigma=5)
    y = pm.Normal('y', mu=alpha+df_turnout['turnout'].values[:17]*beta, observed=df_turnout['inequality'].values[:17], tau=tau)

with model_norm_red:
    idata_norm_red = pm.sample(draws=2000, chains=4, tune=2000,
                               idata_kwargs = {'log_likelihood': True},
                               nuts_sampler='numpyro',
                               random_seed=rng)
    
az.plot_trace(idata_norm_red)

```

![The trace for the new normal model](/docs/assets/images/statistics/robust_regression/trace_norm_red.webp)

```python
with model_norm_red:
    y_pred_red = pm.Normal('y_pred', mu=alpha+x_plt*beta, tau=tau)


with model_norm_red:
    idata_norm_red.extend(pm.sample_posterior_predictive(idata_norm_red, var_names=['y', 'y_pred'], random_seed=rng))
    
fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(x_plt, idata_norm_red.posterior_predictive['y_pred'].mean(dim=['draw', 'chain']))
ax.fill_between(x_plt, idata_norm_red.posterior_predictive['y_pred'].quantile(q=0.025, dim=['draw', 'chain']),
                idata_norm_red.posterior_predictive['y_pred'].quantile(q=0.975, dim=['draw', 'chain']), alpha=0.5, color='grey')
ax.scatter(df_turnout['turnout'].values, df_turnout['inequality'].values)
```

![The PPC for the new model](/docs/assets/images/statistics/robust_regression/ppc_norm_red.webp)

This result looks much more to the robust estimate than to 
the full normal estimate.
While in the full normal model the parameter beta was
not compatible with 0, both for the robust and for the reduced
normal model it is.
This implies that those models contradict the full normal model,
which shows a negative association between the turnover
and the average income inequality.
Since the conclusion of the full normal model are heavily 
affected by the South Africa, before drawing
any conclusion one should carefully assess whether it
makes sense or not. Is the South Africa really representative or
is it a special case?

## Conclusions

We have discussed how to perform a robust linear regression,
and we have shown with an example that using it instead of a normal
linear regression makes our model more stable to the presence
of non-representative items.


## Suggested readings
- <cite>Healy, K. (2019). Data Visualization: A Practical Introduction. Princeton University Press.
</cite>

```python
%load_ext watermark
```

```python
%watermark -n -u -v -iv -w -p xarray,pytensor
```
<div class="code">
Last updated: Wed Nov 20 2024
<br>

<br>
Python implementation: CPython
<br>
Python version       : 3.12.7
<br>
IPython version      : 8.24.0
<br>

<br>
xarray  : 2024.9.0
<br>
pytensor: 2.25.5
<br>

<br>
pandas    : 2.2.3
<br>
seaborn   : 0.13.2
<br>
matplotlib: 3.9.2
<br>
pymc      : 5.17.0
<br>
numpy     : 1.26.4
<br>
arviz     : 0.20.0
<br>

<br>
Watermark: 2.4.3
<br>
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/robust_regression/" /><summary type="html"><![CDATA[Reducing sensitivity to large deviations]]></summary></entry><entry><title type="html">Multi-linear regression</title><link href="http://localhost:4000/statistics/multivariate_regression" rel="alternate" type="text/html" title="Multi-linear regression" /><published>2024-10-30T00:00:00+00:00</published><updated>2024-10-30T00:00:00+00:00</updated><id>http://localhost:4000/statistics/multivariate_regression</id><content type="html" xml:base="http://localhost:4000/statistics/multivariate_regression"><![CDATA[When dealing with real-world datasets, you will often only
have to deal with more than one independent variable.
Here we will see how to adapt our framework to this case.
As you will see, doing so is straightforward, at least in theory.
In practice, this is not true, as deciding how to improve your model may be a tricky question,
and only practice and domain knowledge will, sometimes, help you in this task.

## The dataset

In this post, we will use the dataset provided in [this](https://www.tandfonline.com/doi/full/10.1080/10691898.2001.11910659)
very nice article, where the aim of the author is to show some of the difficulties
one faces when dealing with real World datasets.
The aim is to predict the price of a set of diamonds, given their carat numbers,
their color, their clarity and their certification.
I found this dataset in [this amazing repo](https://vincentarelbundock.github.io/Rdatasets/datasets.html)

```python
import seaborn as sns
import pandas as pd
from matplotlib import pyplot as plt
import numpy as np
import pymc as pm
import arviz as az

df = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Diamond.csv')

df.head()
```

|    |   rownames |   carat | colour   | clarity   | certification   |   price |
|---:|-----------:|--------:|:---------|:----------|:----------------|--------:|
|  0 |          1 |    0.3  | D        | VS2       | GIA             |    1302 |
|  1 |          2 |    0.3  | E        | VS1       | GIA             |    1510 |
|  2 |          3 |    0.3  | G        | VVS1      | GIA             |    1510 |
|  3 |          4 |    0.3  | G        | VS1       | GIA             |    1260 |
|  4 |          5 |    0.31 | D        | VS1       | GIA             |    1641 |

It is known that white, clear diamonds look brighter, and because of this they
have higher price than more opaque or colorful diamonds.
When considering colour and clarity, one should however keep in mind
that these values are assigned by experts, and two experts might provide
different values for the same diamond.

Let us now take a look at the relation between carat and price.

```python
sns.scatterplot(df, x='carat', y='price')
```
![](/docs/assets/images/statistics/multilinear/scatter.webp)

From the above figure, we can see that it is unlikely that a linear fit would
work, since the dataset shows a very pronounced heteroskedasticity.
In order to improve the homoscedasticity, we can try the following transformation

```python
df['log_price'] = np.log(df['price'])
sns.scatterplot(df, x='carat', y='log_price')
```
![](/docs/assets/images/statistics/multilinear/scatter_log.webp)

The above transformation improved the homoscedasticity, so we now have higher chances
to be able to properly fit the dataset.

Let us now take a look at the categorical columns.

```python
df.certification.drop_duplicates()
```

<div class="code">
0      GIA
<br>
151    IGI
<br>
229    HRD
<br>
Name: certification, dtype: object
</div>

```python
df.colour.drop_duplicates()
```

<div class="code">
0    D <br>
1    E <br>
2    G <br>
6    F <br>
8    H <br>
9    I <br>
Name: colour, dtype: object
</div>

```python
df.clarity.drop_duplicates()
```

<div class="code">
0      VS2 <br>
1      VS1 <br>
2     VVS1 <br>
7     VVS2 <br>
83      IF <br>
Name: clarity, dtype: object
</div>

These columns encode categories, and we should treat them by making an
indicator variable for each possible value of the three variables.

Taking the color as an example, we will take one value as baseline (say "D") such that
all the indicator variables are zero for it.
We will then define four indicator variables "E", "F", "G" and "H"
with value 0 if the color is not the one corresponding to the variable,
1 otherwise.
This can be easily done with pandas as follows:

```python
df_col = pd.get_dummies(df['colour'], drop_first=True).astype(int)

df_clar = pd.get_dummies(df['clarity'], drop_first=True).astype(int)

df_cert = pd.get_dummies(df['certification'], drop_first=True).astype(int)

df_cat = pd.concat([df_col, df_clar, df_cert], axis=1)
```

We can now try and fit the data with a linear model.
We will use two additional features which PyMC provides us, namely the
"coords" option and the "Data" class.

```python
coords = {'ind': df_cat.index, 'col': df_cat.columns}

yobs = df['log_price'].values

rng = np.random.default_rng(42)

with pm.Model(coords=coords) as model:
    alpha = pm.Normal('alpha', mu=0, sigma=10)
    X_cat = pm.Data('X_cat', value=df_cat, dims=['obs_idx', 'feature'])
    X_carat = pm.Data('X_carat', value=(df['carat']-df['carat'].mean())/(2*df['carat'].std()), dims=['obs_idx'])
    beta_cat = pm.Normal('beta_cat', dims=['feature'], mu=0, sigma=10)
    beta_carat = pm.Normal('beta_carat', mu=0, sigma=10)
    sigma = pm.HalfNormal('sigma', sigma=20)
    mu = pm.Deterministic('mu', alpha + pm.math.dot(beta_cat, X_cat.T) + beta_carat*X_carat)
    y = pm.Normal('y', mu=mu, sigma=sigma, observed=yobs)

pm.model_to_graphviz(model)
```

![](/docs/assets/images/statistics/multilinear/model.webp)

In our model $\alpha$ is the intercept for the baseline diamond, $\beta_{cat}$ the average log-price
difference associated to the categorical variables and $\beta_{carat}$ the slope,
while $sigma$ is the standard deviation of our model.

As explained by Gelman, it is often suitable to replace a continuous regressor
$X$ with its standardized  version, as we did in our model,
in order to simplify the comparison between the corresponding
variable and the ones associated to discrete variables.
We divided by two standard deviations so that a difference between $-\sigma$
and $\sigma$ is not mapped into a $\Delta X = 1\,.$

By using the standardized  regressor, we also have two more advantages.
The first one is that the parameter $\alpha$ is now associated with an observable quantity,
namely the value of the log price when the carat number is equal to the average
carat number, and we don't need to relate it to the extrapolated log price
when the carat number is 0.
The second advantage is that it is now easier to guess a prior for $\beta_{carat}\,,$
while it might not be so easy to do the same for the un-standardized regressor.

We can now fit our model

```python
with model:
    idata = pm.sample(nuts_sampler='numpyro', random_seed=rng)

az.plot_trace(idata, var_names=['alpha', 'beta', 'sigma'], filter_vars='like')
fig = plt.gcf()
fig.tight_layout()
```


![](/docs/assets/images/statistics/multilinear/trace_model.webp)

The traces look fine, let us now take a look at the posterior predictive

```python
with model:
    idata.extend(pm.sample_posterior_predictive(idata, random_seed=rng))

az.plot_ppc(idata)
```

![](/docs/assets/images/statistics/multilinear/ppc_model.webp)

It doesn't look like our model is appropriate to describe the data:
the log-price is overestimated close to the tails, while it is underestimated
close to the center of the distribution.

If we look again at the carat vs log-price scatterplot,
it looks like close to 0 the behavior is not polynomial, and this
suggests us that we should use, as independent variable,
a function of $x$ which is not analytic in 0.
The two most common choices are the logarithm and the square root.
We will follow the linked article and use the square root.

```python
with pm.Model(coords=coords) as model_s:
with pm.Model(coords=coords) as model_s:
    alpha = pm.Normal('alpha', mu=0, sigma=10)
    X_cat = pm.Data('X_cat', value=df_cat, dims=['obs_idx', 'feature'])
    X_carat = pm.Data('X_carat', value=(np.sqrt(df['carat'])-np.mean(np.sqrt(df['carat'])))/(np.mean(np.sqrt(df['carat']))), dims=['obs_idx'])
    beta_cat = pm.Normal('beta_cat', dims=['feature'], mu=0, sigma=10)
    beta_carat = pm.Normal('beta_carat', mu=0, sigma=10)
    sigma = pm.HalfNormal('sigma', sigma=20)
    mu = pm.Deterministic('mu', alpha + pm.math.dot(beta_cat, X_cat.T) + beta_carat*X_carat)
    y = pm.Normal('y', mu=mu, sigma=sigma, observed=yobs)
    idata_s = pm.sample(nuts_sampler='numpyro', random_seed=rng)

az.plot_trace(idata_s, var_names=['alpha', 'beta', 'sigma'], filter_vars='like')
fig = plt.gcf()
fig.tight_layout()
```


![](/docs/assets/images/statistics/multilinear/trace_model_s.webp)

Also in this case the trace looks fine. Let us now look at the posterior predictive
distribution

```python
with model_s:
    idata_s.extend(pm.sample_posterior_predictive(idata_s, random_seed=rng))

az.plot_ppc(idata_s)

```

![](/docs/assets/images/statistics/multilinear/ppc_model_s.webp)

It looks like the result slightly improved.
Let us try and compare the two models.

```python
with model:
    pm.compute_log_likelihood(idata)

with model_s:
    pm.compute_log_likelihood(idata_s)

df_comp = az.compare({'linear': idata, 'square root': idata_s})

df_comp
```

|             |   rank |   elpd_loo |   p_loo |   elpd_diff |      weight |      se |     dse | warning   | scale   |
|:------------|-------:|-----------:|--------:|------------:|------------:|--------:|--------:|:----------|:--------|
| square root |      0 |    363.968 | 13.2115 |       0     | 1           | 9.61651 | 0       | False     | log     |
| linear      |      1 |    164.762 | 13.7875 |     199.206 | 6.12772e-10 | 9.96487 | 8.65212 | False     | log     |

```python
az.plot_compare(df_comp)
fig = plt.gcf()
fig.tight_layout()
```

![](/docs/assets/images/statistics/multilinear/compare_1.webp)

There are no warnings, we can therefore consider the estimate
as reliable, and there is no doubt that the latter model greatly improved the result.

Let us also take a look at the LOO-PIT

```python
fig, ax = plt.subplots(ncols=2, figsize=(15, 6))
az.plot_loo_pit(idata_s, y='y', ax=ax[0])
az.plot_loo_pit(idata_s, y='y', ax=ax[1], ecdf=True)
fig.tight_layout()
```

![](/docs/assets/images/statistics/multilinear/loo_pit_s.webp)

There is still margin for improvement, as it doesn't really look like
the LOO-PIT is compatible with the uniform distribution.
We won't however improve our model for now.

Let us instead inspect the impact of the categorical variables on the log price

```python
fig = plt.figure()
ax = fig.add_subplot(111)
az.plot_forest(idata_s, var_names='beta_cat', filter_vars='like', ax=ax, combined=True)
ax.set_yticklabels(df_cat.columns)
ax.axvline(x=0, ls=':', color='grey')
fig.tight_layout()
```

![](/docs/assets/images/statistics/multilinear/forest_s.webp)

All the categorical variables has an important effect on the log price,
as we expected.
In this case, we already knew which variables to include in our model,
in general it won't be this case, as you might have more variables than needed.

Gelman, in the textbook "Data Analysis Using Regression and Multilevel/Hierarchical Models".
suggests the following method to decide which variables are relevant:

Keep all variables that you expect might be relevant in the outcome prediction.
If you also have additional variables:

| A parameter               | has the expected sign | does not have the expected sign                                                    |
|:--------------------------|-----------------------|------------------------------------------------------------------------------------|
| **is not significant** | Keep it               | Don't keep it                                                                      |
| **is significant**        | Keep it               | You should ask yourself why this is happening. Are you not considering a variable? |

Finally, if an independent variable has a large effect, consider including an interaction
term.

## Conclusion

The regression with multiple variables is a deep topic, and we barely introduced
the main concepts and gave few hints on how to work with this kind of model.
We did so by using a real-World dataset, and we also showed some of the issues one 
might face when dealing with problematic datasets.



## Suggested readings
- <cite>Gelman, A., Hill, J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models. CUP.
</cite>

```python
%load_ext watermark
```

```python
%watermark -n -u -v -iv -w -p xarray,pytensor
```

<div class="code">
Last updated: Wed Nov 20 2024
<br>

<br>
Python implementation: CPython
<br>
Python version       : 3.12.7
<br>
IPython version      : 8.24.0
<br>

<br>
xarray  : 2024.9.0
<br>
pytensor: 2.25.5
<br>

<br>
matplotlib: 3.9.2
<br>
arviz     : 0.20.0
<br>
seaborn   : 0.13.2
<br>
pandas    : 2.2.3
<br>
pymc      : 5.17.0
<br>
numpy     : 1.26.4
<br>

<br>
Watermark: 2.4.3
<br>
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/linear_regression/" /><summary type="html"><![CDATA[Including many covariates]]></summary></entry><entry><title type="html">Linear regression with binary input</title><link href="http://localhost:4000/statistics/regression_binary_input" rel="alternate" type="text/html" title="Linear regression with binary input" /><published>2024-10-23T00:00:00+00:00</published><updated>2024-10-23T00:00:00+00:00</updated><id>http://localhost:4000/statistics/regression_binary_input</id><content type="html" xml:base="http://localhost:4000/statistics/regression_binary_input"><![CDATA[Linear regression can be straightforwardly applied when the independent variable
$X$ is discrete. One should only pay attention to the interpretation
of the parameters in this case, as the interpretation provided in the 
previous post may not apply.

## The model

Here we will use the "Student alcohol consumption"
dataset, available on [this data.world page](https://data.world/databeats/student-alcohol-consumption/).
In this study, the authors analyzed the relationship between
alcohol consumption and many aspects of the student's life, including
the school behavior.
We will analyze the difference in the math grades between male and female
students.

Our model will be the following

$$
y_i = \mathcal{N}(\mu_i, \sigma)
$$

where

$$
\mu_i = \beta_0 + \beta_1 x_i
$$

and

$$
x_i =
\begin{cases}
0 & if\, x=F\\
1 & if\, x=M
\end{cases}
$$

In this case we have that, for female students,
the average grade is $\beta_0\,,$
while for male students it is $\beta_0 + \beta_1\,.$

The female group and, more generally, the group 
with average dependent variable $\beta_0\,,$ is called the **reference group**.

The parameter $\beta_1$ can be now interpreted as the difference
between the average male and female grade.


## The implementation

Let us now see how to implement this model

```python
import seaborn as sns
import pandas as pd
from matplotlib import pyplot as plt
import numpy as np
import pymc as pm
import arviz as az

df = pd.read_csv('https://query.data.world/s/qz5sf27veajjivl3bpa5npazsxzn7z?dws=00000')

```

Let us now only keep the columns we are interested in

```python

df_red = df[['sex', 'G3']]

df_red['sex'] = df_red['sex'].astype('category')

df_red.head()
```


|    | sex   |   G3 |
|---:|:------|-----:|
|  0 | F     |    6 |
|  1 | F     |    6 |
|  2 | F     |   10 |
|  3 | F     |   15 |
|  4 | F     |   10 |

We can now easily build our model.
We could easily do this as follows:

```python
x = (df_red['sex']=='M').astype(int)
```

This method works fine for a binary category, but this method becomes
cumbersome as the number of categories grows.
Fortunately, pandas provides a builtin function to do this job,
namely the "factorize" function.

```python
x, cat_data = pd.factorize(df_red['sex'])

cat_data
```
<div class="code">
CategoricalIndex(['F', 'M'], categories=['F', 'M'], ordered=False, dtype='category')
</div>

Since the first category is "F", the females will be associated 
to the 0 values in x, while males will be associated to 1.

We can now implement the model

```python
rng = np.random.default_rng(42)
with pm.Model() as model:
    beta = pm.Normal('beta', mu=0, sigma=20, shape=(2))
    sigma = pm.HalfNormal('sigma', sigma=20)
    mu = beta[0] + beta[1]*x
    y = pm.Normal('y', mu=mu, sigma=sigma, observed=df_red['G3'])
    idata = pm.sample(nuts_sampler='numpyro', random_seed=rng)

az.plot_trace(idata)
fig = plt.gcf()
fig.tight_layout()
```

![The trace for our simple model](/docs/assets/images/statistics/regression_binary/trace.webp)

Let us take a better look to the beta parameters

```python
az.plot_forest(idata, var_names='beta')
```

![The forest plot for the beta parameters](/docs/assets/images/statistics/regression_binary/forest.webp)

It looks like, in this study, the male students 
have on average a slightly higher math grade than the female students.

## Posterior predictive checks

Let us now verify if the observed data are included within the predicted
uncertainties

```python
with model:
    ppc = pm.sample_posterior_predictive(idata, random_seed=rng)

fig = plt.figure()
ax = fig.add_subplot(111)
for yt in az.extract(ppc, num_samples=20, group='posterior_predictive')['y'].T:
    s = rng.uniform(low=-0.1, high=0.1, size=len(x))
    ax.scatter(x+s, yt, color='lightgray', s=5)
ax.scatter(x, df_red['G3'], s=5)
ax.set_xticks([0, 1])
ax.set_xticklabels(['F', 'M'])
```

![The posterior predictive for our model](/docs/assets/images/statistics/regression_binary/ppc.webp)

In the above code block, we "jittered" (added a small random number
to the x variable) in order to be able to distinguish the points.

The posterior predictive looks good, so we can conclude that
on average the male students performed slightly better than the female
ones.
We could of course improve our model by imposing that the grade
is non-negative, but if we simply want to investigate the mean
difference, this model is enough for our purposes.
We want to stress that this does not imply that being males makes
you perform better at math with respect to females or vice versa,
this would be wrong for many reasons.
First of all, as we will discuss in the section about causal inference, this
statement has no meaning in the counterfactual definition of causality,
as you cannot manipulate someone's biological sex.

## Conclusions

We extended the linear regression model to a binary outcome,
and discussed the interpretation of the parameter's models when
the binary regressor encodes a categorical variable.

```python
%load_ext watermark
```

```python
%watermark -n -u -v -iv -w -p xarray,pytensor,numpyro,jax,jaxlib
```

<div class="code">
Last updated: Wed Nov 20 2024
<br>

<br>
Python implementation: CPython
<br>
Python version       : 3.12.7
<br>
IPython version      : 8.24.0
<br>

<br>
xarray  : 2024.9.0
<br>
pytensor: 2.25.5
<br>
numpyro : 0.15.0
<br>
jax     : 0.4.28
<br>
jaxlib  : 0.4.28
<br>

<br>
arviz     : 0.20.0
<br>
pandas    : 2.2.3
<br>
pymc      : 5.17.0
<br>
numpy     : 1.26.4
<br>
seaborn   : 0.13.2
<br>
matplotlib: 3.9.2
<br>

<br>
Watermark: 2.4.3
<br>
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/binary_linear_regression/" /><summary type="html"><![CDATA[Extending regression to discrete variables]]></summary></entry><entry><title type="html">Horseshoe priors</title><link href="http://localhost:4000/statistics/horseshoe" rel="alternate" type="text/html" title="Horseshoe priors" /><published>2024-10-20T00:00:00+00:00</published><updated>2024-10-20T00:00:00+00:00</updated><id>http://localhost:4000/statistics/horseshoe</id><content type="html" xml:base="http://localhost:4000/statistics/horseshoe"><![CDATA[When you perform multilinear regression with many independent variables,
having highly correlated regressors might have the undesired drawback that 
most of the associated parameters are barely different from zero.
As an example, you might want to find an approximation for some quantity,
and you want to include as fewer regressors as possible, maybe because
measuring each of them requires effort.
Alternatively, if your aim is to assess the impact of some regressor
on your model, the instabilities originated by multi-collinearity
might lead you to drawing wrong conclusions.

In the frequentist framework, this issue is generally solved by adding
regularizing priors, as in the case of LASSO regression.
In the Bayesian one, the natural way to overcome this problem
is by means of an appropriate choice of the priors for the regressors,
and the Bayesian community proposed the family of **sparsifying priors**.
Here we will only discuss the horseshoe prior, but many more have been
proposed, and we will provide some link the topic.

## The Body Fat dataset
In this example, we will use the Body Fat dataset,
which is discussed in [this blog](https://stat-ata-asu.github.io/PredictiveModelBuilding/BFdata.html).

The body fat percentage is the mass of the fat of the body
divided by the total mass. This quantity can be accurately estimated
by using some dedicated instrument, which are however time-consuming
and require some knowledge in order to be used.
In order to have a less precise measurement, we can try and estimate
it by relating it to some other quantity which is easier to perform,
such as weight or length measurements.
While it is reasonable to use the body weight, it is however less clear
which length measurement we should use in order to find a good
proxy for the desired quantity.
We could use the total height, but we could also use many more
length measurements, such as the neck circumference or the abdomen one.

A starting point can be to use a multilinear regression in order to
find out which quantity better predicts the body fat percentage.
However, a more robust person will have large values for all
of them, we can therefore expect that our dataset will show a quite
strong multicollinearity.

```python
import numpy as np
import pandas as pd
import pymc as pm
import arviz as az
import seaborn as sns
from matplotlib import pyplot as plt

rng = np.random.default_rng(42)

df_bf = pd.read_csv('http://jse.amstat.org/datasets/fat.dat.txt', header=None, sep=r"\s+")

df_bf.columns = ["case", "brozek", "siri", 
                                    "density", "age", 
                                    "weight_lbs", 
                                    "height_in", "bmi", 
                                    "fat_free_weight", "neck_cm", 
                                    "chest_cm", "abdomen_cm", 
                                    "hip_cm", "thigh_cm", 
                                    "knee_cm", "ankle_cm", 
                                    "biceps_cm", "forearm_cm",
                                    "wrist_cm"]
```

There are at least two formulas to estimate the body fat percentage,
and we will use the "Brozek" formula.

```python
df.head()
```

|    |   case |   brozek |   siri |   density |   age |   weight_lbs |   height_in |   bmi |   fat_free_weight |   neck_cm |   chest_cm |   abdomen_cm |   hip_cm |   thigh_cm |   knee_cm |   ankle_cm |   biceps_cm |   forearm_cm |   wrist_cm |
|---:|-------:|---------:|-------:|----------:|------:|-------------:|------------:|------:|------------------:|----------:|-----------:|-------------:|---------:|-----------:|----------:|-----------:|------------:|-------------:|-----------:|
|  0 |      1 |     12.6 |   12.3 |    1.0708 |    23 |       154.25 |       67.75 |  23.7 |             134.9 |      36.2 |       93.1 |         85.2 |     94.5 |       59   |      37.3 |       21.9 |        32   |         27.4 |       17.1 |
|  1 |      2 |      6.9 |    6.1 |    1.0853 |    22 |       173.25 |       72.25 |  23.4 |             161.3 |      38.5 |       93.6 |         83   |     98.7 |       58.7 |      37.3 |       23.4 |        30.5 |         28.9 |       18.2 |
|  2 |      3 |     24.6 |   25.3 |    1.0414 |    22 |       154    |       66.25 |  24.7 |             116   |      34   |       95.8 |         87.9 |     99.2 |       59.6 |      38.9 |       24   |        28.8 |         25.2 |       16.6 |
|  3 |      4 |     10.9 |   10.4 |    1.0751 |    26 |       184.75 |       72.25 |  24.9 |             164.7 |      37.4 |      101.8 |         86.4 |    101.2 |       60.1 |      37.3 |       22.8 |        32.4 |         29.4 |       18.2 |
|  4 |      5 |     27.8 |   28.7 |    1.034  |    24 |       184.25 |       71.25 |  25.6 |             133.1 |      34.4 |       97.3 |        100   |    101.9 |       63.2 |      42.2 |       24   |        32.2 |         27.7 |       17.7 |

We will use only a subset of variables as regressors,
and in order to make evident the effect of the multicollinearity,
we will only use the first 40 measurements.

```python
indx = [5, 6, 11, 12, 13, 14, 15, 16, 17, 18]

df = df_bf.iloc[:40]

yobs = df['brozek']/100

X = df[df.columns[indx]]

Xnorm = (X-np.mean(X, axis=0))/np.std(X, axis=0)/2

sns.pairplot(Xnorm)
```

![The pairplot of the regressors](
/docs/assets/images/statistics/horseshoe/pairplot.webp)

As you can see by the above plot, the regressors dataset shows
a pronounced multi-collinearity, since they are not
pairwise-independent.
In order to facilitate the comparison across coefficients, we will
use the (Gelman's version of the) standardized coefficients.
Since the choice of a suitable scale for the prior is very important,
we scaled the observation by a factor 100,
and in this way the target variable ranges from 0 to 1.

Let us first try and see what happens if we only include one variable
into a linear regression model.

```python
starting_models = []
for i in range(len(X.columns)):
    with pm.Model() as mod:
        alpha = pm.Normal('alpha', sigma=1000)
        sigma = pm.HalfNormal('sigma', sigma=1000)
        beta = pm.Normal('beta', mu=0, sigma=1000)
        mu = alpha + beta*Xnorm[X.columns[i]]
        y = pm.Normal('y', mu=mu, sigma=sigma, observed=yobs)
        idata = pm.sample(nuts_sampler='numpyro', random_seed=rng)
        # pm.compute_log_likelihood(idata)
        # idata.extend(pm.sample_posterior_predictive(idata, random_seed=rng))
        starting_models.append([{'vars': [X.columns[i]], 'model': mod, 'idata': idata}])

traces = [model[0]['idata'] for model in starting_models]
names = [model[0]['vars'][0] for model in starting_models]

fig, ax = plt.subplots(figsize=(5, 9))
az.plot_forest(traces, model_names=names, var_names=['beta'], combined=True, ax=ax)
ax.axvline(x=0, ls=':', color='lightgray')
```

![The forest plot for
the regression coefficients across the different models](/docs/assets/images/statistics/horseshoe/forest_single.webp)

From the above credible intervals, we conclude that height_in, ankle_cm, forearm_cm
are wrist_cm compatible with 0.
Let us include all except these variables into a multilinear regression.

```python
indx_pos = [k for k, trace in enumerate(traces)
            if (az.summary(trace, var_names=['beta'])['hdi_3%']*az.summary(trace, var_names=['beta'])['hdi_97%']).values[0]>0]

Xtmp = Xnorm[[X.columns[k] for k in indx_pos]]

with pm.Model(coords={'cols': Xtmp.columns, 'idx': Xtmp.index}) as model_2:
        alpha = pm.Normal('alpha', sigma=1000)
        sigma = pm.HalfNormal('sigma', sigma=1000)
        beta = pm.Normal('beta', mu=0, sigma=1000, dims=['cols'])
        mu = alpha + pm.math.dot(beta, Xtmp.T)
        y = pm.Normal('y', mu=mu, sigma=sigma, observed=yobs)

with model_2:
    idata_2 = pm.sample(nuts_sampler='numpyro', random_seed=rng)

az.plot_trace(idata_2)
fig = plt.gcf()
fig.tight_layout()
```

![](/docs/assets/images/statistics/horseshoe/trace_2.webp)

We only have few non-zero regression coefficients:

```python
fig, ax = plt.subplots(figsize=(5, 4))
az.plot_forest(idata_2, var_names=['beta'], ax=ax, combined=True)
ax.axvline(x=0, ls=':', color='lightgray')
```

![](/docs/assets/images/statistics/horseshoe/forest_2.webp)

Only two of them appear to be different from zero.
This kind of iterative procedure to prune away the irrelevant variables
is a possible way to proceed. At this point, we could only include 
weight and abdomen, and maybe include one more regressor per time.
This is however both time-consuming and questionable: one might
ask why did you proceed in such an order, and whether using
a different order might have changed the result.
Let us as an example start by using all the variables.

```python
with pm.Model(coords={'ind': X.index, 'col': X.columns}) as model:
    sigma = pm.HalfNormal('sigma', sigma=1000)
    alpha = pm.Normal('alpha', mu=0, sigma=1000)
    beta = pm.Normal('beta', mu=0, sigma=1000, dims=['col'])
    mu = alpha + pm.math.dot(beta, Xnorm.T)
    y = pm.Normal('y', mu=mu, sigma=sigma, observed=yobs)

with model:
    idata = pm.sample(nuts_sampler='numpyro', draws=5000, tune=5000, random_seed=rng, target_accept=0.9)

az.plot_trace(idata)
fig = plt.gcf()
fig.tight_layout()
```

![The trace of the full model](/docs/assets/images/statistics/horseshoe/trace_full.webp)

```python
fig = plt.figure(figsize=(6, 10))
ax = fig.add_subplot(111)
az.plot_forest(idata, var_names=['beta'], ax=ax)
ax.axvline(x=0, color='lightgrey', ls=':')
fig.tight_layout()
```

![](/docs/assets/images/statistics/horseshoe/forest_full.webp)

As you can see, we would have a different result.

As previously anticipated, there is an alternative way to proceed, and
is by using an appropriate set of priors.
A reasonable choice is given by

$$
\begin{align}
\beta_i \sim & \mathcal{N}(0, \sigma \tau_i)\\
\tau_i \sim & \mathcal{HalfCauchy}(0, 1)\\
\end{align}
$$

The reason for this is quite simple: the half-Cauchy distribution
has a large amount of mass close to zero, but it also has
fat tails. In this way, the posterior is either shrunk to zero,
or far away from zero, and intermediate results are somehow
"discouraged" by the prior.

How to properly choose $\sigma$ is not trivial, and a recommended choice
is 

$$
\sigma \sim \mathcal{HalfCauchy}(0, a)
$$

A large value of $a$ will not strongly affect the posterior,
while a small value will shrink the posterior to 0.
In our case, we choose $a=1\,,$ which is quite a large value
if we compare it to the expected effect of the regression
coefficients.
We leave to the reader the sensitivity analysis of the inference
depending on the choice of $a$.
You should keep in mind that your conclusions might strongly depend
on the choice of $a$, so in this case a proper sensitivity analysis
is strongly recommended.

```python
with pm.Model(coords={'ind': X.index, 'col': X.columns}) as model_hh:
    sigma = pm.HalfNormal('sigma', sigma=1000)
    alpha = pm.Normal('alpha', mu=0, sigma=1000)
    sig_beta = pm.HalfCauchy('sig_beta', beta=1, dims=['col'])
    sig = pm.HalfCauchy('sig', 1)
    mu_beta = pm.Normal('mu_beta', mu=0, sigma=100)
    beta = pm.Deterministic('beta', mu_beta+sigma*sig_beta, dims=['col'])
    mu = alpha + pm.math.dot(beta, Xnorm.T)
    y = pm.Normal('y', mu=mu, sigma=sigma, observed=yobs, dims=['ind'])

with model_hh:
    idata_hh = pm.sample(nuts_sampler='numpyro', draws=5000, tune=5000, random_seed=rng, target_accept=0.9)

az.plot_trace(idata_hh)
fig = plt.gcf()
fig.tight_layout()
```

![](/docs/assets/images/statistics/horseshoe/trace_hh.webp)

```python
fig = plt.figure(figsize=(6, 10))
ax = fig.add_subplot(111)
# az.plot_forest({'std': idata, 'hh': idata_hh}, var_names=['beta'], ax=ax, model_names=['std', 'hh'])
az.plot_forest([idata, idata_hh], var_names=['beta'], ax=ax, model_names=['std', 'hh'], combined=True)
ax.axvline(x=0, color='lightgrey', ls=':')
fig.tight_layout()
```

We can now compare the forest plot of the two models
containing all the variables.

![](/docs/assets/images/statistics/horseshoe/forest_compare.webp)

The error bars of the horseshoe model are much smaller than the ones
of the standard model, and the net result
of using the horseshoe is that the posterior distribution
of the regression coefficients are more sparse than
the ones of the standard multilinear model.
If you perform a sensitivity analysis, you should obtain
the same results for quite a large range of choices for the
scale parameter.

## Conclusions

We have seen how an appropriate choice of the prior distribution
allows us to enforce sparsity in the regression coefficients
of a multilinear model.
There are of course other possible approach to the problem,
such as [factor analysis](https://www.pymc.io/projects/examples/en/latest/case_studies/factor_analysis.html)
or only adding one regressor per time,
as we did in the beginning.
However, according to my own view, an appropriate choice of
the prior to enforce some
desired condition is the best way to leverage
the Bayesian workflow to
tackle any issue, including multi-collinearity.

## Suggested readings

- <cite>Piironen, J., & Vehtari, A. (2017). Sparsity information and regularization in the horseshoe and other shrinkage priors. arXiv: Methodology.</cite>

```python
%load_ext watermark
```

```python
%watermark -n -u -v -iv -w -p xarray,numpyro,jax,jaxlib
```

<div class="code">
Last updated: Sun Nov 17 2024
<br>

<br>
Python implementation: CPython
<br>
Python version       : 3.12.7
<br>
IPython version      : 8.24.0
<br>

<br>
xarray : 2024.9.0
<br>
numpyro: 0.15.0
<br>
jax    : 0.4.28
<br>
jaxlib : 0.4.28
<br>

<br>
arviz     : 0.20.0
<br>
numpy     : 1.26.4
<br>
matplotlib: 3.9.2
<br>
seaborn   : 0.13.2
<br>
pandas    : 2.2.3
<br>
pymc      : 5.17.0
<br>

<br>
Watermark: 2.4.3
<br>
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/horseshoe/" /><summary type="html"><![CDATA[Dealing with multi-collinearity]]></summary></entry><entry><title type="html">Introduction to the linear regression</title><link href="http://localhost:4000/statistics/regression" rel="alternate" type="text/html" title="Introduction to the linear regression" /><published>2024-10-16T00:00:00+00:00</published><updated>2024-10-16T00:00:00+00:00</updated><id>http://localhost:4000/statistics/regression</id><content type="html" xml:base="http://localhost:4000/statistics/regression"><![CDATA[So far we discussed how to model one variable. With this
post we will start a discussion on how to model the dependence of one
variable on other variables, named **covariates**,  **regressors**,
**predictors**
or **risk factors** depending on the research area we are dealing with.

## Regression

In regression, we want to model the dependence of one variable
$$Y$$ on one or more external variables $$X$$.
In other words, we are trying to determine an $f$ such that

$$Y_i = f(X_i, \theta) + \varepsilon_i$$

where $\varepsilon_i$ is some random noise and $\theta$ represents a set of parameters.
In the case of regression, we are not interested in modelling $X_i$.
Notice that the distribution of the $Y_i$ is now different among
different elements, as the parameters are assumed to depend on $X_i\,.$
In other words, the $Y_i$ are no more identically distributed.
What we want to model is the **statistical dependence**, which is not an exact one, since
we assume that there is some noise which makes our dependence inaccurate.
This fact makes statistical dependence different from the mathematical dependence,
where the relation is exactly fulfilled.
We must also draw a distinction between statistical dependence and causal dependence,
since a common misconception is that finding a statistical dependence 
implies a causal relation between $X$ and $Y$.

<div class='emphbox'>
Causal inference requires much stronger
assumptions than statistical inference.
</div>

We are only allowed to draw conclusions about causality
when these assumptions are satisfied, as we will discuss later in this blog.
Notice that referring to $X$ as the risk factor is usually done in the context
of causal inference, and we will therefore avoid this term for now.

As pointed out by John Carlin in [this paper](https://arxiv.org/pdf/2309.06668.pdf), there are two main purposes for regression
other than causal inference: we may either want to **describe** a relation between $X$ and $Y\,,$
or we may desire to use our model to **predict** the value of $Y$ one $X$ has been measured.

By Taylor expanding $f$ around $X=0$ we have that the simplest
dependence we can assume is

$$
Y_i = \theta_0 + \theta_1 X_i + \varepsilon_i\,,
$$

we are therefore assuming the **additivity** of $Y_i$ with respect to $X_i\,.$

The assumption which is by far the most common for $\varepsilon_i$ is

$$
\varepsilon_i \sim \mathcal{N}(0, \sigma)
$$

We are therefore assuming that the errors are normally distributed, and that the
variance is independent on $X_i\,.$
The constant variance assumption is named **homoscedasticity**,
while the condition of variable variance is named **heteroskedasticity**.

Let us now take a look at our parameters:
- $\theta_1$ is the average $Y$ difference of two groups with $\Delta Y = 1\,.$
- $\theta_0$ is the intercept of the model. If our data includes $X=0$ we can interpret $\theta_0$ as the value of $Y$ when $X=0\,.$
- $\sigma$ is the average variance.

## GDP-Life expectancy relation

In order to understand our model, we will apply it to investigate the relation between the gross domestic product of
a country and its life expectancy.

First of all, let us import the relevant libraries.

```python
import requests
import json
import pandas as pd
import seaborn as sns
import numpy as np
import pymc as pm
import arviz as az
from matplotlib import pyplot as plt
from zipfile import ZipFile
import io
```

We can now download the GDP data from the IMF rest API as follows:

```python
with requests.get('https://www.imf.org/external/datamapper/api/v1/NGDPDPC') as gdp:
    data_gdp = json.loads(gdp.content)

dt = {key: [data_gdp['values']['NGDPDPC'][key]['2021']]
        for key in data_gdp['values']['NGDPDPC'] if '2021' in data_gdp['values']['NGDPDPC'][key]}

df = pd.DataFrame.from_dict(dt).transpose().rename(columns={0: 'gdp'})

```

We also download the country names from the same API, and we combine the two tables

```python
with requests.get('https://www.imf.org/external/datamapper/api/v1/countries') as countries:
    data_countries = json.loads(countries.content)

df_countries = pd.DataFrame.from_dict({key: [data_countries['countries'][key]['label']]
        for key in data_countries['countries']}).transpose().rename(columns={0: 'name'})

df_n = df.join(df_countries, how='inner')
```

The table containing the life expectancy can be downloaded from [this page of the World Bank website](https://data.worldbank.org/indicator/SP.DYN.LE00.IN).
Rather than clicking on the website, we will download it automatically as follows

```python
with requests.get('https://api.worldbank.org/v2/en/indicator/SP.DYN.LE00.IN?downloadformat=csv') as f:
    data = f.content
    z = ZipFile(io.BytesIO(data))

for name in z.namelist():
    if name.startswith('API'):
        dt = z.extract(name)
        df_lifexp = pd.read_csv(dt, skiprows=4)

```

We can now combine the two dataframes. We will stick to the year 2021, as it is the most recent year for
most of the countries.

```python
df_le = df_lifexp[['Country Code', '2021']].set_index('Country Code').dropna().rename(
    columns={'2021': 'Life expectancy'})

df_final = df_n.join(df_le, how='inner')

sns.pairplot(df_final)
```

![The pairplot of our variables](/docs/assets/images/statistics/regression/pairplot.webp)

There appears to be no linear relation between the two. However, by a suitable variable redefinition,
we can get linearity within a good approximation.

```python
df_final['log GDP'] = np.log(df_final['gdp'])

sns.pairplot(df_final[['log GDP', 'Life expectancy']])
```

![The pairplot of our variables](/docs/assets/images/statistics/regression/pairplot_log.webp)

The homoscedasticity only seems to hold approximately, as in the region with lower GDP the data shows
a larger variance with respect to countries with higher GDP.
For the sake of simplicity, we will stick to the constant variance assumption, and we will see how to deal
with heterogeneous variance in a future post.

Let us now set up our model

```python
rng = np.random.default_rng(42)
x_pred = np.arange(5, 13, 0.1)

with pm.Model() as model:
    alpha = pm.Normal('alpha', mu=0, sigma=50)
    beta = pm.Normal('beta', mu=0, sigma=10)
    sigma = pm.HalfNormal('sigma', sigma=10)
    y = pm.Normal('y', mu=alpha+beta*df_final['log GDP'], sigma=sigma, observed=df_final['Life expectancy'])
    y_pred = pm.Normal('y_pred', mu=alpha+beta*x_pred, sigma=sigma)  # We want to get the error bands for all the values of x_pred

with model:
    trace = pm.sample(random_seed=rng, chains=4, draws=2000, tune=2000, nuts_sampler='numpyro')

az.plot_trace(trace, var_names=['alpha', 'beta', 'sigma'])
fig = plt.gcf()
fig.tight_layout()
```

![The trace plot](/docs/assets/images/statistics/regression/trace.webp)

The trace looks fine. We will directly check the posterior predictive distribution.

```python
with model:
    ppc = pm.sample_posterior_predictive(trace)

y_mean = trace.posterior['y_pred'].values.reshape((-1, len(x_pred))).mean(axis=0)
y_low = np.quantile(trace.posterior['y_pred'].values.reshape((-1, len(x_pred))), 0.025, axis=0)
y_high = np.quantile(trace.posterior['y_pred'].values.reshape((-1, len(x_pred))), 0.975, axis=0)

fig = plt.figure()
ax = fig.add_subplot(111)
ax.fill_between(x_pred, y_low, y_high, color='lightgray')
ax.plot(x_pred, y_mean, color='grey')
ax.scatter(df_final['log GDP'], df_final['Life expectancy'])
ax.set_xlim([np.min(x_pred), np.max(x_pred)])
ax.set_xlabel('Log GDP per Capita')
ax.set_title('Life Expectancy 2021')
```

![The posterior predictive](/docs/assets/images/statistics/regression/ppc.webp)

While our model correctly reproduces the relation between the GDP and the average life expectancy,
it fails to reproduce the observed variance, confirming that the homoscedasticity assumption is violated.

Let us now inspect which nations show the biggest error

```python
(df_final['Life expectancy'] - np.mean(trace.posterior['alpha'].values.reshape(-1))
- np.mean(trace.posterior['beta'].values.reshape(-1))*df_final['log GDP']).sort_values(ascending=True).head()
```

<div class='code'>
NGA   -13.328530
<br>
SWZ   -12.201977
<br>
GNQ   -11.809278
<br>
NRU   -11.197007
<br>
NAM   -10.621142
<br>
dtype: float64
</div>

The above nations are Nigeria, eSwatini, Equatorial Guinea, Nuaru and Nambia,
so it looks like our model fails to reproduce some
African and Oceania countries, which have an average life expectancy
much lower than non-African and non-Oceania countries with similar GDP.

Let us also check if the assumption about the normality of the deviation
from the average trend is fulfilled within a good approximation

```python
fig = plt.figure()
ax = fig.add_subplot(111)
ax.hist(
(df_final['Life expectancy'] - np.mean(trace.posterior['alpha'].values.reshape(-1)) 
 - np.mean(trace.posterior['beta'].values.reshape(-1))*df_final['log GDP']), bins=np.arange(-15, 10,1.5))
ax.set_title('Residuals histogram')
```

![The pairplot of our variables](/docs/assets/images/statistics/regression/res_plot.webp)

It appears that the distribution of the residual is left skewed, so in order
to improve our model we could use a skewed distribution,
like the [Skewed Normal distribution](https://www.pymc.io/projects/docs/en/stable/api/distributions/generated/pymc.SkewNormal.html) or the [Variance Gamma](https://arxiv.org/pdf/2303.05615.pdf)
instead of a normal distribution.

This is however a somehow more advanced topic with respect to an introductory
post on linear regression, so we won't implement these models here.

## Conclusions

We introduced the linear model, and we saw how to implement it
with an example.
As we will see in the future posts, the linear model is the 
starting point for almost any regression model.
We discussed the interpretation of the parameters and some
of the most relevant assumptions we made about data.

## Suggested readings

-  <cite> Kutner, M. H., Nachtsheim, C., Neter, J. (2004). Applied linear regression models.UK: McGraw-Hill/Irwin. </cite>
- <cite> Gelman, A., Hill, J., Vehtari, A. (2020). Regression and Other Stories. India: Cambridge University Press. </cite>

```python
%load_ext watermark
```

```python
%watermark -n -u -v -iv -w -p xarray,pytensor,numpyro,jax,jaxlib
```
<div class="code">
Last updated: Wed Nov 20 2024
<br>

<br>
Python implementation: CPython
<br>
Python version       : 3.12.7
<br>
IPython version      : 8.24.0
<br>

<br>
xarray  : 2024.9.0
<br>
pytensor: 2.25.5
<br>
numpyro : 0.15.0
<br>
jax     : 0.4.28
<br>
jaxlib  : 0.4.28
<br>

<br>
requests  : 2.32.3
<br>
seaborn   : 0.13.2
<br>
matplotlib: 3.9.2
<br>
numpy     : 1.26.4
<br>
arviz     : 0.20.0
<br>
json      : 2.0.9
<br>
pymc      : 5.17.0
<br>
pandas    : 2.2.3
<br>

<br>
Watermark: 2.4.3
<br>
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/linear_regression/" /><summary type="html"><![CDATA[Including dependence on external variables]]></summary></entry></feed>