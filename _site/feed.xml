<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-08-20T10:43:41+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Data Perspectives</title><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><entry><title type="html">Gaussian processes</title><link href="http://localhost:4000/statistics/gp" rel="alternate" type="text/html" title="Gaussian processes" /><published>2024-07-19T00:00:00+00:00</published><updated>2024-07-19T00:00:00+00:00</updated><id>http://localhost:4000/statistics/gp</id><content type="html" xml:base="http://localhost:4000/statistics/gp"><![CDATA[<p>As explained on the <a href="https://mc-stan.org/docs/stan-users-guide/gaussian-processes.html">stan user guide</a>,
gaussian processes, for short GPs, is a powerful tool to perform regression,
and its underlying assumption is
that the variable $y$ follows a multivariate normal distribution</p>

\[y \sim \mathcal{MvN}(m(x), K(x \vert \theta))\]

<p>where $m(\cdot)$ is any function, and it represents the <strong>mean</strong>,
while $K(\cdot \vert \theta)$ is the <strong>covariance  function</strong>,
and it is a matrix-valued function.</p>

<p>Since $K(x \vert \theta)$ represents the covariance, it must be a <strong>symmetric</strong>
and <strong>positive-defined</strong> function of $x\,.$</p>

<p>Different choices of $m$ and $K$ will give different properties to the GP.
The two most common choices for $m$ are the constant function and the linear function.
The possible choices for the covariance function $K$ are much broader,
and here we will discuss some of the most popular choices.</p>

<p>Since the kernel must be a symmetric matrix, its elements must be given
by a (scalar) function $k(x, y)\,,$ where we neglect the parameter dependence.
A very versatile choice is to assume that the kernel is a function of a scalar
combination of $x$ and $y$ (which can be vector-valued), therefore
it can depend on $\lVert x-y\rVert^2$ or on $x\cdot y\,.$
In the first case you get a <strong>stationary</strong> kernel, otherwise you get
a <strong>polynomial</strong> kernel.
Stationary kernels are invariant under translation, so their value
does not depend on the absolute position on the point,
but only on the relative distance from the other points.</p>

<p>In the following, we will restrict our discussion to the one-dimensional
case, so we will replace $\lVert x-y \rVert^2$ with $(x-y)^2$
and $x\cdot y$ with $xy\,,$
but it is immediate to recover the general definition.</p>

<p>Notice that, generally, if the dependence on a parameter is multiplicative,
then in PyMC the parameter is generally dropped.
We will however discuss it since it is useful to understand what role is played
by the parameter.</p>

<h2 id="some-common-kernel-choice">Some common kernel choice</h2>

<h3 id="the-rational-quadratic-kernel">The Rational Quadratic kernel</h3>

<p>This kernel is, often, the first choice, due to its nice properties.
It has in fact some nice properties one may desire:</p>
<ul>
  <li>it vanishes as $\lVert x-y \rVert \rightarrow \infty$</li>
  <li>it is a smooth function of $\lVert x-y \rVert^2\,.$</li>
</ul>

<p>This kernel reads:</p>

\[K(x, y \vert \sigma, \ell) = \sigma^2 
\exp{\left( -\frac{(x-y)^2}{2\ell^2} \right)}\]

<p>Let us now visualize how does a function distributed according to this
kernel behave</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">xtest</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">kexpquad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">l</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="n">sigma</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">lng</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">nplot</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">sigma</span><span class="p">),</span> <span class="n">ncols</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">lng</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sigma</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lng</span><span class="p">):</span>
        <span class="n">kf</span> <span class="o">=</span> <span class="n">s</span><span class="o">**</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">kexpquad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nplot</span><span class="p">):</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="o">*</span><span class="n">xtest</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">kf</span><span class="p">))</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">"$\sigma=$"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">+</span><span class="sa">r</span><span class="s">", $\ell=$"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">))</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">r</span><span class="s">"$K(x,y)=\sigma^2 \exp\left({-(x-y)^2/(2\ell^2)}\right)$"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/gp/expquadkernel.webp" alt="Some sample of a function distributed
according to the Exponential Quadratic kernel" /></p>

<h3 id="the-rational-quadratic-kernel-1">The Rational Quadratic kernel</h3>

<p>This kernel has the form
\(K(x, y) = \left(1+\frac{(x-y)^2}{2\alpha \ell^2}\right)^{-\alpha}\)</p>

<p>is a little bit more versatile than the previous one,
since, as explained in <a href="https://www.cs.toronto.edu/~duvenaud/cookbook/">this very useful reference</a>,
it is equivalent to a linear combination of squared exponential kernels with different
$l\,.$
Moreover, as $\alpha \rightarrow \infty\,,$ one recovers the previous kernel.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">kratquad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">a</span><span class="o">*</span><span class="n">l</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="p">)</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">lng</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">nplot</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">alpha</span><span class="p">),</span> <span class="n">ncols</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">lng</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">alpha</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lng</span><span class="p">):</span>
        <span class="n">kf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">kratquad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nplot</span><span class="p">):</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="o">*</span><span class="n">xtest</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">kf</span><span class="p">))</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">"$\alpha=$"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">+</span><span class="sa">r</span><span class="s">", $\ell=$"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">))</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">r</span><span class="s">"$K(x,y)=\left(1+\frac{(x-y)^2}{2\alpha \ell^2}\right)^{-\alpha}$"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/gp/ratquadkernel.webp" alt="Some sample of a function distributed
according to the Rational Quadratic kernel" /></p>

<h2 id="matern-kernels">Matern kernels</h2>

<p>The above kernels are $C^\infty\,,$ and they will ensure $C^\infty\,,$
functions, so they are only appropriate if the underlying data-generation
mechanism ensure smoothness.
A family of non-smooth kernels is given by the <a href="https://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function">Matérn kernels</a>,
which is a family of kernels $K^{Matern}_\nu(x, y, \ell)$ which, in the limit
$\nu \rightarrow\infty\,,$ converges to the exponential quadratic kernel.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">kmatern</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">p</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="n">l</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">p</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="n">l</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="n">l</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">p</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="n">l</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="n">l</span><span class="o">+</span><span class="mi">5</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="n">l</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="n">pval</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">lng</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">nplot</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">pval</span><span class="p">),</span> <span class="n">ncols</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">lng</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pval</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lng</span><span class="p">):</span>
        <span class="n">kf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">kmatern</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nplot</span><span class="p">):</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="o">*</span><span class="n">xtest</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">kf</span><span class="p">))</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">"$p=$"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">+</span><span class="sa">r</span><span class="s">", $\ell=$"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">))</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">r</span><span class="s">"$K^{Matern}_{p+1/2}(x,y,\ell)$"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/gp/maternkernel.webp" alt="Some sample of a function distributed
according to the Matern kernel" /></p>

<p>Notice that the $p=0$ kernel is sometimes named exponential
or Ornstein-Uhlenbeck kernel, and it defines a Markov process.
An GP defined by the exponential kernel is equivalent to an AR(1) process.</p>

<h3 id="the-cosine-kernel">The cosine kernel</h3>

<p>Another property one can encode into GPs is the periodicity.
As an example, the cosine kernel gives cosine-shaped functions.</p>

\[K(x, y) = \cos\left( \frac{2 \pi \left|x-y \right|}{\ell^2} \right)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">kcos</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="n">l</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">lng</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="n">nplot</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">lng</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lng</span><span class="p">):</span>
    <span class="n">kf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">kcos</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nplot</span><span class="p">):</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="o">*</span><span class="n">xtest</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">kf</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">"$\ell=$"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">))</span>
<span class="n">fig</span><span class="p">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">r</span><span class="s">"$K(x,y)=\cos\left(\frac{2\pi\left| x-y\right|}{\ell^2}\right)$"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/gp/cosinekernel.webp" alt="Some sample of a function distributed
according to the cosine kernel" /></p>

<h3 id="the-periodic-kernel">The periodic kernel</h3>

<p>Sometimes a cosine is not enough, and you might be interested into a more general
form of periodic function. In this case, the periodic kernel might be what you need.</p>

\[K(x, y) = 
\exp{\left( - \frac{\sin^2{\left(\pi \left(x-y\right)/T\right)}}{2\ell^2} \right)}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">kperiodic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="n">T</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">l</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="n">tval</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="n">lng</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">nplot</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">tval</span><span class="p">),</span> <span class="n">ncols</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">lng</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tval</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lng</span><span class="p">):</span>
        <span class="n">kf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">kperiodic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nplot</span><span class="p">):</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="o">*</span><span class="n">xtest</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">kf</span><span class="p">))</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">"$T=$"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">+</span><span class="sa">r</span><span class="s">", $\ell=$"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">))</span>
<span class="n">fig</span><span class="p">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">r</span><span class="s">"$K(x,y)=\exp(-\frac{\sin^2(\pi(x-y)/T)}{2\ell^2})$"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/gp/periodickernel.webp" alt="Some sample of a function distributed
according to the periodic kernel" /></p>

<h3 id="the-polynomial-kernel">The polynomial kernel</h3>

<p>By using GPs one can also perform polynomial regression.
The kernel</p>

\[K(x, y) = ((x-c)(y-c))^k\]

<p>will sample a monomial of order $k$ with origin $c\,.$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">lng</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">c</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">c</span><span class="p">):</span>
        <span class="n">kf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[((</span><span class="n">x</span><span class="o">-</span><span class="n">l</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">l</span><span class="p">))</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nplot</span><span class="p">):</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="o">*</span><span class="n">xtest</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">kf</span><span class="p">))</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">"$k="</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="s">", c="</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)</span><span class="o">+</span><span class="s">"$"</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">r</span><span class="s">"$K(x,y)= ((x-c)(y-c))^k$"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/gp/polynomialkernel.webp" alt="Some sample of a function distributed
according to the polynomial kernel" /></p>

<p>Notice that, if $k=0\,,$ we simply get the constant kernel.</p>

<h2 id="the-white-noise">The white noise</h2>

<p>Another useful kernel is the white noise.
If we take</p>

\[k(x, y) = \sigma^2 I\]

<p>where $I$ is the identity matrix, we have that the $n$ dimensional multivariate
normal becomes the product of $n$ univariate gaussian distribution,
so the points $y_i$ are i.i.d. according to a normal distribution with mean 0 and variance
$\sigma\,.$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">lng</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">sig</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sigma</span><span class="p">):</span>
    <span class="n">kf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xtest</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">xtest</span><span class="p">))</span><span class="o">*</span><span class="n">sig</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nplot</span><span class="p">):</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="o">*</span><span class="n">xtest</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">kf</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">"$\sigma=$"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">sig</span><span class="p">))</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">r</span><span class="s">"$K(x,y)=\sigma^2 \delta_{ij}$"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/gp/whitenoise.webp" alt="A GP with the white noise kernel" /></p>

<p>This kernel is usually implemented to encode the fact that the measurements are noisy.</p>

<h3 id="the-brownian-motion">The Brownian motion</h3>

<p>The brownian motion can be considered a GP too, and this process is obtained
by using the following kernel:</p>

\[K(x, y) = min(x, y)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">kf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">])</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nplot</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="o">*</span><span class="n">xtest</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">kf</span><span class="p">))</span>
<span class="n">fig</span><span class="p">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">r</span><span class="s">"$K(x,y)=min(x, y)$"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/gp/minkernel.webp" alt="The result of the min kernel" /></p>

<h2 id="building-new-kernels">Building new kernels</h2>

<p>We only showed some of the most common kernels, and you could look for new ones
by yourself.
You can however also build new ones from the previous ones, 
by only keeping in mind that the covariance matrix must be symmetric and positive-defined.
In particular, some possible ways to build new kernels are the following:</p>
<ul>
  <li>you can take any linear combination $K_1(x, y)+K_2(x, y)$</li>
  <li>you can multiply by any positive scalar $\alpha K(x, y)$</li>
  <li>you can multiply two kernels $K_1(x, y)K_2(x, y)$</li>
  <li>you can replace your variable with any function of it $K(\Phi(x), K(\Phi(y))$</li>
  <li>You can define $\Phi(x) K(x, y) \Phi(y)$ for any positive $\Phi(x)\,.$</li>
  <li>You can take the convolution of a kernel with any positive function $\int dx dy K(x, y)f(x-x_0)f(y-y_0)$</li>
</ul>

<p>As an example, if you take as kernel the superposition
of a squared exponential kernel and a periodic kernel, you will
get a periodic signal superimposed to a squared exponential GP.
On the other hand, if you multiply them, you
will get a periodic kernel with varying amplitude.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">kf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">kperiodic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">kexpquad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">])</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nplot</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="o">*</span><span class="n">xtest</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">kf</span><span class="p">))</span>
<span class="c1"># fig.suptitle(r"$K(x,y)=min(x, y)$")
</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">kf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">kperiodic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">kexpquad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">xtest</span><span class="p">])</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nplot</span><span class="p">):</span>
    <span class="n">ax1</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="o">*</span><span class="n">xtest</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">kf</span><span class="p">))</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/gp/kernelcombination.webp" alt="An example of kernel combination" /></p>

<h2 id="conclusions">Conclusions</h2>

<p>I home I managed to convince you that GPs can be extremely powerful to add flexibility and encod desired properties
into your models, and in the next post we will discuss some practical application.</p>

<h2 id="suggested-readings">Suggested readings</h2>
<ul>
  <li><cite><a href="https://gaussianprocess.org/gpml/">Rasmussen, C. E., Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press.</a>
</cite></li>
</ul>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/nonparametric_intro/" /><summary type="html"><![CDATA[How to make the normal distribution infinite dimensional]]></summary></entry><entry><title type="html">Nonparametric models</title><link href="http://localhost:4000/statistics/nonparametric_intro" rel="alternate" type="text/html" title="Nonparametric models" /><published>2024-07-12T00:00:00+00:00</published><updated>2024-07-12T00:00:00+00:00</updated><id>http://localhost:4000/statistics/nonparametric_intro</id><content type="html" xml:base="http://localhost:4000/statistics/nonparametric_intro"><![CDATA[<p>There are situations where a simple model is not appropriate for your purpose.
In these cases, you may decide and use a model with a number of parameters
which is not fixed a priori, but it is rather decided depending on the data.
In this section we will discuss this kind of model, and we will focus
on the following families:</p>

<ul>
  <li>Gaussian Processes (GP)</li>
  <li>Dirichlet Processes (DP), in particular Dirichlet Process Mixtures (DPM)</li>
  <li>Basis function expansions, in particular splines</li>
  <li>Bayesian Additive Regression Trees (BART)</li>
</ul>

<p>These model are generally more flexible than the models discussed up
to now. They are also however more involved, and consequently harder
to understand. You also have a higher probability to overfit, so you should
be careful when using them.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/nonparametric_intro/" /><summary type="html"><![CDATA[When your focus in on flexibility]]></summary></entry><entry><title type="html">Regression discontinuity design</title><link href="http://localhost:4000/statistics/rdd" rel="alternate" type="text/html" title="Regression discontinuity design" /><published>2024-07-06T00:00:00+00:00</published><updated>2024-07-06T00:00:00+00:00</updated><id>http://localhost:4000/statistics/rdd</id><content type="html" xml:base="http://localhost:4000/statistics/rdd"><![CDATA[<p>Regression Discontinuity Design (RDD) can be applied when there is a threshold
above which some causal effect applies, and allows you to infer the impact of such an effect
on your population.
In most countries, there is a retirement age, and you might analyze the impact of the
retirement on your lifestyle.
There are also countries where school classes has a maximum number of students,
and this has been used to assess the impact of the number of students on the students’ performances.
Here we will re-analyze, in a Bayesian way, the impact of alcohol on the mortality, as done in “Mastering Metrics”.
In the US, at 21, you are legally allowed to drink alcohol,
and we will use RDD to assess the impact on this on the probability of death in the US.</p>

<h2 id="implementation">Implementation</h2>

<p>Let us first of all take a look at the dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">df_madd</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"https://raw.githubusercontent.com/seramirezruiz/stats-ii-lab/master/Session%206/data/mlda.csv"</span><span class="p">)</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_madd</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="s">'forcing'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'outcome'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/rdd/data.webp" alt="" /></p>

<p>A linear model seems appropriate, and it seems quite clear that there is a jump when
the forcing variable (age-21) is zero.</p>

<p>While RDD can be both applied with a sharp cutoff and a fuzzy one, we will
limit our discussion to the sharp one.
We will take a simple linear model, as <a href="https://stat.columbia.edu/~gelman/research/published/2018_gelman_jbes.pdf">polynomial models should be generally avoided in RDD models</a>
as they tend to introduce artifacts.</p>

\[y \sim \mathcal{N}( \alpha + \beta x + \gamma \theta(x), \sigma)\]

<p>Here $x$ is the age minus 21, while $\theta(x)$ is the Heaviside theta</p>

\[\theta(x)
=
\begin{cases}
0 &amp; x\leq0 \\
1 &amp; x &gt; 0\\
\end{cases}\]

<p>As usual, we will assume a non-informative prior for all the parameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">madd_model</span><span class="p">:</span>
  <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
  <span class="n">gamma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'gamma'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
  <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
  <span class="n">mu_0</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">df_madd</span><span class="p">[</span><span class="s">'forcing'</span><span class="p">].</span><span class="n">values</span>
  <span class="n">mu</span> <span class="o">=</span> <span class="n">mu_0</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">heaviside</span><span class="p">(</span><span class="n">df_madd</span><span class="p">[</span><span class="s">'forcing'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
  <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> 
                <span class="n">observed</span><span class="o">=</span><span class="n">df_madd</span><span class="p">[</span><span class="s">'outcome'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
  <span class="n">idata</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata</span><span class="p">)</span>
<span class="n">fig_trace</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig_trace</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/rdd/trace.webp" alt="" /></p>

<p>The trace looks fine, and it is clear that the value of the discontinuity is quite large.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">filter_vars</span><span class="o">=</span><span class="s">'like'</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="s">'gamma'</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/rdd/effect.webp" alt="" /></p>

<p>Let us now verify if our model is capable of reproducing the observed data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">madd_model</span><span class="p">:</span>
    <span class="n">x_pl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">x_pl</span><span class="o">+</span><span class="n">gamma</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">heaviside</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="n">y_pl</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y_pl'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
    <span class="n">pp_plot</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="o">=</span><span class="n">idata</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y_pl'</span><span class="p">],</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">pp_madd</span> <span class="o">=</span> <span class="n">pp_plot</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">.</span><span class="n">y_pl</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_pl</span><span class="p">)))</span>

<span class="n">madd_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pp_madd</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">madd_qqmax</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">pp_madd</span><span class="p">,</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">madd_qqmin</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">pp_madd</span><span class="p">,</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_madd</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="s">'forcing'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'outcome'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">madd_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">madd_qqmin</span><span class="p">,</span> <span class="n">madd_qqmax</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/rdd/posterior_predictive.webp" alt="" /></p>

<h2 id="conclusions">Conclusions</h2>
<p>We re-analyzed the effect of the Minimum Legal Driving Age (MLDA)
on the mortality, and we discussed how to apply RDD to perform causal inference
in the presence of a threshold.</p>

<h2 id="suggested-readings">Suggested readings</h2>

<ul>
  <li><cite>Imbens, G. W., Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. US: Cambridge University Press.<cite></cite></cite></li>
  <li><cite><a href="https://arxiv.org/pdf/2206.15460.pdf">Li, Ding, Mealli (2022). Bayesian Causal Inference: A Critical Review</a></cite></li>
  <li><cite>Ding, P. (2024). A First Course in Causal Inference. CRC Press.</cite></li>
  <li><cite>Angrist, J. D., Pischke, J. (2014). Mastering ‘Metrics: The Path from Cause to Effect.   Princeton University Press.</cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">pytensor</span>
</code></pre></div></div>

<div class="code">
Last updated: Tue Aug 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.4
<br />
IPython version      : 8.24.0
<br />

<br />
xarray  : 2024.5.0
<br />
pytensor: 2.20.0
<br />

<br />
matplotlib: 3.9.0
<br />
seaborn   : 0.13.2
<br />
pandas    : 2.2.2
<br />
arviz     : 0.18.0
<br />
pymc      : 5.15.0
<br />
numpy     : 1.26.4
<br />

<br />
Watermark: 2.4.3
<br />
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/discontinuity_regression/" /><summary type="html"><![CDATA[Introducing an arbitrary threshold to infer causality]]></summary></entry><entry><title type="html">Synthetic control</title><link href="http://localhost:4000/statistics/synthetic_control" rel="alternate" type="text/html" title="Synthetic control" /><published>2024-07-05T00:00:00+00:00</published><updated>2024-07-05T00:00:00+00:00</updated><id>http://localhost:4000/statistics/synthetic_control</id><content type="html" xml:base="http://localhost:4000/statistics/synthetic_control"><![CDATA[<p>The <strong>synthetic control</strong> method recently became a very popular method
among economists (although I honestly can’t see the same enthusiasm in
the statistics community).
This method has been widely (and a little bit wildly) used to assess
the effects on a quantity \(Y^{\bar{s}}_t\) of the introduction of a new policy into a country $s$
(or other geographical region) at a time $t=t_1$.
Assuming that you have the same quantity for a set of similar countries $s_i$
as well as for the target country $\bar{s}\,,$
you assume that the time behavior of \(Y_{\bar{s}} = (Y_{t_0}^{\bar{s}}, \dots, Y^{\bar{s}}_{t_1})\) before the intervention is given by a weighted
average of $Y^{s_i}\,.$</p>

<p>You moreover assume, as control, the same weighted average
\(\bar{Y}^{\bar{s}}\) after the intervention.</p>

<p>A very detailed discussion of this method can be found on <a href="https://juanitorduz.github.io/synthetic_control_pymc/">Juan Camilo Orduz’ page</a>.
We will use the same model, but we will apply it to a different dataset.</p>

<p>While in fact he uses PyMC to reproduce <a href="https://matheusfacure.github.io/python-causality-handbook/landing-page.html">this example</a>,
we will use it to perform a simplified re-analysis of <a href="https://link.springer.com/article/10.1007/s10584-021-03111-2">this article</a>, where the authors analyze the impact of the introduction
of a policy for the reduction of the $CO_2$ emissions in the UK.
The dataset used in this work can be found <a href="https://zenodo.org/records/4566804">on Zenodo</a>.</p>

<p>The authors of the original work, in fact, performed a careful analysis
of the control set, while we will limit ourself to the set of countries
who were in the OECD organization in 2001 and who had not adopted any 
$CO_2$ reduction policy before that year.
We will assume</p>

\[Y^{\bar{s}} \sim \mathcal{N}(\mu, \sigma)\]

<p>In order to ensure that the behavior before the intervention is carefully
reproduced, we assume a small variance</p>

\[\sigma \sim \mathcal{Exp}(100)\]

<p>As anticipated, $\mu$ is given by</p>

\[\mu = \sum_{i=1}^n \omega_{i} Y^i\]

<p>We assume that the weights sum up to one, so we assume</p>

\[\omega \sim \mathcal{Dir}(1/n,\dots,1/n)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">123454321</span><span class="p">)</span>

<span class="n">df_dt</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./data/climate_policies.csv'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">';'</span><span class="p">)</span>
<span class="n">df_carb</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./data/nation.1751_2014.csv'</span><span class="p">)</span>

<span class="n">df_red</span> <span class="o">=</span> <span class="n">df_carb</span><span class="p">[</span><span class="n">df_carb</span><span class="p">[</span><span class="s">'Year'</span><span class="p">]</span><span class="o">&gt;=</span><span class="mi">1990</span><span class="p">][[</span><span class="s">'Nation'</span><span class="p">,</span> <span class="s">'Year'</span><span class="p">,</span> <span class="s">'Per capita CO2 emissions (metric tons of carbon)'</span><span class="p">]]</span>

<span class="n">df_co2</span> <span class="o">=</span> <span class="n">df_red</span><span class="p">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s">'Year'</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s">'Nation'</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s">'Per capita CO2 emissions (metric tons of carbon)'</span><span class="p">)</span>

<span class="c1"># Taken from the repo
</span>
<span class="n">oecd</span> <span class="o">=</span> <span class="p">[</span><span class="s">"AUSTRALIA"</span><span class="p">,</span><span class="s">"AUSTRIA"</span><span class="p">,</span><span class="s">"BELGIUM"</span><span class="p">,</span><span class="s">"CANADA"</span><span class="p">,</span><span class="s">"CZECH REPUBLIC"</span><span class="p">,</span>
        <span class="s">"DENMARK"</span><span class="p">,</span><span class="s">"FINLAND"</span><span class="p">,</span><span class="s">"FRANCE (INCLUDING MONACO)"</span><span class="p">,</span><span class="s">"GERMANY"</span><span class="p">,</span>
        <span class="s">"GREECE"</span><span class="p">,</span><span class="s">"HUNGARY"</span><span class="p">,</span><span class="s">"ICELAND"</span><span class="p">,</span><span class="s">"IRELAND"</span><span class="p">,</span><span class="s">"ITALY (INCLUDING SAN MARINO)"</span><span class="p">,</span>
        <span class="s">"JAPAN"</span><span class="p">,</span><span class="s">"LUXEMBOURG"</span><span class="p">,</span><span class="s">"MEXICO"</span><span class="p">,</span><span class="s">"NETHERLANDS"</span><span class="p">,</span><span class="s">"NEW ZEALAND"</span><span class="p">,</span><span class="s">"NORWAY"</span><span class="p">,</span>
        <span class="s">"POLAND"</span><span class="p">,</span><span class="s">"PORTUGAL"</span><span class="p">,</span><span class="s">"SLOVAKIA"</span><span class="p">,</span><span class="s">"REPUBLIC OF KOREA"</span><span class="p">,</span><span class="s">"SPAIN"</span><span class="p">,</span><span class="s">"SWEDEN"</span><span class="p">,</span>
        <span class="s">"SWITZERLAND"</span><span class="p">,</span><span class="s">"TURKEY"</span><span class="p">,</span><span class="s">"UNITED KINGDOM"</span><span class="p">,</span><span class="s">"UNITED STATES OF AMERICA"</span><span class="p">]</span>

<span class="n">to_exclude</span> <span class="o">=</span> <span class="p">[</span><span class="s">'DENMARK'</span><span class="p">,</span> <span class="s">'ESTONIA'</span><span class="p">,</span> <span class="s">'FINLAND'</span><span class="p">,</span> <span class="s">'NETHERLANDS'</span><span class="p">,</span> <span class="s">'NORWAY'</span><span class="p">,</span>
       <span class="s">'SLOVENIA'</span><span class="p">,</span> <span class="s">'SWEDEN'</span><span class="p">,</span> <span class="s">"UNITED KINGDOM"</span><span class="p">]</span>

<span class="n">donors</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">oecd</span><span class="p">)</span><span class="o">-</span><span class="nb">set</span><span class="p">(</span><span class="n">to_exclude</span><span class="p">))</span>

<span class="n">df_in</span> <span class="o">=</span> <span class="n">df_co2</span><span class="p">[</span><span class="n">donors</span><span class="p">].</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">coords</span><span class="o">=</span><span class="p">{</span><span class="s">'countries'</span><span class="p">:</span> <span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="s">'years'</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1990</span><span class="p">,</span> <span class="mi">2001</span><span class="p">)})</span> <span class="k">as</span> <span class="n">sc_model</span><span class="p">:</span>
    <span class="n">lam</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Gamma</span><span class="p">(</span><span class="s">'lam'</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">))</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="s">'w'</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">))</span><span class="o">*</span><span class="n">lam</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">)),</span>
                     <span class="n">dims</span><span class="o">=</span><span class="p">[</span><span class="s">'countries'</span><span class="p">])</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">):</span>
        <span class="n">mu</span> <span class="o">+=</span> <span class="n">w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">df_in</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">loc</span><span class="p">[</span><span class="mi">1990</span><span class="p">:</span><span class="mi">2001</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_co2</span><span class="p">[</span><span class="s">'UNITED KINGDOM'</span><span class="p">].</span><span class="n">loc</span><span class="p">[</span><span class="mi">1990</span><span class="p">:</span><span class="mi">2001</span><span class="p">])</span>

<span class="k">with</span> <span class="n">sc_model</span><span class="p">:</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
                     <span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span>
                     <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/synthetic_control/trace.webp" alt="The trace plot" /></p>

<p>We ran quite a large number of draws as the number of parameters is quite large
and rather unconstrained. However, the trace looks fine.
An important thing that one should always verify when using
a synthetic control, is that the weights must be sparse (only few should
dominate, while the remaining should be close to 0).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'w'</span><span class="p">])</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/synthetic_control/weights.webp" alt="The forest plot of the weights" /></p>

<p>The requirement seems fulfilled, as only few dominate the entire fit.
We can now compute the posterior predictive before and after the intervention.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">sc_model</span><span class="p">:</span>
    <span class="n">mu1</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">):</span>
        <span class="n">mu1</span> <span class="o">+=</span> <span class="n">w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">df_in</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">loc</span><span class="p">[</span><span class="mi">2002</span><span class="p">:].</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">y1</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y1'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu1</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y'</span><span class="p">,</span> <span class="s">'y1'</span><span class="p">])</span>

<span class="n">yv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">ppc</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">20000</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
                     <span class="n">ppc</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y1'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">20000</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))],</span>
                     <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[[</span><span class="s">'right'</span><span class="p">,</span> <span class="s">'top'</span><span class="p">]].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
<span class="n">uk</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">df_co2</span><span class="p">[</span><span class="s">'UNITED KINGDOM'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'UK'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">yv</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">yv</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">synth</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">yv</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'Synthetic UK'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">2001</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Year"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">"Per capita $CO_2$ $m^3/Year$"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">'Synthetic'</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">yv</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">color</span><span class="o">=</span><span class="n">synth</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">get_color</span><span class="p">()</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">'UK'</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">df_co2</span><span class="p">[</span><span class="s">'UNITED KINGDOM'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">color</span><span class="o">=</span><span class="n">uk</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">get_color</span><span class="p">()</span> <span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/synthetic_control/posterior_predictive.webp" alt="The comparison between the true and the synthetic UK" /></p>

<p>As we can see, the behavior is very similar up to 2001, while after this date
the synthetic UK $CO_2$ consumption is larger than one of the true $UK\,.$
You can verify yourself that, by only fitting up to 2000, the result doesn’t
change, and the lines still diverge starting from 2002.
This is another important check that you should always perform when using the
synthetic control method.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We have seen how to implement the synthetic control method, together with
some of the most important checks that you should always do in order to
exclude major problems in your model.
We also re-analyzed <a href="https://link.springer.com/article/10.1007/s10584-021-03111-2">this article</a>, obtaining the same conclusions.</p>

<h2 id="suggested-readings">Suggested readings</h2>
<p>-<cite>Alice Lépissier &amp; Matto Mildenberger, 2021.
<a href="https://ideas.repec.org/a/spr/climat/v166y2021i3d10.1007_s10584-021-03111-2.html">Unilateral climate policies can substantially reduce national carbon pollution</a>,
<a href="https://ideas.repec.org/s/spr/climat.html">Climatic Change</a>, Springer, vol. 166(3), pages 1-21, June.</cite></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>

<div class="code">
Last updated: Tue Aug 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.4
<br />
IPython version      : 8.24.0
<br />

<br />
xarray : 2024.5.0
<br />
numpyro: 0.15.0
<br />
jax    : 0.4.28
<br />
jaxlib : 0.4.28
<br />

<br />
numpy     : 1.26.4
<br />
pandas    : 2.2.2
<br />
pymc      : 5.15.0
<br />
matplotlib: 3.9.0
<br />
arviz     : 0.18.0
<br />

<br />
Watermark: 2.4.3
<br />
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/synthetic_control/" /><summary type="html"><![CDATA[Building a doppleganger from the control group]]></summary></entry><entry><title type="html">Difference in difference</title><link href="http://localhost:4000/statistics/difference_in_differences" rel="alternate" type="text/html" title="Difference in difference" /><published>2024-06-30T00:00:00+00:00</published><updated>2024-06-30T00:00:00+00:00</updated><id>http://localhost:4000/statistics/difference_in_differences</id><content type="html" xml:base="http://localhost:4000/statistics/difference_in_differences"><![CDATA[<p>Difference in differences is a very old technique,
and one of the first applications of
this method was done by John Snow, who’s also
popular due to the cholera outbreak data visualization.</p>

<p>In his study, he used the <strong>Difference in Difference</strong>
(DiD) method to provide some evidence that,
during the London cholera epidemic of 1866,
the cholera was caused by drinking from a water
pump.
This method has been more recently used <a href="https://davidcard.berkeley.edu/papers/njmin-aer.pdf">by 
Card and Krueger in this work</a>
to analyze the causal relationship between
minimum wage and employment.
In 1992, the New Jersey increased the minimum wage
from 4.25 dollars to 5.00 dollars.
They compared the employment in Pennsylvania
and New Jersey before and after the minimum wage increase
to assess if it caused a decrease in the New Jersey
occupation, as supply and demand theory would predict.</p>

<p>DiD assumes that, before the intervention $I$,
the untreated group and the treated one
both evolve linearly with the time $t$ with the
same slope,
while after the intervention the treated group
changes slope.
Assuming, that the intervention was applied at time
$t=0$</p>

\[\begin{align}
&amp;
Y_{P}^0 = \alpha_{P} 
\\
&amp;
Y_{P}^1 = \alpha_{P} +\beta
\\
&amp;
Y_{NJ}^0 = \alpha_{NJ} 
\\
&amp;
Y_{NJ}^1 = \alpha_{NJ} +\beta + \gamma
\end{align}\]

<p>In the above formulas, the intervention effect
is simply $\gamma\,.$</p>

<h2 id="the-implementation">The implementation</h2>

<p>We downloaded the dataset from <a href="https://www.kaggle.com/code/harrywang/difference-in-differences-in-python/input">this page</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">df_employment</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/employment.csv'</span><span class="p">)</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_employment</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'state'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/difference_in_difference/pairplot.webp" alt="The dataset pairplot" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_before</span> <span class="o">=</span> <span class="n">df_employment</span><span class="p">[[</span><span class="s">'state'</span><span class="p">,</span> <span class="s">'total_emp_feb'</span><span class="p">]]</span>
<span class="n">df_after</span> <span class="o">=</span> <span class="n">df_employment</span><span class="p">[[</span><span class="s">'state'</span><span class="p">,</span> <span class="s">'total_emp_nov'</span><span class="p">]]</span>

<span class="c1"># We will assign t=0 data before treatment and t=1 after the treatment
# Analogously g=0 will be the control group, g=1 will be the test group
</span>
<span class="n">df_before</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">df_after</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">df_before</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'total_emp_feb'</span><span class="p">:</span> <span class="s">'Y'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_after</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'total_emp_nov'</span><span class="p">:</span> <span class="s">'Y'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df_before</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'state'</span><span class="p">:</span> <span class="s">'g'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_after</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'state'</span><span class="p">:</span> <span class="s">'g'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df_reg</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_before</span><span class="p">,</span> <span class="n">df_after</span><span class="p">])</span>

<span class="c1">## Let us build the interaction term
</span>
<span class="n">df_reg</span><span class="p">[</span><span class="s">'gt'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_reg</span><span class="p">[</span><span class="s">'g'</span><span class="p">]</span><span class="o">*</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span>

<span class="n">df_reg</span> <span class="o">=</span> <span class="n">df_reg</span><span class="p">[[</span><span class="s">'g'</span><span class="p">,</span> <span class="s">'t'</span><span class="p">,</span> <span class="s">'gt'</span><span class="p">,</span> <span class="s">'Y'</span><span class="p">]]</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">did_model</span><span class="p">:</span>
    <span class="n">beta_0</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_0'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">beta_g</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_g'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">beta_t</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_t'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">beta_gt</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_gt'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s">'nu'</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="n">beta_g</span><span class="o">*</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'g'</span><span class="p">]</span><span class="o">+</span> <span class="n">beta_t</span><span class="o">*</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span><span class="o">+</span> <span class="n">beta_gt</span><span class="o">*</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'gt'</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">,</span>
                  <span class="n">observed</span><span class="o">=</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'Y'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
    <span class="n">trace_did</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                         <span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_did</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/difference_in_difference/trace.webp" alt="The model trace" /></p>

<p>The trace looks fine, let us now verify the posterior
predictive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">did_model</span><span class="p">:</span>
    <span class="n">y00</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y00'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">beta_0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    <span class="n">y10</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y10'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">beta_0</span><span class="o">+</span><span class="n">beta_g</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    <span class="n">y01</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y01'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">beta_0</span><span class="o">+</span><span class="n">beta_t</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    <span class="n">y11</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y11'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">beta_0</span><span class="o">+</span><span class="n">beta_g</span><span class="o">+</span><span class="n">beta_t</span><span class="o">+</span><span class="n">beta_gt</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    
    <span class="n">ppc_check</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span>
        <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y00'</span><span class="p">,</span><span class="s">'y01'</span><span class="p">,</span><span class="s">'y10'</span><span class="p">,</span><span class="s">'y11'</span><span class="p">],</span> <span class="n">trace</span><span class="o">=</span><span class="n">trace_did</span><span class="p">)</span>


<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">].</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">80</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
            <span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">az</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">ppc_check</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s">"y</span><span class="si">{</span><span class="n">g</span><span class="si">}{</span><span class="n">t</span><span class="si">}</span><span class="s">"</span><span class="p">],</span> <span class="n">group</span><span class="o">=</span><span class="s">'posterior_predictive'</span><span class="p">,</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">],</span>
                       <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">)</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">az</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">ppc_check</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s">"y</span><span class="si">{</span><span class="n">g</span><span class="si">}{</span><span class="n">t</span><span class="si">}</span><span class="s">"</span><span class="p">],</span> <span class="n">group</span><span class="o">=</span><span class="s">'posterior_predictive'</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">])</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">df_reg</span><span class="p">[(</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'g'</span><span class="p">]</span><span class="o">==</span><span class="n">g</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span><span class="o">==</span><span class="n">t</span><span class="p">)][</span><span class="s">'Y'</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s">"g=</span><span class="si">{</span><span class="n">g</span><span class="si">}</span><span class="s">, t=</span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="n">legend</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">legend</span><span class="p">(</span><span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/difference_in_difference/posterior_predictives.webp" alt="The comparison between the predicted
and observed distributions of Y" /></p>

<p>The posterior predictive distributions agree with the observed data. We extracted some random sub-sample to
provide an estimate of the uncertainties.</p>

<p>We can finally verify if there is any effect:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_posterior</span><span class="p">(</span><span class="n">trace_did</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'beta_gt'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/difference_in_difference/effect_estimate.webp" alt="Our estimate for the minimum wage increase effect
" /></p>

<p>As you can see, the effect is compatible with 0, therefore there is no evidence
that by increasing the minimum salary there is an effect on the occupation.</p>

<p>Our model has a small issue: it allows for negative values of the occupation,
which doesn’t make sense. This problem can be easily circumvented by using 
the <a href="https://www.pymc.io/projects/docs/en/v4.4.0/api/distributions/generated/pymc.Truncated.html">truncated PyMC class</a>.</p>

<p>I suggest you to try it and verify yourself if there is any effect.
Remember that in that case $\mu$ is no more the mean for $Y$,
so you can’t use it to estimate the average effect.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We have seen how to implement the DiD method with PyMC, and we used to
re-analyze the Krueger and Card article on the relation between the minimum
salary and the occupation.</p>

<h2 id="suggested-readings">Suggested readings</h2>

<ul>
  <li><cite>Imbens, G. W., Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. US: Cambridge University Press.<cite></cite></cite></li>
  <li><cite><a href="https://arxiv.org/pdf/2206.15460.pdf">Li, Ding, Mealli (2022). Bayesian Causal Inference: A Critical Review</a></cite></li>
  <li><cite>Ding, P. (2024). A First Course in Causal Inference. CRC Press.</cite></li>
  <li><cite>Angrist, J. D., Pischke, J. (2009). Mostly harmless econometrics : an empiricist’s companion. Princeton University Press.</cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>

<div class="code">
Last updated: Tue Aug 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.4
<br />
IPython version      : 8.24.0
<br />

<br />
xarray : 2024.5.0
<br />
numpyro: 0.15.0
<br />
jax    : 0.4.28
<br />
jaxlib : 0.4.28
<br />

<br />
numpy     : 1.26.4
<br />
seaborn   : 0.13.2
<br />
matplotlib: 3.9.0
<br />
pandas    : 2.2.2
<br />
pymc      : 5.15.0
<br />
arviz     : 0.18.0
<br />

<br />
Watermark: 2.4.3
<br />

<br />
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/causal_intro/" /><summary type="html"><![CDATA[Causal inference from 1850]]></summary></entry><entry><title type="html">Instrumental variable regression</title><link href="http://localhost:4000/statistics/instrumental_variable" rel="alternate" type="text/html" title="Instrumental variable regression" /><published>2024-06-23T00:00:00+00:00</published><updated>2024-06-23T00:00:00+00:00</updated><id>http://localhost:4000/statistics/instrumental_variable</id><content type="html" xml:base="http://localhost:4000/statistics/instrumental_variable"><![CDATA[<p>In many circumstances you cannot randomize, either because it is unethical
or simply because it’s too expensive.
There are however methods which, if appropriately applied, may provide
you some convincing causal evidence.</p>

<p>Let us consider the case where you cannot randomly assign the treatment $T\,,$
and in this case it could be affected by any confounder $X$
leading you to a biased estimate of the treatment effect.
However, if you have a variable $Z$ that only affects $T$
and does not affect your outcome in any other way other than via $T\,,$
than you can apply <strong>Instrumental Variable Regression</strong>.</p>

<p><img src="/docs/assets/images/statistics/instrumental_variable/causal_structure.webp" alt="The assumed causal flow" /></p>

<p>Of course, the above causal assumption is quite strong, but it holds
in quite a good approximation in some circumstance.</p>

<p>This method has been applied to analyze the effect of school years ($T$)
on earning ($Y$).
In this case the variable $Z$ was the assignment of some economical assistance
(a voucher) to go to school.</p>

<p>One would be tempted to simply use linear regression to fit this model:</p>

\[Y = \alpha + \beta T + \gamma Z + \varepsilon\]

<p>However, linear regression assumes independence between the regressors,
while in our case we have that $T$ is determined by $Z\,.$
This has an impact on the variance estimate of $Y\,,$ as we do not
correctly propagate the uncertainty due to the $T$ dependence on $Z\,.$
In fact, linear regression always predicts homoscedastic variance,
while IV can also reproduce heteroscedasticity.</p>

<h2 id="application-to-the-cigarettes-sales">Application to the cigarettes sales</h2>

<p>We will use IV to see if an increase in the cigarettes price ($T$)
causes a decrease in the cigarettes sales ($Y$), and we will use the
tobacco taxes as instrumental variable $Z$.
In order to linearize the dependence between the variables,
instead of the value of each quantity, we will consider the
difference between the 1995 log value and the 1985 log value.</p>

\[\begin{pmatrix}
T \\
Y \\
\end{pmatrix}
\sim 
\mathcal{t}
\left(
\left(
\alpha_0 + \beta_0 Z
\atop
\alpha_1 + \beta_1 T
\right),
\Sigma, \nu
\right)\]

<p>where $t$ represents the 2 dimensional Student-T distribution and $\Sigma$ is the $2\times2$ covariance matrix.
If $Z$ has a causal effect on $Y$ via $T\,,$ then the correlation
between $Y$ and $T$ is different from zero.</p>

<p>We will assume</p>

\[\alpha_i, \beta_i \sim \mathcal{N}(0, 10^3)\]

<p>and</p>

\[\nu \sim \mathcal{HalfNormal}(100)\]

<p>\(\Sigma\) must be a positive semi-defined matrix, and an easy way to
provide it a prior is using the
<a href="https://en.wikipedia.org/wiki/Lewandowski-Kurowicka-Joe_distribution">Lewandowski-Kurowicka-Joe distribution
</a>.
This distribution takes a shape parameter $\eta\,,$
and we will take $\eta=1\,,$ which implies that we will take a uniform
prior over $[-1, 1]$ for the correlation matrix.
We will moreover assume that the standard deviations are distributed according to</p>

\[\sigma_i \sim \mathcal{HalfCauchy}(20)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">pytensor.tensor.extra_ops</span> <span class="kn">import</span> <span class="n">cumprod</span>

<span class="n">random_seed</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">df_iv</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/AER/CigarettesSW.csv'</span><span class="p">)</span>

<span class="n">X_iv</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1995</span><span class="p">][</span><span class="s">'price'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'price'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
       <span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'price'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
<span class="n">Y_iv</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1995</span><span class="p">][</span><span class="s">'packs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'packs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
       <span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'packs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
<span class="n">Z_iv</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1995</span><span class="p">][</span><span class="s">'taxs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'taxs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
       <span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'taxs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">instrumental_variable</span><span class="p">:</span>
    <span class="n">sd_dist</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">20.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'nu'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">100.0</span><span class="p">)</span>
    <span class="n">chol</span><span class="p">,</span> <span class="n">corr</span><span class="p">,</span> <span class="n">sigmas</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">LKJCholeskyCov</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sd_dist</span><span class="o">=</span><span class="n">sd_dist</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">Z_iv</span><span class="p">,</span> <span class="n">X_iv</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">X_iv</span><span class="p">,</span> <span class="n">Y_iv</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">w</span><span class="p">)</span>  <span class="c1"># so we will recover it easily
</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">MvStudentT</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">chol</span><span class="o">=</span><span class="n">chol</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_iv</span><span class="p">)),</span> <span class="n">observed</span><span class="o">=</span><span class="n">u</span><span class="p">)</span>
    <span class="c1"># We directly compute the posterior predictive
</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">MvStudentT</span><span class="p">(</span><span class="s">'y_pred'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">chol</span><span class="o">=</span><span class="n">chol</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>

<span class="k">with</span> <span class="n">instrumental_variable</span><span class="p">:</span>
    <span class="n">trace_instrumental_variable</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">,</span>
                                           <span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_instrumental_variable</span> <span class="p">,</span>
              <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">,</span> <span class="s">'beta'</span><span class="p">,</span> <span class="s">'sigma'</span><span class="p">,</span> <span class="s">'nu'</span><span class="p">],</span>
              <span class="n">coords</span><span class="o">=</span><span class="p">{</span><span class="s">'sigma_corr_dim_0'</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s">'sigma_corr_dim_1'</span><span class="p">:</span><span class="mi">1</span><span class="p">})</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/instrumental_variable/trace.webp" alt="The trace plot of the above model" /></p>

<p>As we can see, there is no signal of problems in thee trace plot.</p>

<p>A few remarks on the above code. Since the model is not very fast,
we used the numpyro sampler, which hundred of times
faster than the standard PyMC sampler.
Moreover, we instructed arviz to only plot the off-diagonal elements
of the correlation matrix. We must do this because the diagonal elements
are always one, as they must be, but this causes an error in arviz
(which assumes a random behavior in all the variables of the trace).</p>

<p>We can now verify the posterior predictive distribution.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a0</span> <span class="o">=</span> <span class="n">trace_instrumental_variable</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">])[</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span>
<span class="n">b0</span> <span class="o">=</span> <span class="n">trace_instrumental_variable</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'beta'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">])[</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span>

<span class="n">x_min</span> <span class="o">=</span> <span class="mf">0.06</span>
<span class="n">x_max</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="n">x_pl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mf">0.0002</span><span class="p">)</span>

<span class="n">xiv_0</span> <span class="o">=</span> <span class="n">trace_instrumental_variable</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">2</span><span class="p">))[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">xiv_1</span> <span class="o">=</span> <span class="n">trace_instrumental_variable</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">2</span><span class="p">))[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">sampled_index</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">sampled_index</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">a0</span><span class="o">+</span><span class="n">b0</span><span class="o">*</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">sampled_index</span><span class="p">:</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xiv_0</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">xiv_1</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'x'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_iv</span><span class="p">,</span> <span class="n">Y_iv</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'steelblue'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'t'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/instrumental_variable/posterior_predictive.webp" alt="The posterior predictive distribution" /></p>

<p>Our model also looks capable to reproduce the observed data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">az</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">trace_instrumental_variable</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">"sigma_corr"</span><span class="p">])[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">"IV Model - Posterior Distribution Correlation"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$\sigma$'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">''</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/instrumental_variable/correlation.webp" alt="The off-diagonal component of the correlation matrix" /></p>

<h2 id="conclusions">Conclusions</h2>

<p>We have seen how IV allows us to make causal inference in absence of randomization,
but making some rather strong assumptions about the causal structure of the problem.
We have also seen how to implement it in PyMC.</p>

<h2 id="suggested-readings">Suggested readings</h2>

<ul>
  <li><cite>Imbens, G. W., Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. US: Cambridge University Press.<cite></cite></cite></li>
  <li><cite><a href="https://arxiv.org/pdf/2206.15460.pdf">Li, Ding, Mealli (2022). Bayesian Causal Inference: A Critical Review</a></cite></li>
  <li><cite>Ding, P. (2024). A First Course in Causal Inference. CRC Press.</cite></li>
  <li><cite>Angrist, J. D., Pischke, J. (2009). Mostly harmless econometrics : an empiricist’s companion. Princeton University Press.</cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>

<div class="code">
Last updated: Tue Aug 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.4
<br />
IPython version      : 8.24.0
<br />

<br />
xarray : 2024.5.0
<br />
numpyro: 0.15.0
<br />
jax    : 0.4.28
<br />
jaxlib : 0.4.28
<br />

<br />
matplotlib: 3.9.0
<br />
pandas    : 2.2.2
<br />
pymc      : 5.15.0
<br />
seaborn   : 0.13.2
<br />
arviz     : 0.18.0
<br />
numpy     : 1.26.4
<br />

<br />
Watermark: 2.4.3
<br />
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/causal_intro/" /><summary type="html"><![CDATA[Making causal inference without randomization]]></summary></entry><entry><title type="html">Randomized controlled trials</title><link href="http://localhost:4000/statistics/randomized" rel="alternate" type="text/html" title="Randomized controlled trials" /><published>2024-06-16T00:00:00+00:00</published><updated>2024-06-16T00:00:00+00:00</updated><id>http://localhost:4000/statistics/randomized</id><content type="html" xml:base="http://localhost:4000/statistics/randomized"><![CDATA[<p>As we anticipated in the last post, when we have randomization, association
implies causation.
In this case we can use a simple regression model to assess if the treatment
causes an effect.</p>

<p>Randomized controlled trials are the golden standards in clinical studies,
but they are widely used in other fields like industry or marketing
campaigns.
Thanks to their popularity, even marketing providers such as Mailchimp allow you
to easily implement this kind of studies, and in this post we will see how
to analyze them by using Bayesian regression.
In this experiment we will analyze the data from a newsletter, and what we will
determine is whether the presence of the first name (which is required
in the login form) in the mail preview increases the probability of opening the
email.
When we programmed the newsletter, we divided the total audience into
two blocks, and each recipient has been randomly assigned to one block.
In the control block (t=0) we sent the email without the first name in the mail
preview, while to the other recipients we sent the email with the first name
in the mail preview.</p>

<p>Some mails were bounced, but at the end $n_t = 2326$ users received the test mail
and $n_c = 2347$ received the control mail.
$y_t = 787$ users out of 2326 opened the test email, while $y_c=681$ users out
of 2347 opened the control one.</p>

<p>Since the opening action is a binary variable, we will take
a binomial likelihood.
We will therefore use a logistic regression to estimate the ATE.</p>

\[\begin{align}
&amp;
y_{c} \sim \mathcal{Binom}(p_c, n_n)
\\
&amp;
y_{t} \sim \mathcal{Binom}(p_t, n_t)
\\
&amp;
p_c = \frac{1}{1+e^{-\beta_0}}
\\
&amp;
p_t = \frac{1}{1+e^{-(\beta_0+ \beta_1)}}
\end{align}\]

<p>We will take a non-informative prior for both the parameters</p>

\[\beta_i \sim \mathcal{N}(0, 10^3)\]

<p>We can now easily implement our model in PyMC</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">random_seed</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">n_t</span> <span class="o">=</span> <span class="mi">2326</span>
<span class="n">n_c</span> <span class="o">=</span> <span class="mi">2347</span>

<span class="n">k_t</span> <span class="o">=</span> <span class="mi">787</span>
<span class="n">k_c</span> <span class="o">=</span> <span class="mi">681</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">pt</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'pt'</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]))))</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'pc'</span><span class="p">,</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]))))</span>
    <span class="n">ate</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'ate'</span><span class="p">,</span> <span class="n">pt</span><span class="o">-</span><span class="n">pc</span><span class="p">)</span>
    <span class="n">y_t</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">'y_t'</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_t</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pt</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">k_t</span><span class="p">)</span>
    <span class="n">y_c</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">'y_c'</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_c</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pc</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">k_c</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">,</span> <span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/randomized/trace.webp" alt="The trace of our model" /></p>

<p>The average treatment effect is greater than 0 with a probability
approximately equal to 1,
therefore we are almost sure that, in our test,
using the first name in the mail preview increased the opening
probability of the newsletter.</p>

<p>Notice that we restricted our discussion to one single newsletter, and we
avoided more general claims regarding future newsletters we will send.
However, we have some indication that our audience may prefer more
personal newsletters.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We saw an example of how to perform causal inference in Bayesian statistics for randomized controlled experiments
by using regression models in PyMC. We also discussed the proper interpretation of the results.</p>

<h2 id="suggested-readings">Suggested readings</h2>

<ul>
  <li><cite>Imbens, G. W., Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. US: Cambridge University Press.<cite></cite></cite></li>
  <li><cite><a href="https://arxiv.org/pdf/2206.15460.pdf">Li, Ding, Mealli (2022). Bayesian Causal Inference: A Critical Review</a></cite></li>
  <li><cite>Ding, P. (2024). A First Course in Causal Inference. CRC Press.</cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>

<div class="code">
Last updated: Tue Aug 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.4
<br />
IPython version      : 8.24.0
<br />

<br />
xarray : 2024.5.0
<br />
numpyro: 0.15.0
<br />
jax    : 0.4.28
<br />
jaxlib : 0.4.28
<br />

<br />
matplotlib: 3.9.0
<br />
arviz     : 0.18.0
<br />
pandas    : 2.2.2
<br />
pymc      : 5.15.0
<br />
numpy     : 1.26.4
<br />

<br />
Watermark: 2.4.3
<br />
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/randomized/" /><summary type="html"><![CDATA[When association implies causation]]></summary></entry><entry><title type="html">Causal inference</title><link href="http://localhost:4000/statistics/causal_intro" rel="alternate" type="text/html" title="Causal inference" /><published>2024-06-09T00:00:00+00:00</published><updated>2024-06-09T00:00:00+00:00</updated><id>http://localhost:4000/statistics/causal_intro</id><content type="html" xml:base="http://localhost:4000/statistics/causal_intro"><![CDATA[<p>In this post we will try and clarify when it is possible to make statements
about causation rather than sticking to statistical association,
and we will do so on the basis of Rubin’s potential outcomes.</p>

<p>The main reference for this part will be the material in
<a href="https://www.bradyneal.com/Introduction_to_Causal_Inference-Dec17_2020-Neal.pdf">these</a>
notes by Brady Neal, but I strongly recommend to read the textbook by Guido Imbens
(who, in 2021, shared the Nobel prize for economics with Joshua Angrist and David Card for their works on causal inference) and Carl Rubin
(who first developed the potential outcomes framework).</p>

<h2 id="the-counterfactual-definition-of-causality">The counterfactual definition of causality</h2>

<p>You may have heard the mantra “association is not causation” or the more colloquial 
(but less accurate) “correlation is not causation”.
Correlation is a statistical measure linear dependence,
while association generally means statistical dependence.
However the exact meaning of causation is never given,
and the first part of these notes will be devoted to clarify what do we mean with causation.</p>

<p>Let us assume that we took a medicine because we had a headache,
what do we mean when we say that the medicine
caused the headache to disappear? It means that, if we hadn’t taken the medicine,
the headache wouldn’t have gone away.</p>

<p>We will therefore stuck to the counterfactual definition of causation,
so we will say that an event causes an outcome if,
by removing the event then the outcome disappears.
The above definition works for binary outcomes, but has some
problems when we want to investigate causes which can take any real value.
More generally, we can say that an event causes an outcome if, by modifying
the cause, the outcome changes.
This definition already puts a strong constrain on what we can investigate,
since it requires that we must be able, at least in principle, to modify the
cause.</p>

<div class="emphbox">
There's no causation without manipulation.
</div>

<p>While the meaning of the above sentence may seem obvious at a first sight,
you should carefully think about it when making causal inference.
If you want to assess the effect of the ethnicity on the probability of being hired,
you may not be able to manipulate someone’s ethnicity,
but you can still manipulate people’s perception of ethnicity by modifying 
the CV.</p>

<p>When talking about causality, one can be either interested in the determination
of the effect of a cause (e.g. does my headache disappears when I take medicine?)
or the cause of an effect (e.g. is my headache gone because I took the medicine?).</p>

<p>Within the counterfactual framework, and in science in general, one generally wants
to assess the effect of a specific cause.
Determining the cause of an effect, in fact, is an ill-posed question, as one could
find a cause of the cause, a cause of the cause of the cause and so on.</p>

<p>A relevant aspect which we must keep in mind is that there could be more than one
cause. We know that, in order to light a fire, we need oxygen, heat and fuel,
and all the above are necessary conditions for fire.</p>

<p>Let’s assume that we want to assess if heat causes fire ignition,
and we perform an experiment to determine it.
If we first provide both three the elements,
and we then remove oxygen and heat, we can’t conclude anything about the
causal relation between heat and fire, since we also removed the oxygen.
The counterfactual definition of causality requires that
only the cause must change, while all the other elements must be unchanged.</p>

<h2 id="potential-outcomes">Potential outcomes</h2>

<p>But how can we measure the effect of an event? Let us indicate with $T=1$
the case where the event happens, as an example we take a therapy,
while $T=0$ means that we do not take the therapy.
Suppose that the outcome of the event $T=0$ is $y_0$ while the outcome of $T=1$ is
$y_1\,.$ We define</p>

\[Y(t) = t y_1 + (1-t) y_0\,.\]

<p>From a counterfactual point of view, a natural way to assess the causal
effect is via the <strong>Individual Treatment Effect</strong> ITE</p>

\[\tau = Y(1)-Y(0)\]

<p>The definition of $\tau$ is of course arbitrary, but quite general.
As an example, one could prefer taking
the ratio of the two, but then taking the logarithm we recover the above definition.</p>

<p>Despite on the exact definition, $\tau$ of course cannot be measured, as either we take the treatment and
we observe $Y(1)$ or we don’t and we observe $Y(0)\,,$
and any reasonable definition of $\tau$ involves both the quantities.
This implies that,</p>

<div class="emphbox">
while our definition of effect may be individual,
its quantification can only be done on a larger sample.
</div>

<p>We must therefore do an experiment in order to estimate it:
we collect $2N$ individuals and divide them into 2 groups.
Half individuals are treated with $T=1$ (the treatment group)
and half of them with $T=0$ (the control group).</p>

<p>In order to proceed, we will stick to the definition $\tau = Y(1)-Y(0)\,.$
The simplest estimate that we may do is the <strong>Average Treatment Effect</strong> ATE</p>

\[ATE = \mathbb{E}[Y_i(1) - Y_i(0)] =  \mathbb{E}[Y(1) - Y(0)] = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]\]

<p>where the average is meant both on the individual and on any other possible source
of randomness.</p>

<p>To clarify what we are doing, we can put the collected data as</p>

<table>
  <thead>
    <tr>
      <th>i</th>
      <th>T</th>
      <th>Y</th>
      <th>Y(0)</th>
      <th>Y(1)</th>
      <th>X</th>
      <th>Y(1) - Y(0)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>$y^1$</td>
      <td>$y^1$</td>
      <td>?</td>
      <td>$x^1$</td>
      <td>?</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0</td>
      <td>$y^2$</td>
      <td>$y^2$</td>
      <td>?</td>
      <td>$x^2$</td>
      <td>?</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0</td>
      <td>$y^3$</td>
      <td>$y^3$</td>
      <td>?</td>
      <td>$x^3$</td>
      <td>?</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1</td>
      <td>$y^4$</td>
      <td>?</td>
      <td>$y^4$</td>
      <td>$x^4$</td>
      <td>?</td>
    </tr>
    <tr>
      <td>5</td>
      <td>1</td>
      <td>$y^5$</td>
      <td>?</td>
      <td>$y^5$</td>
      <td>$x^5$</td>
      <td>?</td>
    </tr>
    <tr>
      <td>6</td>
      <td>1</td>
      <td>$y^6$</td>
      <td>?</td>
      <td>$y^6$</td>
      <td>$x^6$</td>
      <td>?</td>
    </tr>
  </tbody>
</table>

<p>where $X$ represents other possibly relevant quantity where we must take
into account to estimate the averages or, in other words, any quantity we may suspect could
affect the outcome.
The estimate of the ATE is the so-called <strong>fundamental problem of causal inference</strong>,
and since the question marks can be seen as missing values,</p>

<div class="emphbox">
the fundamental problem of causal inference is a missing value problem. 
</div>

<p>We did a step further, but still we don’t know how to compute that quantity.</p>

<p>In writing the above table, we implicitly made the <strong>no interference</strong> assumptions, namely that</p>

\[Y_i(t_1, t_2, ..., t_{i-1}, t_i, t_{i+1}, ..., t_n) = Y_i(t_i)\]

<p>So each unit’s outcome only depends on his own treatment and not on the treatment of other individuals.
This implies that, if we are checking the effect of a product in some tomato field,
we must be sure that the product does not goes in another studied field by mistake.
Another case can be a study where we are studying an experimental study program in a class.
If a student is selected in the treatment group and a friend of his is not,
the latter could be sad for not being selected and his outcome could be lowered.</p>

<p>Generally, a good strategy to enforce this requirement is to take well separated units
and not letting them communicate during the experiment.</p>

<p>Notice that this is not a necessary requirement, but it greatly simplifies the discussion,
as it allows us to threat each unit independently on the others.</p>

<p>A quantity that is closely related to the ATE is the <strong>associational difference</strong></p>

\[\mathbb{E}[Y|T=1] - \mathbb{E}[Y|T=0]\]

<p>When are we allowed to replace the ATE with the associational difference?
In other words, when are we allowed to compute the average only over the observed 
values and replace the question marks with the appropriate average?</p>

<p>The assumptions that the observed data do not depend on the missing ones
is called <strong>ignorability</strong>, and it is one of the most important assumptions
in causal inference.
Ignorability can be written in mathematical language as</p>

\[Y(0), Y(1) \perp\!\!\!\!\perp T\]

<p>where \(A \perp\!\!\!\!\perp B\) means that $A$ and $B$ are independent one on
the other one.</p>

<p>If ignorability holds, we are allowed to estimate the average of $Y(0)$
by only using the $T=0$ group and replace it in the $T=1$ group and vice versa,
and this is why this assumption is often named <strong>exchangeability</strong>.</p>

<p>We can mathematically write the exchangeability assumption as</p>

\[\mathbb{E}[Y(0) | T=0] = \mathbb{E}[Y(0) | T=1] = \mathbb{E}[Y(0)]\]

<p>and</p>

\[\mathbb{E}[Y(1) | T=0] = \mathbb{E}[Y(1) | T=1] = \mathbb{E}[Y(1)]\]

<p>The above assumption is almost equivalent to <strong>identifiability</strong> assumption:
a causal quantity $\mathbb{E}[Y(t)]$ is identifiable if it can be computed from a pure statistical quantity $\mathbb{E}[Y | T=t]$.</p>

<p>There are cases where exchangeability does not hold.
As an example, assume that you are testing a medicine, and that this medicine
is more effective on patients with a severe version of the disease you are treating.
If in the $T=1$ group we have
people with a more severe version of the disease than in the $T=0$
group we may not be allowed to exchange the two groups,
as we have no guarantee that the result would be invariant under the group exchange.</p>

<p>Let us decompose the associational difference as</p>

\[\mathbb{E}[Y(1) | T=1] - \mathbb{E}[Y(0) | T=0]
=
(\mathbb{E}[Y(1) | T=1] - \mathbb{E}[Y(0) | T=1])
+(\mathbb{E}[Y(0) | T=1] - \mathbb{E}[Y(0) | T=0])\]

<p>The associational difference can be decomposed as
the average treatment effect on the treated (the first parenthesis)
plus the sampling bias (the second parenthesis).</p>

<p>Consider the case where $Y$ is the health status of a person and the treatment is
the hospitalization.
The associational difference is simply the difference between the health status
of hospitalized patients and the health status of non-hospitalized people.
This is simply the sum between the effect of the hospitalization on hospitalized
patients plus the baseline health difference between hospitalized and non-hospitalized individuals.
In general, even if hospitalization improves health, the health status of those who go to
the hospital is generally worst than the other individuals.
Therefore, in absence of randomization, if we simply use the associational difference to assess the effect of
hospitalization, we may end up with the conclusion that health get worst due to hospitalization
simply because only sick people goes to the hospital.</p>

<p>If exchangeability does not hold, then \(\mathbb{E}[Y \vert T=0]\) is different from
\(\mathbb{E}[Y \vert T=1]\,,\) therefore the associational quantity \(\mathbb{E}[Y \vert T=t]\) is a biased estimator for \(\mathbb{E}[Y(t)]\,.\)</p>

<p>One possible way to ensure that exchangeability holds is to ensure that the missing
terms are randomly distributed.
This can be experimentally done by randomly assigning the
treatment $T$ to each unit, and in this case we are dealing with a randomized experiment.</p>

<p>In a randomized experiment, the treatment assignment does not depend on anything other
other than the result of a coin toss, therefore</p>

\[\mathbb{E}[Y(1)]-\mathbb{E}[Y(0)] = \mathbb{E}[Y(1)|T=1]-\mathbb{E}[Y(0)|T=0] = \mathbb{E}[Y | T=1]-\mathbb{E}[Y | T=0]\]

<p>We stress that this is only a statistical property, and it doesn’t guarantee that the outcome
estimate of an experiment will be correct.</p>

<p>In other words, as explained in <a href="http://www.fsb.muohio.edu/lij14/420_paper_Rubin74.pdf">the breaktrhough 1974 Rubin paper</a>:</p>
<blockquote cite="https://hedibert.org/wp-content/uploads/2015/10/causality-meeting2.pdf">
Whether treatments are randomly assigned or not, no matter how carefully
matched the trials, and no matter how large N, a skeptical observer could always
eventually find some variable that systematically differs in the E (T=1) trials and C (T=0) trials.
<br />
Within the experiment there can be no refutation of this claim; only a logical
argument explaining that the variable cannot causally affect the dependent
variable or additional data outside the study can be used to counter it.
</blockquote>

<p>Generally exchangeability is an unrealistic assumption, as it would impossible to verify
that $X$ and $Y$ are equally distributed with respect to all the 
relevant variables except for the treatment.
A weaker assumption is that the assigned treatment only depends on some
relevant quantity $X$ while the two groups are exchangeable
with respect to any other quantity.
This condition is called <strong>conditional exchangeability</strong> or <strong>unconfoundedness</strong> and it is indicated as</p>

\[Y(0), Y(1) \perp\!\!\!\!\perp T | X\]

<p>If conditional exchangeability holds, we have that</p>

\[\begin{align}
 \mathbb{E}[Y(1)-Y(0)|X] 
 &amp; = \mathbb{E}[Y(1)|X] - \mathbb{E}[Y(0)|X] \\
 &amp; = \mathbb{E}[Y(1)| T=1, X] - \mathbb{E}[Y(0)|T=0, X] \\
 &amp; = \mathbb{E}[Y| T=1, X] - \mathbb{E}[Y|T=0, X] \\
 \end{align}\]

<p>In order to get the ATE we must simply take the expectation value over $X$</p>

\[\mathbb{E}[Y(1) - Y(0)] = \mathbb{E}_X[ \mathbb{E}[Y(1) - Y(0) | X] ] 
 = \mathbb{E}_X[ \mathbb{E}[Y |T=1, X] ] - \mathbb{E}_X[ \mathbb{E}[Y |T=0, X] ]\]

<p>And the equality between the first and the last term of this equation is called the <strong>adjustment formula</strong>.</p>

<p>In the above equation we assumed <strong>consistency</strong>, which can be written as</p>

\[T=t \Longrightarrow Y(T) = Y(t)\]

<p>This means that the treatment must be well specified: the treatment must not be “get some medicine” but should rather be “take 15 mg of medicine every 8 hours for 7 days”.
Only thanks to this hypothesis we can replace</p>

\[\mathbb{E}[Y(T=t) | T=t] = \mathbb{E}[Y | T=t] \,.\]

<p>This is not a necessary requirement, but it greatly simplifies the discussion, otherwise we would be forced to model
this random aspect too.
Notice that the concept of consistency is not a mathematical requirement, but rather a conceptual one,
and only agreement among domain experts can assess whether it holds or not.</p>

<p>In the literature it is often required the <strong>Stable Unit Treatment Value Assumption</strong> SUTVA, which is simply requiring consistency and no interference.</p>

<p>Let us now write explicitly the adjustment formula for $X$ discrete:</p>

\[\begin{align}
&amp;
\mathbb{E}_X[ \mathbb{E}[Y |T=1, X] ] - \mathbb{E}_X[ \mathbb{E}[Y |T=0, X] ]  
\\
&amp; =  \sum_{x}P(X=x) \sum_{y} y \left(P(Y=y|T=1, X=x) - P(Y=y| T=0, X=x) \right) \\
&amp; =   \sum_{x}P(X=x) \sum_{y} y \left(\frac{P(Y=y,T=1, X=x)}{P(T=1, X=x)} - \frac{P(Y=y,T=0, X=x)}{P(T=0, X=x)}\right) \\
= &amp; \sum_{x}P(X=x) \sum_{y} y \left(\frac{P(Y=y,T=1, X=x)}{P(T=1| X=x) P(X=x)} - \frac{P(Y=y,T=0, X=x)}{P(T=0| X=x) P(X=x)}\right) 
\\
&amp; = 
\sum_{x}\sum_{y} y \left(\frac{P(Y=y,T=1, X=x)}{P(T=1| X=x)} - \frac{P(Y=y,T=0, X=x)}{P(T=0| X=x)}\right) 
\end{align}\]

<p>where the first equivalence comes from the definition of conditional probability,
the second one from the hypothesis that \(P(T, X) = P(T | X) P(X)\)
so that $T$ causally depends on $X\,.$</p>

<p>In order for this quantity to be finite we must require that both the denominators are
strictly positive, and since $P(T=0 \vert X) = 1 - P(T=1\vert X)$ we can write this requirement,
named the <strong>positivity</strong> assumption, as</p>

\[0 &lt; P(T=t | X) &lt; 1 \, \forall t\]

<p>In other words, for each value of X we must have some representative individual in
each group.
If, for some group, everyone receives the treatment or everyone receives the control,
we are not able to estimate the treatment versus control effect.</p>

<!--
We can better see why randomization is important from the adjustment formula.
If the treatment assignment is not random, one cannot assume that $P(X)$ is the same for the two
groups, and therefore one should replace it with $P(X=x | T=t)\,.$
A careful observer could therefore always find some confounder which could differ
between the two groups, and claim that the difference in the effect is due to the
different distribution of that confounder.
Within randomization, this cannot happen, as the probability distribution
of $X$ is independent on $T\,.$
-->

<h2 id="some-remarks">Some remarks</h2>

<p>Our discussion on causality both applies to frequentist statistics
and to bayesian one. However, as pointed out by Rubin himself in his 1990
article “Formal mode of statistical inference for causal effects”,
it is straightforward to apply fully bayesian methods to causal inference.
However, it is very easy to misuse it, as</p>

<blockquote cite="https://www.sciencedirect.com/science/article/abs/pii/0378375890900778">
there appears to be no formal requirement to make sure
that the models conform at all to reality. 
In practice, careful model monitoring is
needed, and for this purpose, the randomization-based approaches we have presented
can be regarded as providing useful guidelines.
</blockquote>

<p>It is therefore crucial both to ensure that experimental setup fulfills the
above mentioned assumptions and that the statistical model is appropriate
in describing it.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We introduced the counterfactual definition of causality, and we introduced
Rubin’s potential outcomes. We also discussed under which conditions
we can compute the average treatment effect.</p>

<h2 id="suggested-readings">Suggested readings</h2>

<ul>
  <li><cite>Imbens, G. W., Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. US: Cambridge University Press.<cite></cite></cite></li>
  <li><cite><a href="https://arxiv.org/pdf/2206.15460.pdf">Li, Ding, Mealli (2022). Bayesian Causal Inference: A Critical Review</a></cite></li>
  <li><cite>Ding, P. (2024). A First Course in Causal Inference. CRC Press.</cite></li>
</ul>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/causal_intro/" /><summary type="html"><![CDATA[When association implies causation]]></summary></entry><entry><title type="html">Experiment analysis with many blocking variables</title><link href="http://localhost:4000/statistics/experiment_design_cont" rel="alternate" type="text/html" title="Experiment analysis with many blocking variables" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>http://localhost:4000/statistics/experiment_design_cont</id><content type="html" xml:base="http://localhost:4000/statistics/experiment_design_cont"><![CDATA[<p>In the last post, we discussed how to design experiments
either without blocking variables, or with a single blocking variable.
In this post we will extend the previous discussion to a larger number of
blocking variables.</p>

<h2 id="full-factorial-design">Full factorial design</h2>

<p>Let us now consider the extension of the RBD to $k&gt;2$ blocking variables,
and let us assume that each of them has $n$ possible levels.
In a full factorial design, you run the experiment for all the possible combinations
of the different levels of the blocking variables.
Since the number of possible combinations is $n^k\,,$ the number of levels and
of variables is usually indicated as $n^k\,.$
Using this kind of design has of course pros and cons: the design is easier to
perform and to communicate, and you can measure all the possible interactions
among the variables.
It requires however many different runs, and this can be both time and resources demanding.</p>

<p>In this section, we will re-analyze the experiment proposed as an exercise at page
68 of “Design and analysis of experiments with R” taken from <a href="https://www.tandfonline.com/doi/abs/10.1080/08982118908962680">an article by Stuart Hunter</a>,
and here’s the dataset</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">ethanol</th>
      <th style="text-align: right">air_fuel_ratio</th>
      <th style="text-align: right">co2_emission_1</th>
      <th style="text-align: right">co2_emission_2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0.1</td>
      <td style="text-align: right">14</td>
      <td style="text-align: right">66</td>
      <td style="text-align: right">62</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: right">0.1</td>
      <td style="text-align: right">15</td>
      <td style="text-align: right">72</td>
      <td style="text-align: right">67</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">0.1</td>
      <td style="text-align: right">16</td>
      <td style="text-align: right">68</td>
      <td style="text-align: right">66</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">0.2</td>
      <td style="text-align: right">14</td>
      <td style="text-align: right">78</td>
      <td style="text-align: right">81</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">0.2</td>
      <td style="text-align: right">15</td>
      <td style="text-align: right">80</td>
      <td style="text-align: right">81</td>
    </tr>
    <tr>
      <td style="text-align: right">5</td>
      <td style="text-align: right">0.2</td>
      <td style="text-align: right">16</td>
      <td style="text-align: right">66</td>
      <td style="text-align: right">69</td>
    </tr>
    <tr>
      <td style="text-align: right">6</td>
      <td style="text-align: right">0.3</td>
      <td style="text-align: right">14</td>
      <td style="text-align: right">90</td>
      <td style="text-align: right">94</td>
    </tr>
    <tr>
      <td style="text-align: right">7</td>
      <td style="text-align: right">0.3</td>
      <td style="text-align: right">15</td>
      <td style="text-align: right">75</td>
      <td style="text-align: right">78</td>
    </tr>
    <tr>
      <td style="text-align: right">8</td>
      <td style="text-align: right">0.3</td>
      <td style="text-align: right">16</td>
      <td style="text-align: right">60</td>
      <td style="text-align: right">58</td>
    </tr>
  </tbody>
</table>

<p>The author replicated each measure twice, and he determined the $CO_2$
emissions of a fuel depending on the amount of ethanol used
and on the air/fuel ratio.</p>

<p>As in the original article, we will use a quadratic model with a linear interaction term.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_in</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./data/co2.csv'</span><span class="p">)</span>
<span class="n">df1</span> <span class="o">=</span> <span class="n">df_in</span><span class="p">[[</span><span class="s">'ethanol'</span><span class="p">,</span> <span class="s">'air_fuel_ratio'</span><span class="p">,</span> <span class="s">'co2_emission_1'</span><span class="p">]]</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">df_in</span><span class="p">[[</span><span class="s">'ethanol'</span><span class="p">,</span> <span class="s">'air_fuel_ratio'</span><span class="p">,</span> <span class="s">'co2_emission_2'</span><span class="p">]]</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df1</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'co2_emission_1'</span><span class="p">:</span> <span class="s">'co2_emission'</span><span class="p">}),</span>
<span class="n">df2</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'co2_emission_2'</span><span class="p">:</span> <span class="s">'co2_emission'</span><span class="p">})],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>Let us first normalize the regression variables,
and let us assume a linear interacting model for the outcome</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x0</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'ethanol'</span><span class="p">]</span><span class="o">-</span><span class="mf">0.2</span><span class="p">)</span><span class="o">/</span><span class="mf">0.2</span>
<span class="n">x1</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'air_fuel_ratio'</span><span class="p">]</span><span class="o">-</span><span class="mi">14</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x0</span><span class="p">)),</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x0</span><span class="o">*</span><span class="n">x1</span><span class="p">,</span> <span class="n">x0</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">x1</span><span class="o">**</span><span class="mi">2</span><span class="p">])</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">coords</span><span class="o">=</span>
              <span class="p">{</span><span class="s">'col_id'</span><span class="p">:</span> 
                   <span class="p">[</span><span class="s">'intercept'</span><span class="p">,</span> <span class="s">'ethanol'</span><span class="p">,</span> <span class="s">'af_ratio'</span><span class="p">,</span> <span class="s">'interaction'</span><span class="p">,</span> <span class="s">'ethanol_sq'</span><span class="p">,</span> <span class="s">'af_ratio_sq'</span><span class="p">],</span>
                      <span class="s">'obs_id'</span><span class="p">:</span> <span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">})</span> <span class="k">as</span> <span class="n">ffmodel</span><span class="p">:</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Data</span><span class="p">(</span><span class="s">'X'</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">coords</span><span class="o">=</span><span class="p">[</span><span class="s">'col_id'</span><span class="p">,</span> <span class="s">'obs_id'</span><span class="p">])</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="p">[</span><span class="s">'col_id'</span><span class="p">])</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'co2_emission'</span><span class="p">])</span>
    <span class="n">idata_rbd</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_rbd</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/experiment_design/ff_trace.webp" alt="" /></p>

<p>The trace seems fine, but let us take a closer look at the values of the $\beta$ coefficients.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">idata_rbd</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'beta'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/experiment_design/ff_forest.webp" alt="" /></p>

<p>Both the variables play an important role, and the presence of an interaction
between them is quite strong.
Let us verify if we are able to reproduce the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_out</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s">'ethanol'</span><span class="p">,</span> <span class="s">'air_fuel_ratio'</span><span class="p">]].</span><span class="n">drop_duplicates</span><span class="p">()</span>
<span class="n">x0new</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_out</span><span class="p">[</span><span class="s">'ethanol'</span><span class="p">]</span><span class="o">-</span><span class="mf">0.2</span><span class="p">)</span><span class="o">/</span><span class="mf">0.2</span>
<span class="n">x1new</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_out</span><span class="p">[</span><span class="s">'air_fuel_ratio'</span><span class="p">]</span><span class="o">-</span><span class="mi">14</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
<span class="n">xnew</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x0new</span><span class="p">)),</span> <span class="n">x0new</span><span class="p">,</span> <span class="n">x1new</span><span class="p">,</span> <span class="n">x0new</span><span class="o">*</span><span class="n">x1new</span><span class="p">,</span> <span class="n">x0new</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">x1new</span><span class="o">**</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ffmodel</span><span class="p">.</span><span class="n">add_coords</span><span class="p">({</span><span class="s">'sample_new'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x0new</span><span class="p">))})</span>

<span class="k">with</span> <span class="n">ffmodel</span><span class="p">:</span>
    <span class="n">Xnew</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Data</span><span class="p">(</span><span class="s">'Xnew'</span><span class="p">,</span> <span class="n">xnew</span><span class="p">,</span> <span class="n">coords</span><span class="o">=</span><span class="p">[</span><span class="s">'col_id'</span><span class="p">,</span> <span class="s">'sample_new'</span><span class="p">])</span>
    <span class="n">mu_new</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">Xnew</span><span class="p">)</span>
    <span class="n">y_new</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y_new'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu_new</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata_rbd</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y_new'</span><span class="p">])</span>
    
<span class="n">df_out</span><span class="p">[</span><span class="s">'mean'</span><span class="p">]</span> <span class="o">=</span> <span class="n">ppc</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_new'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">)).</span><span class="n">values</span>
<span class="n">df_out</span><span class="p">[</span><span class="s">'q03'</span><span class="p">]</span> <span class="o">=</span> <span class="n">ppc</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_new'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">)).</span><span class="n">values</span>
<span class="n">df_out</span><span class="p">[</span><span class="s">'q97'</span><span class="p">]</span> <span class="o">=</span> <span class="n">ppc</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_new'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.97</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">)).</span><span class="n">values</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'ethanol'</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'air_fuel_ratio'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'co2_emission'</span><span class="p">)</span>
<span class="n">legend</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_out</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'ethanol'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'mean'</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'air_fuel_ratio'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_out</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'ethanol'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'q03'</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'air_fuel_ratio'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_out</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'ethanol'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'q97'</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'air_fuel_ratio'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/experiment_design/ff_ppc.webp" alt="" /></p>

<p>The model seems to reproduce the observed data quite well.
As an exercise, verify the performances of the linear model with the above model.</p>

<h2 id="latin-square-design">Latin square design</h2>

<p>In the randomized block design, one can only control for one factor, but it may also be the case
that you need to control for more than one factor.
The latin square design is useful when you need to control for two factors,
but you don’t have enough resources to perform a full factorial design.
This design can be visualized by drawing an $n\times n$ table, where each row corresponds
to the level of one factor, the other level is represented by the column, and each matrix element
is represented by a number $1,…,n$ or by a (latin) letter,
and correspond to the treatment group.
In a latin square, no letter can appear twice in any row or column.
By using the latin square, you assign each possible treatment to each row and column.</p>

<p>All the possible $2\times 2$ latin squares are</p>

\[\begin{pmatrix}
1 &amp; 2 \\
2 &amp; 1 \\
\end{pmatrix},
\begin{pmatrix}
2 &amp; 1 \\
1 &amp; 2 \\
\end{pmatrix}\]

<p>while a possible $3\times 3$ latin square is</p>

\[\begin{pmatrix}
1 &amp; 2 &amp; 3 \\
2 &amp; 3 &amp; 1 \\
3 &amp; 1 &amp; 2 \\
\end{pmatrix}\]

<p>In order to obtain a random $n\times n$ latin square, you can simply use the following
function</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">random</span>

<span class="k">def</span> <span class="nf">latin_square</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">row</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
    <span class="n">mat</span> <span class="o">=</span> <span class="p">[</span><span class="n">row</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">mat</span> <span class="o">+=</span> <span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span><span class="o">+</span><span class="n">row</span><span class="p">[:</span><span class="n">i</span><span class="p">]]</span>
    <span class="n">random</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">mat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mat</span>
</code></pre></div></div>

<p>We will analyze the dataset provided at <a href="https://rdrr.io/github/kwstat/agridat/man/bridges.cucumber.html">this link</a>
where we want to assess the performances of a set of cucumber cultivars, and the
experimental setup is a latin square design experiment repeated into two different locations.</p>

<p>We will use a hierarchical model for the latin square components, plus a linear term 
to account for the location.
We will only take a non-zero mean for the cultivar parameter, while we will
assume a zero mean for the row and column components.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_latin_square</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/kwstat/agridat/main/data/bridges.cucumber.txt'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="p">)</span>
<span class="n">df_latin_square</span><span class="p">[</span><span class="s">'gen'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">df_latin_square</span><span class="p">[</span><span class="s">'gen'</span><span class="p">])</span>
<span class="n">df_latin_square</span><span class="p">[</span><span class="s">'loc'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">df_latin_square</span><span class="p">[</span><span class="s">'loc'</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">lsmodel</span><span class="p">:</span>
    <span class="n">xi</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'xi'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'rho'</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">sig_alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'sig_alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">sig_beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'sig_beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">sig_gamma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'sig_gamma'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span><span class="n">mu</span> <span class="o">+</span> <span class="n">rho</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">sig_alpha</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">rho</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">sig_beta</span><span class="p">)</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'gamma'</span><span class="p">,</span> <span class="n">rho</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">sig_gamma</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">xi</span><span class="o">*</span><span class="n">df_latin_square</span><span class="p">[</span><span class="s">'loc'</span><span class="p">].</span><span class="n">cat</span><span class="p">.</span><span class="n">codes</span><span class="o">+</span><span class="n">alpha</span><span class="p">[</span><span class="n">df_latin_square</span><span class="p">[</span><span class="s">'gen'</span><span class="p">].</span><span class="n">cat</span><span class="p">.</span><span class="n">codes</span><span class="p">]</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="n">df_latin_square</span><span class="p">[</span><span class="s">'row'</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span> <span class="n">gamma</span><span class="p">[</span><span class="n">df_latin_square</span><span class="p">[</span><span class="s">'col'</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_latin_square</span><span class="p">[</span><span class="s">'yield'</span><span class="p">])</span>

<span class="k">with</span> <span class="n">lsmodel</span><span class="p">:</span>
    <span class="n">idata_ls</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span>
                        <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_ls</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/experiment_design/ls_trace.webp" alt="" /></p>

<p>In the above model, $\alpha$ corresponds to the average cultivar yield
for the first location, where the average is taken over the rows and columns.
We can also identify $\mu$ with the average yield for the first location.</p>

<p>Another reasonable choice would have been to parametrize the model in order to obtain
the difference between the first cultivar and the other cultivars as parameters,
and we leave the implementation of this model as an exercise.</p>

<p>Let us now take a closer look at the values of the different $\alpha$s.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">idata_ls</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'^alpha'</span><span class="p">],</span> <span class="n">filter_vars</span><span class="o">=</span><span class="s">'regex'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/experiment_design/ls_forest.webp" alt="" /></p>

<p>We can easily assess the performances of the cultivars with respect to the benchmark cultivar:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span>
    <span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">idata_ls</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">][:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="n">idata_ls</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">][:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
               <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">"$\alpha_"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="sa">r</span><span class="s">"-\alpha_0$"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/experiment_design/ls_alpha_diff.webp" alt="" /></p>

<h2 id="conclusions">Conclusions</h2>

<p>We have discussed how to adapt some model used in experimental
design with more than one blocking variable to make them Bayesian, and again we did so by using PyMC.</p>

<h2 id="suggested-readings">Suggested readings</h2>

<ul>
  <li><cite>Box, G. E. P., Hunter, J. S., Hunter, W.G. (2005). Statistics for experimenters: design, innovation, and discovery. Wiley.</cite></li>
  <li><cite>Lawson, J. (2014). Design and Analysis of Experiments with R. CRC Press.<cite></cite></cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>

<div class="code">
Last updated: Mon Aug 19 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.4
<br />
IPython version      : 8.24.0
<br />

<br />
xarray : 2024.5.0
<br />
numpyro: 0.15.0
<br />
jax    : 0.4.28
<br />
jaxlib : 0.4.28
<br />

<br />
seaborn   : 0.13.2
<br />
pandas    : 2.2.2
<br />
arviz     : 0.18.0
<br />
matplotlib: 3.9.0
<br />
numpy     : 1.26.4
<br />
pymc      : 5.15.0
<br />

<br />
Watermark: 2.4.3
<br />
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/experiment_analysis/" /><summary type="html"><![CDATA[Dealing with more complicated experiments]]></summary></entry><entry><title type="html">Experiment analysis</title><link href="http://localhost:4000/statistics/experiment_design" rel="alternate" type="text/html" title="Experiment analysis" /><published>2024-06-02T00:00:00+00:00</published><updated>2024-06-02T00:00:00+00:00</updated><id>http://localhost:4000/statistics/experiment_design</id><content type="html" xml:base="http://localhost:4000/statistics/experiment_design"><![CDATA[<p>Experimental design was developed by Fisher in the context of agriculture,
with the aim to better make decisions based on experimental data.
Nowadays experimental design is applied in many different fields,
but unfortunately there aren’t many textbook which treat this
topic from a Bayesian perspective.
The advantage of a Bayesian treatment is clear if you consider
that, often, collective data requires a big effort.
As an example, running an experiment in the agriculture
may take months, so it is crucial to extract as much information as possible
from every single datum.
The reason for sticking to the traditional data analysis approaches
might be due to the fact that Fisher had a very strong positions
against the Bayesian statistics, or maybe because most of the available
software is based on Fisher’s models.
It is however very easy to build Bayesian models for this kind
of application, and we will take a look at how to do so.</p>

<h2 id="principles-of-experimental-design">Principles of experimental design</h2>
<p>When designing an experiment, you should keep in mind three fundamental
principles:</p>
<ul>
  <li>randomization</li>
  <li>replication</li>
  <li>blocking</li>
</ul>

<p>When we talk about <strong>randomization</strong>, we refer to the random assignment of the
experimental units to different treatment condition.
Randomization allows you to reduce the systematic error, and any other residual
variation in the experimental procedure is random by construction.</p>

<p><strong>Replication</strong> is the repetition of the experimental procedure,
from the preparation to the measurement.
The purpose of replication is to have a reliable estimate of the variance,
and without replication your effect estimate might not be reliable.</p>

<p><strong>Blocking</strong> refers to fixing (when possible) or measuring (otherwise)
any factor which might reasonably affect the experimental outcome.
Controlling for external factors which might affect our measurement
helps us in identifying the sources of variability in the outcome,
and therefore will give us a more precise estimate of the effect.</p>

<h2 id="completely-randomized-design">Completely randomized design</h2>

<p>In a completely randomized experiment we assign the treatment to each experimental
unit completely at random.
In the simplest version of this design, we have $n \times k$ units,
and we want to randomly assign each unit to one of $n$ treatments.</p>

<p>As an example, consider the dataset at <a href="https://www.itl.nist.gov/div898/software/dataplot/data/BOXBLOOD.DAT">this link</a>.
The aim of the experiment is to determine whether different diets
had different effects on the blood coagulation time.
The example is taken from page 155 of <a href="https://pages.stat.wisc.edu/~yxu/Teaching/16%20spring%20Stat602/%5BGeorge_E._P._Box,_J._Stuart_Hunter,_William_G._Hu(BookZZ.org).pdf">this textbook</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/coagulation.csv'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'Y'</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'X1'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/experiment_design/coagulation_boxplot.webp" alt="The boxplot of the
coagulation dataset" /></p>

<p>In the dataset we have 24 units, and to each unit we want to assign
one of the four possible treatments.
This could have been done by running</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n</span><span class="o">=</span><span class="mi">4</span>
<span class="n">k</span><span class="o">=</span><span class="mi">6</span>

<span class="n">items</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">k</span><span class="p">))</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>
<span class="n">groups</span> <span class="o">=</span> <span class="p">[</span><span class="n">items</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">k</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
</code></pre></div></div>

<p>We have different way to analyze this dataset, and we will use the following
model</p>

\[\begin{align}
&amp;
y_{ij} \sim \mathcal{N}(\mu_i, \sigma)
\\
&amp;
\sigma \sim \mathcal{HN}(0, 100)
\\
&amp;
\mu_i \sim \mathcal{N}(0, 100)
\end{align}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">coag_model</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">'X1'</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'Y'</span><span class="p">])</span>
    <span class="n">idata_coag</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_coag</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/experiment_design/coagulation_trace.webp" alt="The trace of the
coagulation model" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">idata_coag</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'mu'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/experiment_design/coagulation_forest.webp" alt="The forest plot of the
means for the coagulation model" /></p>

<p>There is no doubt that, in this study, we observe that a diet variation
corresponds to a coagulation time variation.</p>

<h2 id="randomized-block-design">Randomized block design</h2>
<p>In a randomized block design, we furthermore block one or more
factor (confounder) which may be a source of variability for
the outcome.
As an example, let us consider the example in chapter 4.2 of the same book.
In this experiment the authors compared the penicillin yield
of different processes (formulas).
The yield does not only depend on the process, but also on the
ingredients, so the authors used one of the main ingredients,
the corn steep liquor, as blocking factor.
The corresponding dataset can be found <a href="https://www.itl.nist.gov/div898/software/dataplot/data/BOXPENIC.DAT">here</a>.</p>

<p>In order to analyze this dataset, we will use the following model</p>

\[\begin{align}
&amp;
y_{ijk} \sim \mathcal{N}(\mu_i + \tau_j, \sigma)
\\
&amp;
\sigma \sim \mathcal{HN}(0, 100)
\\
&amp;
\mu_i \sim \mathcal{N}(0, 100)
\\
&amp;
\tau_j \sim \mathcal{N}(0, \rho)
\\
&amp;
\rho \sim \mathcal{HN}(0, 100)
\end{align}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_rbd</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/penicillin.csv'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">crb_model</span><span class="p">:</span>
    
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s">'rho'</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'tau'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">rho</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">[</span><span class="n">df_rbd</span><span class="p">[</span><span class="s">'T'</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">tau</span><span class="p">[</span><span class="n">df_rbd</span><span class="p">[</span><span class="s">'X'</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_rbd</span><span class="p">[</span><span class="s">'Y'</span><span class="p">])</span>
    <span class="n">idata_rbd</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_rbd</span><span class="p">)</span>
</code></pre></div></div>

<p>As we can clearly see, the ‘X’ effect has average zero, therefore
$\mu$ correctly estimates the average treatment effect.</p>

<p><img src="/docs/assets/images/statistics/experiment_design/penicillin_trace.webp" alt="The trace plot of the
means for the penicillin model" /></p>

<p>We can now take a better look at the values of $\mu$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">idata_rbd</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'mu'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/experiment_design/penicillin_forest.webp" alt="The forest plot of the
means for the penicillin model" /></p>

<p>As you can see, the treatment 2 gives slightly higher yields with respect to the
other treatment.</p>

<p>When designing an experiment, the simplest solution is to use a completely randomized design,
and measure the remaining blocking variables.
This might however allocate treatment in an undesired way,
so you might decide to assign the treatment according to a probability
which depends on the blocking variable.</p>

<p>In fact, if a relevant variable is unbalanced across the treatment groups, you cannot
exclude that a difference into the outcome can be imputed to the different average values
of the variable across the groups.</p>

<p>If the blocking variable is discrete, blocking for it is  a straightforward procedure.
If it is continuous, however, in order to do so, you are forced stratify it,
namely to build a discrete variable and mapping the continuous variable to the discrete one.
As an example, when blocking on the age of a person, you might stratify it into
0-9, 10-19, 20-29 etc.
The values of the discrete variable are often named strata (plural of stratum) of levels
of the continuous variable, and the first term is generally used if you are measuring it,
while the latter is preferred when you are fixing it.
For a more in-depth discussion on this topic, you can take a look at
<a href="https://arxiv.org/pdf/2305.18793">https://arxiv.org/pdf/2305.18793</a>.</p>

<h2 id="matched-pairs-design">Matched pairs design</h2>

<p>The matched pairs design can be considered a special case of the randomized block design
with two treatment groups (the extension to more groups is straightforward).
Rather than randomly assigning each unit to one of two groups, we first pair
units with similar relevant features, and then we toss a coin to decide which element
of the pair belongs to which group.</p>

<p>This kind of pairing can be useful when we have small samples or if we have very similar
pairs of units, such as twins.</p>

<p>In order to illustrate the analysis method, we will re-analyze the <a href="https://www.jstor.org/stable/2117766">Orley Ashenfelter and Alan Krueger</a>
article, where the authors observed the impact on the instruction on the wage on a large set
of twins.
The dataset is available <a href="https://dataspace.princeton.edu/handle/88435/dsp012801pg35n">here</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dfk</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_stata</span><span class="p">(</span><span class="s">'data/pubtwins.dta'</span><span class="p">)</span>

<span class="n">df_red</span> <span class="o">=</span> <span class="n">dfk</span><span class="p">[</span><span class="n">dfk</span><span class="p">[</span><span class="s">'first'</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">][[</span><span class="s">'deduct'</span><span class="p">,</span> <span class="s">'dlwage'</span><span class="p">]]</span>
</code></pre></div></div>

<p>In the above dataset, we only kept the rows corresponding to the first child,
and the columns corresponding to the number of education difference between the first
and the second child in years and their wage difference.
Since each subject has a similar test subject,
we can directly make inference on their wage difference.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_red</span><span class="p">[</span><span class="s">'deduct'</span><span class="p">],</span> <span class="n">df_red</span><span class="p">[</span><span class="s">'dlwage'</span><span class="p">])</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/experiment_design/twins_scatter.webp" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span>  <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">beta</span><span class="o">*</span><span class="n">df_red</span><span class="p">[</span><span class="s">'deduct'</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_red</span><span class="p">[</span><span class="s">'dlwage'</span><span class="p">])</span>
    <span class="n">idata_twins</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
                      <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_twins</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/experiment_design/twins_trace.webp" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xpl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">6.5</span><span class="p">,</span> <span class="mf">6.5</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">)</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">idata_twins</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'beta'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="n">xpl</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">xpl</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.03</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.97</span><span class="p">),</span>
               <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xpl</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_red</span><span class="p">[</span><span class="s">'deduct'</span><span class="p">],</span> <span class="n">df_red</span><span class="p">[</span><span class="s">'dlwage'</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/experiment_design/twins_ppc.webp" alt="" /></p>

<h2 id="conclusions">Conclusions</h2>

<p>We have discussed how to adapt some classical model used in experimental
design to make them Bayesian, and we have done so by using PyMC.
This was only an introductory discussion on the topic, as experimental design
is a very broad and active research topic.
In the next post, we will continue our discussion about
experimental design for more involved experiments.</p>

<h2 id="suggested-readings">Suggested readings</h2>

<ul>
  <li><cite>Box, G. E. P., Hunter, J. S., Hunter, W.G. (2005). Statistics for experimenters: design, innovation, and discovery. Wiley.</cite></li>
  <li><cite>Lawson, J. (2014). Design and Analysis of Experiments with R. CRC Press.<cite></cite></cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>

<div class="code">
Last updated: Mon Aug 19 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.4
<br />
IPython version      : 8.24.0
<br />

<br />
xarray : 2024.5.0
<br />
numpyro: 0.15.0
<br />
jax    : 0.4.28
<br />
jaxlib : 0.4.28
<br />

<br />
matplotlib: 3.9.0
<br />
pymc      : 5.15.0
<br />
seaborn   : 0.13.2
<br />
numpy     : 1.26.4
<br />
arviz     : 0.18.0
<br />
pandas    : 2.2.2
<br />

<br />
Watermark: 2.4.3
<br />
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/experiment_analysis/" /><summary type="html"><![CDATA[How to design and analyze experiment]]></summary></entry></feed>