<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-07-15T06:45:09+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Data Perspectives</title><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><entry><title type="html">Introduction to this section</title><link href="http://localhost:4000/statistics/other_intro" rel="alternate" type="text/html" title="Introduction to this section" /><published>2024-07-05T00:00:00+00:00</published><updated>2024-07-05T00:00:00+00:00</updated><id>http://localhost:4000/statistics/other_intro</id><content type="html" xml:base="http://localhost:4000/statistics/other_intro"><![CDATA[<p>In this section I will put personal opinions and references to statistics which
are not strictly related to Bayesian models.</p>

<p>I decided to add this section because statistics is a huge discipline,
and the more I study it, the more I found interesting topics, which deserve to
be discussed.
The reason to discuss them is rather selfish, and it’s simply because
I think discussing is one of the best ways to clarify my own ideas and spotting
my own mistakes.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/other_intro/" /><summary type="html"><![CDATA[Where I will discuss more general topics]]></summary></entry><entry><title type="html">Synthetic control</title><link href="http://localhost:4000/statistics/synthetic_control" rel="alternate" type="text/html" title="Synthetic control" /><published>2024-06-30T00:00:00+00:00</published><updated>2024-06-30T00:00:00+00:00</updated><id>http://localhost:4000/statistics/synthetic_control</id><content type="html" xml:base="http://localhost:4000/statistics/synthetic_control"><![CDATA[<p>The <strong>synthetic control</strong> method recently became a very popular method
among economists (although I honestly can’t see the same enthusiasm in
the statistics community).
This method has been widely (and a little bit wildly) used to assess
the effects on a quantity \(Y^{\bar{s}}_t\) of the introduction of a new policy into a country $s$
(or other geographical region) at a time $t=t_1$.
Assuming that you have the same quantity for a set of similar countries $s_i$
as well as for the target country $\bar{s}\,,$
you assume that the time behavior of \(Y_{\bar{s}} = (Y_{t_0}^{\bar{s}}, \dots, Y^{\bar{s}}_{t_1})\) before the intervention is given by a weighted
average of $Y^{s_i}\,.$</p>

<p>You moreover assume, as control, the same weighted average
\(\bar{Y}^{\bar{s}}\) after the intervention.</p>

<p>A very detailed discussion of this method can be found on <a href="https://juanitorduz.github.io/synthetic_control_pymc/">Juan Camilo Orduz’ page</a>.
We will use the same model, but we will apply it to a different dataset.</p>

<p>While in fact he uses PyMC to reproduce <a href="https://matheusfacure.github.io/python-causality-handbook/landing-page.html">this example</a>,
we will use it to perform a simplified re-analysis of <a href="https://link.springer.com/article/10.1007/s10584-021-03111-2">this article</a>, where the authors analyze the impact of the introduction
of a policy for the reduction of the $CO_2$ emissions in the UK.
The dataset used in this work can be found <a href="https://zenodo.org/records/4566804">on Zenodo</a>.</p>

<p>The authors of the original work, in fact, performed a careful analysis
of the control set, while we will limit ourself to the set of countries
who were in the OECD organization in 2001 and who had not adopted any 
$CO_2$ reduction policy before that year.
We will assume</p>

\[Y^{\bar{s}} \sim \mathcal{N}(\mu, \sigma)\]

<p>In order to ensure that the behavior before the intervention is carefully
reproduced, we assume a small variance</p>

\[\sigma \sim \mathcal{Exp}(100)\]

<p>As anticipated, $\mu$ is given by</p>

\[\mu = \sum_{i=1}^n \omega_{i} Y^i\]

<p>We assume that the weights sum up to one, so we assume</p>

\[\omega \sim \mathcal{Dir}(1/n,\dots,1/n)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc.sampling_jax</span> <span class="k">as</span> <span class="n">pmjax</span>

<span class="n">df_dt</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./data/climate_policies.csv'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">';'</span><span class="p">)</span>
<span class="n">df_carb</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./data/nation.1751_2014.csv'</span><span class="p">)</span>

<span class="n">df_red</span> <span class="o">=</span> <span class="n">df_carb</span><span class="p">[</span><span class="n">df_carb</span><span class="p">[</span><span class="s">'Year'</span><span class="p">]</span><span class="o">&gt;=</span><span class="mi">1990</span><span class="p">][[</span><span class="s">'Nation'</span><span class="p">,</span> <span class="s">'Year'</span><span class="p">,</span> <span class="s">'Per capita CO2 emissions (metric tons of carbon)'</span><span class="p">]]</span>

<span class="n">df_co2</span> <span class="o">=</span> <span class="n">df_red</span><span class="p">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s">'Year'</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s">'Nation'</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s">'Per capita CO2 emissions (metric tons of carbon)'</span><span class="p">)</span>

<span class="c1"># Taken from the repo
</span>
<span class="n">oecd</span> <span class="o">=</span> <span class="p">[</span><span class="s">"AUSTRALIA"</span><span class="p">,</span><span class="s">"AUSTRIA"</span><span class="p">,</span><span class="s">"BELGIUM"</span><span class="p">,</span><span class="s">"CANADA"</span><span class="p">,</span><span class="s">"CZECH REPUBLIC"</span><span class="p">,</span>
        <span class="s">"DENMARK"</span><span class="p">,</span><span class="s">"FINLAND"</span><span class="p">,</span><span class="s">"FRANCE (INCLUDING MONACO)"</span><span class="p">,</span><span class="s">"GERMANY"</span><span class="p">,</span>
        <span class="s">"GREECE"</span><span class="p">,</span><span class="s">"HUNGARY"</span><span class="p">,</span><span class="s">"ICELAND"</span><span class="p">,</span><span class="s">"IRELAND"</span><span class="p">,</span><span class="s">"ITALY (INCLUDING SAN MARINO)"</span><span class="p">,</span>
        <span class="s">"JAPAN"</span><span class="p">,</span><span class="s">"LUXEMBOURG"</span><span class="p">,</span><span class="s">"MEXICO"</span><span class="p">,</span><span class="s">"NETHERLANDS"</span><span class="p">,</span><span class="s">"NEW ZEALAND"</span><span class="p">,</span><span class="s">"NORWAY"</span><span class="p">,</span>
        <span class="s">"POLAND"</span><span class="p">,</span><span class="s">"PORTUGAL"</span><span class="p">,</span><span class="s">"SLOVAKIA"</span><span class="p">,</span><span class="s">"REPUBLIC OF KOREA"</span><span class="p">,</span><span class="s">"SPAIN"</span><span class="p">,</span><span class="s">"SWEDEN"</span><span class="p">,</span>
        <span class="s">"SWITZERLAND"</span><span class="p">,</span><span class="s">"TURKEY"</span><span class="p">,</span><span class="s">"UNITED KINGDOM"</span><span class="p">,</span><span class="s">"UNITED STATES OF AMERICA"</span><span class="p">]</span>

<span class="n">to_exclude</span> <span class="o">=</span> <span class="p">[</span><span class="s">'DENMARK'</span><span class="p">,</span> <span class="s">'ESTONIA'</span><span class="p">,</span> <span class="s">'FINLAND'</span><span class="p">,</span> <span class="s">'NETHERLANDS'</span><span class="p">,</span> <span class="s">'NORWAY'</span><span class="p">,</span>
       <span class="s">'SLOVENIA'</span><span class="p">,</span> <span class="s">'SWEDEN'</span><span class="p">,</span> <span class="s">"UNITED KINGDOM"</span><span class="p">]</span>

<span class="n">donors</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">oecd</span><span class="p">)</span><span class="o">-</span><span class="nb">set</span><span class="p">(</span><span class="n">to_exclude</span><span class="p">))</span>

<span class="n">df_in</span> <span class="o">=</span> <span class="n">df_co2</span><span class="p">[</span><span class="n">donors</span><span class="p">].</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">sc_model</span><span class="p">:</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="s">'w'</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">)))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">):</span>
        <span class="n">mu</span> <span class="o">+=</span> <span class="n">w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">df_in</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">loc</span><span class="p">[</span><span class="mi">1990</span><span class="p">:</span><span class="mi">2001</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_co2</span><span class="p">[</span><span class="s">'UNITED KINGDOM'</span><span class="p">].</span><span class="n">loc</span><span class="p">[</span><span class="mi">1990</span><span class="p">:</span><span class="mi">2001</span><span class="p">])</span>

<span class="k">with</span> <span class="n">sc_model</span><span class="p">:</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pmjax</span><span class="p">.</span><span class="n">sample_numpyro_nuts</span><span class="p">(</span><span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>

</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/synthetic_control/trace.webp" alt="The trace plot" /></p>

<p>We ran quite a large number of draws as the number of parameters is quite large
and rather unconstrained. However, the trace looks fine.
An important thing that one should always verify when using
a synthetic control, is that the weights must be sparse (only few should
dominate, while the remaining should be close to 0).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'w'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/synthetic_control/weights.webp" alt="The forest plot of the weights" /></p>

<p>The requirement seems fulfilled, as only few dominate the entire fit.
We can now compute the posterior predictive before and after the intervention.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">sc_model</span><span class="p">:</span>
    <span class="n">mu1</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">):</span>
        <span class="n">mu1</span> <span class="o">+=</span> <span class="n">w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">df_in</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">loc</span><span class="p">[</span><span class="mi">2002</span><span class="p">:].</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">y1</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y1'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu1</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y'</span><span class="p">,</span> <span class="s">'y1'</span><span class="p">])</span>

<span class="n">yv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">ppc</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">20000</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
                     <span class="n">ppc</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y1'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">20000</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))],</span>
                     <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[[</span><span class="s">'right'</span><span class="p">,</span> <span class="s">'top'</span><span class="p">]].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
<span class="n">uk</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">df_co2</span><span class="p">[</span><span class="s">'UNITED KINGDOM'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'UK'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">yv</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">yv</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">synth</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">yv</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'Synthetic UK'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">2001</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Year"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">"Per capita $CO_2$ $m^3/Year$"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">'Synthetic'</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">yv</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">color</span><span class="o">=</span><span class="n">synth</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">get_color</span><span class="p">()</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">'UK'</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">df_co2</span><span class="p">[</span><span class="s">'UNITED KINGDOM'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">color</span><span class="o">=</span><span class="n">uk</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">get_color</span><span class="p">()</span> <span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'/home/stippe/thestippe.github.io/docs/assets/images/statistics/synthetic_control/posterior_predictive.webp'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/synthetic_control/posterior_predictive.webp" alt="The comparison between the true and the synthetic UK" /></p>

<p>As we can see, the behavior is very similar up to 2001, while after this date
the synthetic UK $CO_2$ consumption is larger than one of the true $UK\,.$
You can verify yourself that, by only fitting up to 2000, the result doesn’t
change, and the lines still diverge starting from 2002.
This is another important check that you should always perform when using the
synthetic control method.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We have seen how to implement the synthetic control method, together with
some of the most important checks that you should always do in order to
exclude major problems in your model.
We also re-analyzed <a href="https://link.springer.com/article/10.1007/s10584-021-03111-2">this article</a>, obtaining the same conclusions.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/discontinuity_regression/" /><summary type="html"><![CDATA[Building a doppleganger from the control group]]></summary></entry><entry><title type="html">Difference in difference</title><link href="http://localhost:4000/statistics/difference_in_differences" rel="alternate" type="text/html" title="Difference in difference" /><published>2024-06-23T00:00:00+00:00</published><updated>2024-06-23T00:00:00+00:00</updated><id>http://localhost:4000/statistics/difference_in_differences</id><content type="html" xml:base="http://localhost:4000/statistics/difference_in_differences"><![CDATA[<p>Difference in differences is a very old technique,
and one of the first applications of
this method was done by John Snow, who’s also
popular due to the cholera outbreak data visualization.</p>

<p>In his study, he used the <strong>Difference in Difference</strong>
(DiD) method to provide some evidence that,
during the London cholera epidemic of 1866,
the cholera was caused by drinking from a water
pump.
This method has been more recently used <a href="https://davidcard.berkeley.edu/papers/njmin-aer.pdf">by 
Card and Krueger in this work</a>
to analyze the causal relationship between
minimum wage and employment.
In 1992, the New Jersey increased the minimum wage
from 4.25 dollars to 5.00 dollars.
They compared the employment in Pennsylvania
and New Jersey before and after the minimum wage increase
to assess if it caused a decrease in the New Jersey
occupation, as supply and demand theory would predict.</p>

<p>DiD assumes that, before the intervention $I$,
the untreated group and the treated one
both evolve linearly with the time $t$ with the
same slope,
while after the intervention the treated group
changes slope.
Assuming, that the intervention was applied at time
$t=0$</p>

\[\begin{align}
&amp;
Y_{P}^0 = \alpha_{P} 
\\
&amp;
Y_{P}^1 = \alpha_{P} +\beta
\\
&amp;
Y_{NJ}^0 = \alpha_{NJ} 
\\
&amp;
Y_{NJ}^1 = \alpha_{NJ} +\beta + \gamma
\end{align}\]

<p>In the above formulas, the intervention effect
is simply $\gamma\,.$</p>

<h2 id="the-implementation">The implementation</h2>

<p>We downloaded the dataset from <a href="https://www.kaggle.com/code/harrywang/difference-in-differences-in-python/input">this page</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">df_employment</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/employment.csv'</span><span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_employment</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'state'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/difference_in_difference/pairplot.webp" alt="The dataset pairplot" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_before</span> <span class="o">=</span> <span class="n">df_employment</span><span class="p">[[</span><span class="s">'state'</span><span class="p">,</span> <span class="s">'total_emp_feb'</span><span class="p">]]</span>
<span class="n">df_after</span> <span class="o">=</span> <span class="n">df_employment</span><span class="p">[[</span><span class="s">'state'</span><span class="p">,</span> <span class="s">'total_emp_nov'</span><span class="p">]]</span>

<span class="c1"># We will assign t=0 data before treatment and t=1 after the treatment
# Analogously g=0 will be the control group, g=1 will be the test group
</span>
<span class="n">df_before</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">df_after</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">df_before</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'total_emp_feb'</span><span class="p">:</span> <span class="s">'Y'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_after</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'total_emp_nov'</span><span class="p">:</span> <span class="s">'Y'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df_before</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'state'</span><span class="p">:</span> <span class="s">'g'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_after</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'state'</span><span class="p">:</span> <span class="s">'g'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df_reg</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_before</span><span class="p">,</span> <span class="n">df_after</span><span class="p">])</span>

<span class="c1">## Let us build the interaction term
</span>
<span class="n">df_reg</span><span class="p">[</span><span class="s">'gt'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_reg</span><span class="p">[</span><span class="s">'g'</span><span class="p">]</span><span class="o">*</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span>

<span class="n">df_reg</span> <span class="o">=</span> <span class="n">df_reg</span><span class="p">[[</span><span class="s">'g'</span><span class="p">,</span> <span class="s">'t'</span><span class="p">,</span> <span class="s">'gt'</span><span class="p">,</span> <span class="s">'Y'</span><span class="p">]]</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">did_model</span><span class="p">:</span>
    <span class="n">beta_0</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_0'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">beta_g</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_g'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">beta_t</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_t'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">beta_gt</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_gt'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s">'nu'</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="n">beta_g</span><span class="o">*</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'g'</span><span class="p">]</span><span class="o">+</span> <span class="n">beta_t</span><span class="o">*</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span><span class="o">+</span> <span class="n">beta_gt</span><span class="o">*</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'gt'</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">,</span>
                  <span class="n">observed</span><span class="o">=</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'Y'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
    <span class="n">trace_did</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_did</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/difference_in_difference/trace.webp" alt="The model trace" /></p>

<p>The trace looks fine, let us now verify the posterior
predictive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">did_model</span><span class="p">:</span>
    <span class="n">y00</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y00'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">beta_0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    <span class="n">y10</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y10'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">beta_0</span><span class="o">+</span><span class="n">beta_g</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    <span class="n">y01</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y01'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">beta_0</span><span class="o">+</span><span class="n">beta_t</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    <span class="n">y11</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y11'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">beta_0</span><span class="o">+</span><span class="n">beta_g</span><span class="o">+</span><span class="n">beta_t</span><span class="o">+</span><span class="n">beta_gt</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    
    <span class="n">ppc_check</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span>
        <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y00'</span><span class="p">,</span><span class="s">'y01'</span><span class="p">,</span><span class="s">'y10'</span><span class="p">,</span><span class="s">'y11'</span><span class="p">],</span> <span class="n">trace</span><span class="o">=</span><span class="n">trace_did</span><span class="p">)</span>


<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">].</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">80</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
            <span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">az</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">ppc_check</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s">"y</span><span class="si">{</span><span class="n">g</span><span class="si">}{</span><span class="n">t</span><span class="si">}</span><span class="s">"</span><span class="p">],</span> <span class="n">group</span><span class="o">=</span><span class="s">'posterior_predictive'</span><span class="p">,</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">],</span>
                       <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">)</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">az</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">ppc_check</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s">"y</span><span class="si">{</span><span class="n">g</span><span class="si">}{</span><span class="n">t</span><span class="si">}</span><span class="s">"</span><span class="p">],</span> <span class="n">group</span><span class="o">=</span><span class="s">'posterior_predictive'</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">])</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">df_reg</span><span class="p">[(</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'g'</span><span class="p">]</span><span class="o">==</span><span class="n">g</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span><span class="o">==</span><span class="n">t</span><span class="p">)][</span><span class="s">'Y'</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s">"g=</span><span class="si">{</span><span class="n">g</span><span class="si">}</span><span class="s">, t=</span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="n">legend</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">legend</span><span class="p">(</span><span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/difference_in_difference/posterior_predictives.webp" alt="The comparison between the predicted
and observed distributions of Y" /></p>

<p>The posterior predictive distributions agree with the observed data. We extracted some random sub-sample to
provide an estimate of the uncertainties.</p>

<p>We can finally verify if there is any effect:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">trace_did</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'beta_gt'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/difference_in_difference/effect_estimate.webp" alt="Our estimate for the minimum wage increase effect
" /></p>

<p>As you can see, the effect is compatible with 0, therefore there is no evidence
that by increasing the minimum salary there is an effect on the occupation.</p>

<p>Our model has a small issue: it allows for negative values of the occupation,
which doesn’t make sense. This problem can be easily circumvented by using 
the <a href="https://www.pymc.io/projects/docs/en/v4.4.0/api/distributions/generated/pymc.Truncated.html">truncated PyMC class</a>.</p>

<p>I suggest you to try it and verify yourself if there is any effect.
Remember that in that case $\mu$ is no more the mean for $Y$,
so you can’t use it to estimate the average effect.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We have seen how to implement the DiD method with PyMC, and we used to
re-analyze the Krueger and Card article on the relation between the minimum
salary and the occupation.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/causal_intro/" /><summary type="html"><![CDATA[Causal inference from 1850]]></summary></entry><entry><title type="html">Instrumental variable regression</title><link href="http://localhost:4000/statistics/instrumental_variable" rel="alternate" type="text/html" title="Instrumental variable regression" /><published>2024-06-16T00:00:00+00:00</published><updated>2024-06-16T00:00:00+00:00</updated><id>http://localhost:4000/statistics/instrumental_variable</id><content type="html" xml:base="http://localhost:4000/statistics/instrumental_variable"><![CDATA[<p>In many circumstances you cannot randomize, either because it is unethical
or simply because it’s too expensive.
There are however methods which, if appropriately applied, may provide
you some convincing causal evidence.</p>

<p>Let us consider the case where you cannot randomly assign the treatment $T\,,$
and in this case it could be affected by any confounder $X$
leading you to a biased estimate of the treatment effect.
However, if you have a variable $Z$ that only affects $T$
and does not affect your outcome in any other way other than via $T\,,$
than you can apply <strong>Instrumental Variable Regression</strong>.</p>

<p><img src="/docs/assets/images/statistics/instrumental_variable/causal_structure.webp" alt="The assumed causal flow" /></p>

<p>Of course, the above causal assumption is quite strong, but it holds
in quite a good approximation in some circumstance.</p>

<p>This method has been applied to analyze the effect of school years ($T$)
on earning ($Y$).
In this case the variable $Z$ was the assignment of some economical assistance
(a voucher) to go to school.</p>

<p>One would be tempted to simply use linear regression to fit this model:</p>

\[Y = \alpha + \beta T + \gamma Z + \varepsilon\]

<p>However, linear regression assumes independence between the regressors,
while in our case we have that $T$ is determined by $Z\,.$
This has an impact on the variance estimate of $Y\,,$ as we do not
correctly propagate the uncertainty due to the $T$ dependence on $Z\,.$
In fact, linear regression always predicts homoscedastic variance,
while IV can also reproduce heteroscedasticity.</p>

<h2 id="application-to-the-cigarettes-sales">Application to the cigarettes sales</h2>

<p>We will use IV to see if an increase in the cigarettes price ($T$)
causes a decrease in the cigarettes sales ($Y$), and we will use the
tobacco taxes as instrumental variable $Z$.
In order to linearize the dependence between the variables,
instead of the value of each quantity, we will consider the
difference between the 1995 log value and the 1985 log value.</p>

\[\begin{pmatrix}
T \\
Y \\
\end{pmatrix}
\sim 
\mathcal{t}
\left(
\left(
\alpha_0 + \beta_0 Z
\atop
\alpha_1 + \beta_1 T
\right),
\Sigma, \nu
\right)\]

<p>where $t$ represents the 2 dimensional Student-T distribution and $\Sigma$ is the $2\times2$ covariance matrix.
If $Z$ has a causal effect on $Y$ via $T\,,$ then the correlation
between $Y$ and $T$ is different from zero.</p>

<p>We will assume</p>

\[\alpha_i, \beta_i \sim \mathcal{N}(0, 10^3)\]

<p>and</p>

\[\nu \sim \mathcal{HalfNormal}(100)\]

<p>\(\Sigma\) must be a positive semi-defined matrix, and an easy way to
provide it a prior is using the
<a href="https://en.wikipedia.org/wiki/Lewandowski-Kurowicka-Joe_distribution">Lewandowski-Kurowicka-Joe distribution
</a>.
This distribution takes a shape parameter $\eta\,,$
and we will take $\eta=1\,,$ which implies that we will take a uniform
prior over $[-1, 1]$ for the correlation matrix.
We will moreover assume that the standard deviations are distributed according to</p>

\[\sigma_i \sim \mathcal{HalfCauchy}(20)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">pytensor.tensor.extra_ops</span> <span class="kn">import</span> <span class="n">cumprod</span>
<span class="kn">import</span> <span class="nn">pymc.sampling_jax</span> <span class="k">as</span> <span class="n">pmjx</span>

<span class="n">random_seed</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">df_iv</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/AER/CigarettesSW.csv'</span><span class="p">)</span>

<span class="n">X_iv</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1995</span><span class="p">][</span><span class="s">'price'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'price'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
       <span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'price'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
<span class="n">Y_iv</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1995</span><span class="p">][</span><span class="s">'packs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'packs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
       <span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'packs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
<span class="n">Z_iv</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1995</span><span class="p">][</span><span class="s">'taxs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'taxs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
       <span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'taxs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">instrumental_variable</span><span class="p">:</span>
    <span class="n">sd_dist</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">20.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'nu'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">100.0</span><span class="p">)</span>
    <span class="n">chol</span><span class="p">,</span> <span class="n">corr</span><span class="p">,</span> <span class="n">sigmas</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">LKJCholeskyCov</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sd_dist</span><span class="o">=</span><span class="n">sd_dist</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">Z_iv</span><span class="p">,</span> <span class="n">X_iv</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">X_iv</span><span class="p">,</span> <span class="n">Y_iv</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">w</span><span class="p">)</span>  <span class="c1"># so we will recover it easily
</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">MvStudentT</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">chol</span><span class="o">=</span><span class="n">chol</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_iv</span><span class="p">)),</span> <span class="n">observed</span><span class="o">=</span><span class="n">u</span><span class="p">)</span>
    <span class="c1"># We directly compute the posterior predictive
</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">MvStudentT</span><span class="p">(</span><span class="s">'y_pred'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">chol</span><span class="o">=</span><span class="n">chol</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>

<span class="k">with</span> <span class="n">instrumental_variable</span><span class="p">:</span>
    <span class="n">trace_instrumental_variable</span> <span class="o">=</span> <span class="n">pmjx</span><span class="p">.</span><span class="n">sample_numpyro_nuts</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_instrumental_variable</span> <span class="p">,</span>

              <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">,</span> <span class="s">'beta'</span><span class="p">,</span> <span class="s">'sigma'</span><span class="p">,</span> <span class="s">'nu'</span><span class="p">],</span>
              <span class="n">coords</span><span class="o">=</span><span class="p">{</span><span class="s">'sigma_corr_dim_0'</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s">'sigma_corr_dim_1'</span><span class="p">:</span><span class="mi">1</span><span class="p">})</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/instrumental_variable/trace.webp" alt="The trace plot of the above model" /></p>

<p>As we can see, there is no signal of problems in thee trace plot.</p>

<p>A few remarks on the above code. Since the model is not very fast,
we used the numpyro sampler, which hundred of times
faster than the standard PyMC sampler.
Moreover, we instructed arviz to only plot the off-diagonal elements
of the correlation matrix. We must do this because the diagonal elements
are always one, as they must be, but this causes an error in arviz
(which assumes a random behavior in all the variables of the trace).</p>

<p>We can now verify the posterior predictive distribution.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a0</span> <span class="o">=</span> <span class="n">trace_instrumental_variable</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">])[</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span>
<span class="n">b0</span> <span class="o">=</span> <span class="n">trace_instrumental_variable</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'beta'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">])[</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span>

<span class="n">x_min</span> <span class="o">=</span> <span class="mf">0.06</span>
<span class="n">x_max</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="n">x_pl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mf">0.0002</span><span class="p">)</span>

<span class="n">xiv_0</span> <span class="o">=</span> <span class="n">trace_instrumental_variable</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">2</span><span class="p">))[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">xiv_1</span> <span class="o">=</span> <span class="n">trace_instrumental_variable</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">2</span><span class="p">))[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">sampled_index</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">sampled_index</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">a0</span><span class="o">+</span><span class="n">b0</span><span class="o">*</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">sampled_index</span><span class="p">:</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xiv_0</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">xiv_1</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'x'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_iv</span><span class="p">,</span> <span class="n">Y_iv</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'steelblue'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'t'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/instrumental_variable/posterior_predictive.webp" alt="The posterior predictive distribution" /></p>

<p>Our model also looks capable to reproduce the observed data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">az</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">trace_instrumental_variable</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">"sigma_corr"</span><span class="p">])[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">"IV Model - Posterior Distribution Correlation"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$\sigma$'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">''</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/instrumental_variable/correlation.webp" alt="The off-diagonal component of the correlation matrix" /></p>

<h2 id="conclusions">Conclusions</h2>

<p>We have seen how IV allows us to make causal inference in absence of randomization,
but making some rather strong assumptions about the causal structure of the problem.
We have also seen how to implement it in PyMC.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/causal_intro/" /><summary type="html"><![CDATA[Making causal inference without randomization]]></summary></entry><entry><title type="html">Randomized controlled trials</title><link href="http://localhost:4000/statistics/randomized" rel="alternate" type="text/html" title="Randomized controlled trials" /><published>2024-06-09T00:00:00+00:00</published><updated>2024-06-09T00:00:00+00:00</updated><id>http://localhost:4000/statistics/randomized</id><content type="html" xml:base="http://localhost:4000/statistics/randomized"><![CDATA[<p>As we anticipated in the last post, when we have randomization, association
implies causation.
In this case we can use a simple regression model to assess if the treatment
causes an effect.</p>

<p>Randomized controlled trials are the golden standards in clinical studies,
but they are widely used in other fields like industry or marketing
campaigns.
Thanks to their popularity, even marketing providers such as Mailchimp allow you
to easily implement this kind of studies, and in this post we will see how
to analyze them by using Bayesian regression.
In this experiment we we will analyze the data from a newsletter, and what we will
determine is whether the presence of the first name (which is required
in the login form) in the mail preview increases the probability of opening the
email.
When we programmed the newsletter, we divided the total audience into
two blocks, and each recipient has been randomly assigned to one block.
In the control block (t=0) we sent the email without the first name in the mail
preview, while to the other recipients we sent the email with the first name
in the mail preview.</p>

<p>Some mails were bounced, but at the end $n_t = 2326$ users received the test mail
and $n_c = 2347$ received the control mail.
$y_t = 787$ users out of 2326 opened the test email, while $y_c=681$ users out
of 2347 opened the control one.</p>

<p>Since the opening action is a binary variable, we will take
a binomial likelihood.
We will therefore use a logistic regression to estimate the ATE.</p>

\[\begin{align}
&amp;
y_{c} \sim \mathcal{Binom}(p_c, n_n)
\\
&amp;
y_{t} \sim \mathcal{Binom}(p_t, n_t)
\\
&amp;
p_c = \frac{1}{1+e^{-\beta_0}}
\\
&amp;
p_t = \frac{1}{1+e^{-(\beta_0+ \beta_1)}}
\end{align}\]

<p>We will take a non-informative prior for both the parameters</p>

\[\beta_i \sim \mathcal{N}(0, 10^3)\]

<p>We can now easily implement our model in PyMC</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">random_seed</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">n_t</span> <span class="o">=</span> <span class="mi">2326</span>
<span class="n">n_c</span> <span class="o">=</span> <span class="mi">2347</span>

<span class="n">k_t</span> <span class="o">=</span> <span class="mi">787</span>
<span class="n">k_c</span> <span class="o">=</span> <span class="mi">681</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">pt</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'pt'</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]))))</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'pc'</span><span class="p">,</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]))))</span>
    <span class="n">ate</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'ate'</span><span class="p">,</span> <span class="n">pt</span><span class="o">-</span><span class="n">pc</span><span class="p">)</span>
    <span class="n">y_t</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">'y_t'</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_t</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pt</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">k_t</span><span class="p">)</span>
    <span class="n">y_c</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">'y_c'</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_c</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pc</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">k_c</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/randomized/trace.webp" alt="The trace of our model" /></p>

<p>The average treatment effect is greater than 0 with a probability
approximately equal to 1,
therefore we are almost sure that, in our test,
using the first name in the mail preview increased the opening
probability of the newsletter.</p>

<p>Notice that we restricted our discussion to one single newsletter, and we
avoided more general claims regarding future newsletters we will send.
However, we have some indication that our audience may prefer more
personal newsletters.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We saw an example of how to perform causal inference in Bayesian statistics for randomized controlled experiments
by using regression models in PyMC. We also discussed the proper interpretation of the results.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/randomized/" /><summary type="html"><![CDATA[When association implies causation]]></summary></entry><entry><title type="html">Causal inference</title><link href="http://localhost:4000/statistics/causal_intro" rel="alternate" type="text/html" title="Causal inference" /><published>2024-06-02T00:00:00+00:00</published><updated>2024-06-02T00:00:00+00:00</updated><id>http://localhost:4000/statistics/causal_intro</id><content type="html" xml:base="http://localhost:4000/statistics/causal_intro"><![CDATA[<p>In this post we will try and clarify when it is possible to make statements
about causation rather than sticking to statistical association,
and we will do so on the basis of Rubin’s potential outcomes.</p>

<p>The main reference for this part will be the material in
<a href="https://www.bradyneal.com/Introduction_to_Causal_Inference-Dec17_2020-Neal.pdf">these</a>
notes by Brady Neal, but I strongly recommend to read the textbook by Guido Imbens
(who, in 2021, shared the Nobel prize for economics with Joshua Angrist and David Card for their works on causal inference) and Carl Rubin
(who first developed the potential outcomes framework).</p>

<h2 id="the-counterfactual-definition-of-causality">The counterfactual definition of causality</h2>

<p>You may have heard the mantra “association is not causation” or the more colloquial 
(but less accurate) “correlation is not causation”.
Correlation is a statistical measure linear dependence,
while association generally means statistical dependence.
However the exact meaning of causation is never given,
and the first part of these notes will be devoted to clarify what do we mean with causation.</p>

<p>Let us assume that we took a medicine because we had a headache,
what do we mean when we say that the medicine
caused the headache to disappear? It means that, if we hadn’t taken the medicine,
the headache wouldn’t have gone away.</p>

<p>We will therefore stuck to the counterfactual definition of causation,
so we will say that an event causes an outcome if,
by removing the event then the outcome disappears.
The above definition works for binary outcomes, but has some
problems when we want to investigate causes which can take any real value.
More generally, we can say that an event causes an outcome if, by modifying
the cause, the outcome changes.
This definition already puts a strong constrain on what we can investigate,
since it requires that we must be able, at least in principle, to modify the
cause.</p>

<div class="emphbox">
There's no causation without manipulation.
</div>

<p>While the meaning of the above sentence may seem obvious at a first sight,
you should carefully think about it when making causal inference.
If you want to assess the effect of the ethnicity on the probability of being hired,
you may not be able to manipulate someone’s ethnicity,
but you can still manipulate people’s perception of ethnicity by modifying 
the CV.</p>

<p>When talking about causality, one can be either interested in the determination
of the effect of a cause (e.g. does my headache disappears when I take medicine?)
or the cause of an effect (e.g. is my headache gone because I took the medicine?).</p>

<p>Within the counterfactual framework, and in science in general, one generally wants
to assess the effect of a specific cause.
Determining the cause of an effect, in fact, is an ill-posed question, as one could
find a cause of the cause, a cause of the cause of the cause and so on.</p>

<p>A relevant aspect which we must keep in mind is that there could be more than one
cause. We know that, in order to light a fire, we need oxygen, heat and fuel,
and all the above are necessary conditions for fire.</p>

<p>Let’s assume that we want to assess if heat causes fire ignition,
and we perform an experiment to determine it.
If we first provide both three the elements,
and we then remove oxygen and heat, we can’t conclude anything about the
causal relation between heat and fire, since we also removed the oxygen.
The counterfactual definition of causality requires that
only the cause must change, while all the other elements must be unchanged.</p>

<h2 id="potential-outcomes">Potential outcomes</h2>

<p>But how can we measure the effect of an event? Let us indicate with $T=1$
the case where the event happens, as an example we take a therapy,
while $T=0$ means that we do not take the therapy.
Suppose that the outcome of the event $T=0$ is $y_0$ while the outcome of $T=1$ is
$y_1\,.$ We define</p>

\[Y(t) = t y_1 + (1-t) y_0\,.\]

<p>From a counterfactual point of view, a natural way to assess the causal
effect is via the <strong>Individual Treatment Effect</strong> ITE</p>

\[\tau = Y(1)-Y(0)\]

<p>The definition of $\tau$ is of course arbitrary, but quite general.
As an example, one could prefer taking
the ratio of the two, but then taking the logarithm we recover the above definition.</p>

<p>Despite on the exact definition, $\tau$ of course cannot be measured, as either we take the treatment and
we observe $Y(1)$ or we don’t and we observe $Y(0)\,,$
and any reasonable definition of $\tau$ involves both the quantities.
This implies that,</p>

<div class="emphbox">
while our definition of effect may be individual,
its quantification can only be done on a larger sample.
</div>

<p>We must therefore do an experiment in order to estimate it:
we collect $2N$ individuals and divide them into 2 groups.
Half individuals are treated with $T=1$ (the treatment group)
and half of them with $T=0$ (the control group).</p>

<p>In order to proceed, we will stick to the definition $\tau = Y(1)-Y(0)\,.$
The simplest estimate that we may do is the <strong>Average Treatment Effect</strong> ATE</p>

\[ATE = \mathbb{E}[Y_i(1) - Y_i(0)] =  \mathbb{E}[Y(1) - Y(0)] = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]\]

<p>where the average is meant both on the individual and on any other possible source
of randomness.</p>

<p>To clarify what we are doing, we can put the collected data as</p>

<table>
  <thead>
    <tr>
      <th>i</th>
      <th>T</th>
      <th>Y</th>
      <th>Y(0)</th>
      <th>Y(1)</th>
      <th>X</th>
      <th>Y(1) - Y(0)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>$y^1$</td>
      <td>$y^1$</td>
      <td>?</td>
      <td>$x^1$</td>
      <td>?</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0</td>
      <td>$y^2$</td>
      <td>$y^2$</td>
      <td>?</td>
      <td>$x^2$</td>
      <td>?</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0</td>
      <td>$y^3$</td>
      <td>$y^3$</td>
      <td>?</td>
      <td>$x^3$</td>
      <td>?</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1</td>
      <td>$y^4$</td>
      <td>?</td>
      <td>$y^4$</td>
      <td>$x^4$</td>
      <td>?</td>
    </tr>
    <tr>
      <td>5</td>
      <td>1</td>
      <td>$y^5$</td>
      <td>?</td>
      <td>$y^5$</td>
      <td>$x^5$</td>
      <td>?</td>
    </tr>
    <tr>
      <td>6</td>
      <td>1</td>
      <td>$y^6$</td>
      <td>?</td>
      <td>$y^6$</td>
      <td>$x^6$</td>
      <td>?</td>
    </tr>
  </tbody>
</table>

<p>where $X$ represents other possibly relevant quantity where we must take
into account to estimate the averages or, in other words, any quantity we may suspect could
affect the outcome.
The estimate of the ATE is the so-called <strong>fundamental problem of causal inference</strong>,
and since the question marks can be seen as missing values,</p>

<div class="emphbox">
the fundamental problem of causal inference is a missing value problem. 
</div>

<p>We did a step further, but still we don’t know how to compute that quantity.</p>

<p>In writing the above table, we implicitly made the <strong>no interference</strong> assumptions, namely that</p>

\[Y_i(t_1, t_2, ..., t_{i-1}, t_i, t_{i+1}, ..., t_n) = Y_i(t_i)\]

<p>So each unit’s outcome only depends on his own treatment and not on the treatment of other individuals.
This implies that, if we are checking the effect of a product in some tomato field,
we must be sure that the product does not goes in another studied field by mistake.
Another case can be a study where we are studying an experimental study program in a class.
If a student is selected in the treatment group and a friend of his is not,
the latter could be sad for not being selected and his outcome could be lowered.</p>

<p>Generally, a good strategy to enforce this requirement is to take well separated units
and not letting them communicate during the experiment.</p>

<p>Notice that this is not a necessary requirement, but it greatly simplifies the discussion,
as it allows us to threat each unit independently on the others.</p>

<p>A quantity that is closely related to the ATE is the <strong>associational difference</strong></p>

\[\mathbb{E}[Y|T=1] - \mathbb{E}[Y|T=0]\]

<p>When are we allowed to replace the ATE with the associational difference?
In other words, when are we allowed to compute the average only over the observed 
values and replace the question marks with the appropriate average?</p>

<p>The assumptions that the observed data do not depend on the missing ones
is called <strong>ignorability</strong>, and it is one of the most important assumptions
in causal inference.
Ignorability can be written in mathematical language as</p>

\[Y(0), Y(1) \perp\!\!\!\!\perp T\]

<p>where \(A \perp\!\!\!\!\perp B\) means that $A$ and $B$ are independent one on
the other one.</p>

<p>If ignorability holds, we are allowed to estimate the average of $Y(0)$
by only using the $T=0$ group and replace it in the $T=1$ group and vice versa,
and this is why this assumption is often named <strong>exchangeability</strong>.</p>

<p>We can mathematically write the exchangeability assumption as</p>

\[\mathbb{E}[Y(0) | T=0] = \mathbb{E}[Y(0) | T=1] = \mathbb{E}[Y(0)]\]

<p>and</p>

\[\mathbb{E}[Y(1) | T=0] = \mathbb{E}[Y(1) | T=1] = \mathbb{E}[Y(1)]\]

<p>The above assumption is almost equivalent to <strong>identifiability</strong> assumption:
a causal quantity $\mathbb{E}[Y(t)]$ is identifiable if it can be computed from a pure statistical quantity $\mathbb{E}[Y | T=t]$.</p>

<p>There are cases where exchangeability does not hold.
As an example, assume that you are testing a medicine, and that this medicine
is more effective on patients with a severe version of the disease you are treating.
If in the $T=1$ group we have
people with a more severe version of the disease than in the $T=0$
group we may not be allowed to exchange the two groups,
as we have no guarantee that the result would be invariant under the group exchange.</p>

<p>Let us decompose the associational difference as</p>

\[\mathbb{E}[Y(1) | T=1] - \mathbb{E}[Y(0) | T=0]
=
(\mathbb{E}[Y(1) | T=1] - \mathbb{E}[Y(0) | T=1])
+(\mathbb{E}[Y(0) | T=1] - \mathbb{E}[Y(0) | T=0])\]

<p>The associational difference can be decomposed as
the average treatment effect on the treated (the first parenthesis)
plus the sampling bias (the second parenthesis).</p>

<p>Consider the case where $Y$ is the health status of a person and the treatment is
the hospitalization.
The associational difference is simply the difference between the health status
of hospitalized patients and the health status of non-hospitalized people.
This is simply the sum between the effect of the hospitalization on hospitalized
patients plus the baseline health difference between hospitalized and non-hospitalized individuals.
In general, even if hospitalization improves health, the health status of those who go to
the hospital is generally worst than the other individuals.
Therefore, in absence of randomization, if we simply use the associational difference to assess the effect of
hospitalization, we may end up with the conclusion that health get worst due to hospitalization
simply because only sick people goes to the hospital.</p>

<p>If exchangeability does not hold, then \(\mathbb{E}[Y \vert T=0]\) is different from
\(\mathbb{E}[Y \vert T=1]\,,\) therefore the associational quantity \(\mathbb{E}[Y \vert T=t]\) is a biased estimator for \(\mathbb{E}[Y(t)]\,.\)</p>

<p>One possible way to ensure that exchangeability holds is to ensure that the missing
terms are randomly distributed.
This can be experimentally done by randomly assigning the
treatment $T$ to each unit, and in this case we are dealing with a randomized experiment.</p>

<p>In a randomized experiment, the treatment assignment does not depend on anything other
other than the result of a coin toss, therefore</p>

\[\mathbb{E}[Y(1)]-\mathbb{E}[Y(0)] = \mathbb{E}[Y(1)|T=1]-\mathbb{E}[Y(0)|T=0] = \mathbb{E}[Y | T=1]-\mathbb{E}[Y | T=0]\]

<p>We stress that this is only a statistical property, and it doesn’t guarantee that the outcome
estimate of an experiment will be correct.</p>

<p>In other words, as explained in <a href="http://www.fsb.muohio.edu/lij14/420_paper_Rubin74.pdf">the breaktrhough 1974 Rubin paper</a>:</p>
<blockquote cite="https://hedibert.org/wp-content/uploads/2015/10/causality-meeting2.pdf">
Whether treatments are randomly assigned or not, no matter how carefully
matched the trials, and no matter how large N, a skeptical observer could always
eventually find some variable that systematically differs in the E (T=1) trials and C (T=0) trials.
<br />
Within the experiment there can be no refutation of this claim; only a logical
argument explaining that the variable cannot causally affect the dependent
variable or additional data outside the study can be used to counter it.
</blockquote>

<p>Generally exchangeability is an unrealistic assumption, as it would impossible to verify
that $X$ and $Y$ are equally distributed with respect to all the 
relevant variables except for the treatment.
A weaker assumption is that the assigned treatment only depends on some
relevant quantity $X$ while the two groups are exchangeable
with respect to any other quantity.
This condition is called <strong>conditional exchangeability</strong> or <strong>unconfoundedness</strong> and it is indicated as</p>

\[Y(0), Y(1) \perp\!\!\!\!\perp T | X\]

<p>If conditional exchangeability holds, we have that</p>

\[\begin{align}
 \mathbb{E}[Y(1)-Y(0)|X] 
 &amp; = \mathbb{E}[Y(1)|X] - \mathbb{E}[Y(0)|X] \\
 &amp; = \mathbb{E}[Y(1)| T=1, X] - \mathbb{E}[Y(0)|T=0, X] \\
 &amp; = \mathbb{E}[Y| T=1, X] - \mathbb{E}[Y|T=0, X] \\
 \end{align}\]

<p>In order to get the ATE we must simply take the expectation value over $X$</p>

\[\mathbb{E}[Y(1) - Y(0)] = \mathbb{E}_X[ \mathbb{E}[Y(1) - Y(0) | X] ] 
 = \mathbb{E}_X[ \mathbb{E}[Y |T=1, X] ] - \mathbb{E}_X[ \mathbb{E}[Y |T=0, X] ]\]

<p>And the equality between the first and the last term of this equation is called the <strong>adjustment formula</strong>.</p>

<p>In the above equation we assumed <strong>consistency</strong>, which can be written as</p>

\[T=t \Longrightarrow Y(T) = Y(t)\]

<p>This means that the treatment must be well specified: the treatment must not be “get some medicine” but should rather be “take 15 mg of medicine every 8 hours for 7 days”.
Only thanks to this hypothesis we can replace</p>

\[\mathbb{E}[Y(T=t) | T=t] = \mathbb{E}[Y | T=t] \,.\]

<p>This is not a necessary requirement, but it greatly simplifies the discussion, otherwise we would be forced to model
this random aspect too.
Notice that the concept of consistency is not a mathematical requirement, but rather a conceptual one,
and only agreement among domain experts can assess whether it holds or not.</p>

<p>In the literature it is often required the <strong>Stable Unit Treatment Value Assumption</strong> SUTVA, which is simply requiring consistency and no interference.</p>

<p>Let us now write explicitly the adjustment formula for $X$ discrete:</p>

\[\begin{align}
&amp;
\mathbb{E}_X[ \mathbb{E}[Y |T=1, X] ] - \mathbb{E}_X[ \mathbb{E}[Y |T=0, X] ]  
\\
&amp; =  \sum_{x}P(X=x) \sum_{y} y \left(P(Y=y|T=1, X=x) - P(Y=y| T=0, X=x) \right) \\
&amp; =   \sum_{x}P(X=x) \sum_{y} y \left(\frac{P(Y=y,T=1, X=x)}{P(T=1, X=x)} - \frac{P(Y=y,T=0, X=x)}{P(T=0, X=x)}\right) \\
= &amp; \sum_{x}P(X=x) \sum_{y} y \left(\frac{P(Y=y,T=1, X=x)}{P(T=1| X=x) P(X=x)} - \frac{P(Y=y,T=0, X=x)}{P(T=0| X=x) P(X=x)}\right) 
\\
&amp; = 
\sum_{x}\sum_{y} y \left(\frac{P(Y=y,T=1, X=x)}{P(T=1| X=x)} - \frac{P(Y=y,T=0, X=x)}{P(T=0| X=x)}\right) 
\end{align}\]

<p>where the first equivalence comes from the definition of conditional probability,
the second one from the hypothesis that \(P(T, X) = P(T | X) P(X)\)
so that $T$ causally depends on $X\,.$</p>

<p>In order for this quantity to be finite we must require that both the denominators are
strictly positive, and since $P(T=0|X) = 1 - P(T=1, X)$ we can write this requirement,
named the <strong>positivity</strong> assumption, as</p>

\[0 &lt; P(T=t | X) &lt; 1 \, \forall t\]

<p>In other words, for each value of X we must have some representative individual in
each group.
If, for some group, everyone receives the treatment or everyone receives the control,
we are not able to estimate the treatment versus control effect.</p>

<!--
We can better see why randomization is important from the adjustment formula.
If the treatment assignment is not random, one cannot assume that $P(X)$ is the same for the two
groups, and therefore one should replace it with $P(X=x | T=t)\,.$
A careful observer could therefore always find some confounder which could differ
between the two groups, and claim that the difference in the effect is due to the
different distribution of that confounder.
Within randomization, this cannot happen, as the probability distribution
of $X$ is independent on $T\,.$
-->

<h2 id="some-remarks">Some remarks</h2>

<p>Our discussion on causality both applies to frequentist statistics
and to bayesian one. However, as pointed out by Rubin himself in his 1990
article “Formal mode of statistical inference for causal effects”,
it is straightforward to apply fully bayesian methods to causal inference.
However, it is very easy to misuse it, as</p>

<blockquote cite="https://www.sciencedirect.com/science/article/abs/pii/0378375890900778">
there appears to be no formal requirement to make sure
that the models conform at all to reality. 
In practice, careful model monitoring is
needed, and for this purpose, the randomization-based approaches we have presented
can be regarded as providing useful guidelines.
</blockquote>

<p>It is therefore crucial both to ensure that experimental setup fulfills the
above mentioned assumptions and that the statistical model is appropriate
in describing it.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We introduced the counterfactual definition of causality, and we introduced
Rubin’s potential outcomes. We also discussed under which conditions
we can compute the average treatment effect.</p>

<h2 id="suggested-readings">Suggested readings</h2>

<ul>
  <li><cite>Imbens, G. W., Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. US: Cambridge University Press.<cite></cite></cite></li>
  <li><cite><a href="https://arxiv.org/pdf/2206.15460.pdf">Li, Ding, Mealli (2022). Bayesian Causal Inference: A Critical Review</a></cite></li>
</ul>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/causal_intro/" /><summary type="html"><![CDATA[When association implies causation]]></summary></entry><entry><title type="html">Experiment analysis</title><link href="http://localhost:4000/statistics/experiment_design" rel="alternate" type="text/html" title="Experiment analysis" /><published>2024-05-26T00:00:00+00:00</published><updated>2024-05-26T00:00:00+00:00</updated><id>http://localhost:4000/statistics/experiment_design</id><content type="html" xml:base="http://localhost:4000/statistics/experiment_design"><![CDATA[<p>Experimental design was developed by Fisher in the context of agriculture,
with the aim to better make decisions based on experimental data.
Nowadays experimental design is applied in many different fields,
but unfortunately there aren’t many textbook which treat this
topic from a Bayesian perspective.
The advantage of a Bayesian treatment is clear if you consider
that, often, collective data requires a big effort.
As an example, running an experiment in the agriculture
may take months, so it is crucial to extract as much information as possible
from every single datum.
The reason for sticking to the traditional data analysis approaches
might be due to the fact that Fisher had a very strong positions
against the Bayesian statistics, or maybe because most of the available
software is based on Fisher’s models.
It is however very easy to build Bayesian models for this kind
of application, and we will take a look at how to do so.</p>

<h2 id="principles-of-experimental-design">Principles of experimental design</h2>
<p>When designing an experiment, you should keep in mind three fundamental
principles:</p>
<ul>
  <li>randomization</li>
  <li>replication</li>
  <li>blocking</li>
</ul>

<p>When we talk about <strong>randomization</strong>, we refer to the random assignment of the
experimental units to different treatment condition.
Randomization allows you to reduce the systematic error, and any other residual
variation in the experimental procedure is random by construction.</p>

<p><strong>Replication</strong> is the repetition of the experimental procedure,
from the preparation to the measurement.
The purpose of replication is to have a reliable estimate of the variance,
and without replication your effect estimate might not be reliable.</p>

<p><strong>Blocking</strong> refers to fixing (when possible) or measuring (otherwise)
any factor which might reasonably affect the experimental outcome.
Controlling for external factors which might affect our measurement
helps us in identifying the sources of variability in the outcome,
and therefore will give us a more precise estimate of the effect.</p>

<h2 id="completely-randomized-design">Completely randomized design</h2>

<p>In a completely randomized experiment we assign the treatment to each experimental
unit completely at random.
In the simplest version of this design, we have $n \times k$ units,
and we want to randomly assign each unit to one of $n$ treatments.</p>

<p>As an example, consider the dataset at <a href="https://www.itl.nist.gov/div898/software/dataplot/data/BOXBLOOD.DAT">this link</a>.
The aim of the experiment is to determine whether different diets
had different effects on the blood coagulation time.
The example is taken from page 155 of <a href="https://pages.stat.wisc.edu/~yxu/Teaching/16%20spring%20Stat602/%5BGeorge_E._P._Box,_J._Stuart_Hunter,_William_G._Hu(BookZZ.org).pdf">this textbook</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/coagulation.csv'</span><span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'Y'</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'X1'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/experiment_design/coagulation_boxplot.webp" alt="The boxplot of the
coagulation dataset" /></p>

<p>In the dataset we have 24 units, and to each unit we want to assign
one of the four possible treatments.
This could have been done by running</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n</span><span class="o">=</span><span class="mi">4</span>
<span class="n">k</span><span class="o">=</span><span class="mi">6</span>

<span class="n">items</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">k</span><span class="p">))</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>
<span class="n">groups</span> <span class="o">=</span> <span class="p">[</span><span class="n">items</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">k</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
</code></pre></div></div>

<p>We have different way to analyze this dataset, and we will use the following
model</p>

\[\begin{align}
&amp;
y_{ij} \sim \mathcal{N}(\mu_i, \sigma)
\\
&amp;
\sigma \sim \mathcal{HN}(0, 100)
\\
&amp;
\mu_i \sim \mathcal{N}(0, 100)
\end{align}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">coag_model</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">'X1'</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'Y'</span><span class="p">])</span>
    <span class="n">idata</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/experiment_design/coagulation_trace.webp" alt="The trace of the
coagulation model" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'mu'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/experiment_design/coagulation_forest.webp" alt="The forest plot of the
means for the coagulation model" /></p>

<p>There is no doubt that, in this study, we observe that a diet variation
corresponds to a coagulation time variation.</p>

<h2 id="randomized-block-design">Randomized block design</h2>
<p>In a randomized block design, we furthermore block one or more
factor (confounder) which may be a source of variability for
the outcome.
As an example, let us consider the example in chapter 4.2 of the same book.
In this experiment the authors compared the penicillin yield
of different processes (formulas).
The yield does not only depend on the process, but also on the
ingredients, so the authors used one of the main ingredients,
the corn steep liquor, as blocking factor.
The corresponding dataset can be found <a href="https://www.itl.nist.gov/div898/software/dataplot/data/BOXPENIC.DAT">here</a>.</p>

<p>In order to analyze this dataset, we will use the following model</p>

\[\begin{align}
&amp;
y_{ijk} \sim \mathcal{N}(\mu_i + \tau_j, \sigma)
\\
&amp;
\sigma \sim \mathcal{HN}(0, 100)
\\
&amp;
\mu_i \sim \mathcal{N}(0, 100)
\\
&amp;
\tau_j \sim \mathcal{N}(0, \rho)
\\
&amp;
\rho \sim \mathcal{HN}(0, 100)
\end{align}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_rbd</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/penicillin.csv'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">crb_model</span><span class="p">:</span>
    
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s">'rho'</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'tau'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">rho</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">[</span><span class="n">df_rbd</span><span class="p">[</span><span class="s">'T'</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">tau</span><span class="p">[</span><span class="n">df_rbd</span><span class="p">[</span><span class="s">'X'</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_rbd</span><span class="p">[</span><span class="s">'Y'</span><span class="p">])</span>
    <span class="n">idata_rbd</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_rbd</span><span class="p">)</span>
</code></pre></div></div>

<p>As we can clearly see, the ‘X’ effect has average zero, therefore
$\mu$ correctly estimates the average treatment effect.</p>

<p><img src="/docs/assets/images/statistics/experiment_design/penicillin_trace.webp" alt="The trace plot of the
means for the penicillin model" /></p>

<p>We can now take a better look at the values of $\mu$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">idata_rbd</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'mu'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/experiment_design/penicillin_forest.webp" alt="The forest plot of the
means for the penicillin model" /></p>

<p>As you can see, the treatment 2 gives slightly higher yields with respect to the
other treatment.</p>

<h2 id="matched-pairs-design">Matched pairs design</h2>

<p>The matched pairs design can be considered a special case of the randomized block design.
Rather than randomly assigning each unit to one of two groups, we first pair
units with similar relevant features, and then we toss a coin to decide which element
of the pair belongs to which group.</p>

<p>This kind of pairing can be useful when we have small samples or if we have very similar
pairs of units, such as twins.</p>

<h2 id="latin-square-design">Latin square design</h2>

<p>In the randomized block design, one can only control for one factor, but it may also be the case
that you need to control for more than one factor.
The latin square design is useful when you need to control for two factors.
This design can be visualized by drawing an $n\times n$ table, where each row corresponds
to the level of one factor, the other level is represented by the column, and each matrix element
is represented by a number $1,…,n$ or by a (latin) letter.
In a latin square, no letter can appear twice in any row or column.</p>

<p>All the possible $2\times 2$ latin squares are</p>

\[\begin{pmatrix}
1 &amp; 2 \\
2 &amp; 1 \\
\end{pmatrix},
\begin{pmatrix}
2 &amp; 1 \\
1 &amp; 2 \\
\end{pmatrix}\]

<p>while a possible $3\times 3$ latin square is</p>

\[\begin{pmatrix}
1 &amp; 2 &amp; 3 \\
2 &amp; 3 &amp; 1 \\
3 &amp; 1 &amp; 2 \\
\end{pmatrix}\]

<h2 id="conclusions">Conclusions</h2>

<p>We have discussed how to adapt some classical model used in experimental
design to make them Bayesian, and we have done so by using PyMC.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/experiment_analysis/" /><summary type="html"><![CDATA[How to design and analyze experiment]]></summary></entry><entry><title type="html">Introduction to Extreme Values theory</title><link href="http://localhost:4000/statistics/extreme_intro" rel="alternate" type="text/html" title="Introduction to Extreme Values theory" /><published>2024-05-19T00:00:00+00:00</published><updated>2024-05-19T00:00:00+00:00</updated><id>http://localhost:4000/statistics/extreme_intro</id><content type="html" xml:base="http://localhost:4000/statistics/extreme_intro"><![CDATA[<p>In some circumstances you may be not interested in modelling the distribution itself,
 but you may be interested in understanding its asymptotic behavior, and the extreme value theory is the discipline which studies this topic.</p>

<p>The EV theory is appropriate when you want to investigate the distribution
of the minimum or maximum value of some quantity,
as the maximum loss of a financial asset or the yearly maximum
volume of rain in a certain location.</p>

<p>The intuition behind the extreme value theory is that any probability distribution
function is positive and must integrate to one,
it must therefore fall to zero fast enough if $x \rightarrow \infty\,.$
This puts strong constraints to its asymptotic behavior,
and this leads to the <a href="https://en.wikipedia.org/wiki/Fisher%E2%80%93Tippett%E2%80%93Gnedenko_theorem">Fisher-Tippet-Gnedenko theorem</a>.</p>

<p>Formally if we have a continuous positive random variable $X$
with cumulative distribution function $F(x)\,,$
and we observe $X_1,…,X_n$ independent identically distributed
variables distributed according to $X\,,$
if we denote $M_n$ the maximum of $X_1,…,X_n\,,$ then</p>

<p>$P(M_n \leq x) = P(X_1 \leq x) P(X_2 \leq x) … P(X_n \leq x) = (F(x))^n$</p>

<p>However one may not know $F$ a priori, but the FTG theorem states that,
if there exist $a_n, b_n \in \mathbb{R}$ such that</p>

\[P\left( \frac{M_n - a_n}{b_n} \leq x \right) \rightarrow G(x)\]

<p>then \(G(x) \propto \exp{\left(-(1+ \xi x)^{-1/\xi}\right)}\,.\)</p>

<p>Once properly normalized and promoted to a location-scale family one arrives to the Generalized Extreme Value distribution:</p>

\[p(x) = \frac{1}{\sigma} t(x)^{\xi + 1}e^{- t(x)}\]

<p>where</p>

\[t(x) =
\begin{cases}
\left(1+ \xi \left(\frac{x-\mu}{\sigma}\right)\right)^{-1/\xi}\,&amp; if\,&amp;  \xi \neq 0 \\
e^{-\left(x-\mu\right)/\sigma}\,&amp; if\,&amp; \xi = 0\\
\end{cases}\]

<p>Notice that, if $X_1,…, X_n$ are distributed according to $G\,,$ then $\max(X_1,…,X_n)$ is distributed according to $G\,.$
This distribution is known as the <strong>Generalized Extreme Value</strong> (GEV) distribution.</p>

<h2 id="maximum-distribution-of-the-apple-stocks">Maximum distribution of the Apple stocks</h2>

<p>I have been working on financial risk assessment for a while, and
one of the central issues in this field is to determine the
risk due to extremely large fluctuations of the stock market.
EVT can be really helpful in this task, and we will show how in this post.
We will use <a href="https://pypi.org/project/yfinance/">Yahoo Finance</a> to download the values of the Apple stock
in the period from the January 1st 2020 to the December 31st 2023.</p>

<p>The Generalized Extreme Values distribution is not directly available
in PyMC, but can be found in the <a href="https://www.pymc.io/projects/experimental/en/latest/index.html">pymc_experimental</a> package.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">pymc_experimental.distributions</span> <span class="k">as</span> <span class="n">pmx</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">yfinance</span> <span class="k">as</span> <span class="n">yf</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">yf</span><span class="p">.</span><span class="n">download</span><span class="p">(</span><span class="s">'AAPL'</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="s">'2020-01-01'</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s">'2023-12-01'</span><span class="p">).</span><span class="n">reset_index</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s">'Date'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'Date'</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="s">'LogRet'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'Close'</span><span class="p">]).</span><span class="n">diff</span><span class="p">()</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">dropna</span><span class="p">()</span>
</code></pre></div></div>

<p>First of all, we converted the close values (the value of the stock at the end of
the day) into logarithmic-returns (log-returns for short).
This is a common operation in finance, since for compound interest
assets the total value is</p>

\[\prod_i (1+r_i)\]

<p>If we take the logarithm of the above formula we transform the product into a sum,
and this makes log-returns so useful.</p>

<p>We are interested in finding the distribution of the weekly minima
of the daily close.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">groupby</span><span class="p">([</span><span class="n">pd</span><span class="p">.</span><span class="n">Grouper</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s">'Date'</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s">'W'</span><span class="p">)])[</span><span class="s">'LogRet'</span><span class="p">].</span><span class="nb">min</span><span class="p">().</span><span class="n">reset_index</span><span class="p">()</span>
<span class="n">dt</span> <span class="o">=</span> <span class="o">-</span><span class="n">data</span><span class="p">[</span><span class="s">'LogRet'</span><span class="p">].</span><span class="n">values</span>
</code></pre></div></div>

<p>Before fitting the model, let us take a look at the behavior of the data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'Date'</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s">'LogRet'</span><span class="p">])</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'LogRet'</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/extreme_intro/logret.webp" alt="" /></p>

<p>There is some evident time dependence. As an example, we can observe quite
a high volatility during the Covid pandemic and another high volatility
period after the Ukraine invasion.
However, for the moment, we will neglect the time dependence, and assume that
the parameters are stationary.</p>

<p>Since we have quite a large amount of data, we can safely use uninformative priors.
We do expect that both $\mu$ and $\sigma$ are typically much smaller than
one, so we will take a standard deviation of 2 for the first one and
equal to 1 for the latter.</p>

<p>From the <a href="https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution">Wikipedia page</a> we observe that, if 
\(| \xi|&gt;1\,,\) the mean does not exists.
Since it is reasonable to assume that it exists, we expect that $\xi$
will be bounded into the $[-1, 1]$ region, therefore we have the following
model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_gev</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">xi</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'xi'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">gev</span> <span class="o">=</span> <span class="n">pmx</span><span class="p">.</span><span class="n">GenExtreme</span><span class="p">(</span><span class="s">'gev'</span><span class="p">,</span><span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="n">xi</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/extreme_intro/trace.webp" alt="The trace of our model" /></p>

<p>Let us take a look at the joint posterior distribution.</p>

<p><img src="/docs/assets/images/statistics/extreme_intro/kde.webp" alt="The KDE plot of the posterior distribution" /></p>

<p>We can now take a look at the PPC in order to verify if our model
is able to reproduce the data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model_gev</span><span class="p">:</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">ppc</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s">'kde'</span><span class="p">,</span> <span class="n">num_pp_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/extreme_intro/ppc.webp" alt="The posterior predictive distribution" /></p>

<p>There’s a very good agreement between the observed and the predicted values,
so our estimate should be quite reliable.</p>

<h2 id="the-generalized-pareto-distribution">The Generalized Pareto distribution</h2>

<p>Keeping only the extreme values may be a waste of information. As an example, we only kept the
weekly maxima, so we trowed away four days out of five.
In some situation, instead of analyzing what is the distribution probability for the maxima,
it may be better to analyze what is the probability that your random variable exceeds some threshold.
More precisely, given $u,y&gt;0\,,$ we want to get information on</p>

\[P(X&gt;u+y | X&gt;u) = \frac{P((X&gt;u+y)\cap (X&gt;u))}{P(X&gt;u)} = \frac{P(X&gt;u+y)}{P(X&gt;u)} = \frac{1-F(u+y)}{1-F(u)}\]

<p>It can be proved (see Coles’ textbook for the outline) that, for large enough $u\,,$
the above distribution must have the form</p>

\[p(y | u, \sigma, \xi) = \left(1+\frac{\xi y}{\sigma}\right)^{-1/\xi}\]

<p>The distribution</p>

\[p(y | \mu, \sigma, \xi) = \left(1+\xi \frac{y-\mu}{\sigma}\right)^{-1/\xi}\]

<p>is named the <strong>Generalized Pareto Distribution</strong> (GPD).
For the mathematical details on the above distribution, see the
<a href="https://en.wikipedia.org/wiki/Generalized_Pareto_distribution">corresponding Wikipedia page</a>.</p>

<p>Now it comes one bad news and one good news. The bad one is that in PyMC it is only
implemented the Pareto type I distribution, which is a special case of the GPD.
The good one is that it is really easy to implement custom distributions in PyMC,
and this can be done following <a href="https://www.pymc.io/projects/examples/en/2022.12.0/howto/custom_distribution.html">this very nice tutorial</a>.
You can find my own implementation <a href="https://github.com/thestippe/thestippe.github.io/blob/main/scripts/generalized_pareto.py">on my github repo</a>.</p>

<p>Let us see how to model the tail of the Apple stocks by using it.
A reasonably high enough threshold for the log returns is $0.03\,,$
as this value is high enough to be far from the center and low enough to provide
a discrete amount of data.
We do expect $\sigma \ll 1\,,$ therefore assuming a variance of 1 for it may be enough.
$\xi$ must be lower than 1. If it is 1, then the mean
does not exists, and this doesn’t make much sense. 
If $\xi$ is negative, then the support of the GDP has an upper bound,
and it doesn’t make much sense too, so we can assume it is non-negative.
We can therefore take a half normal distribution for it, with variance 10.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">generalized_pareto</span> <span class="kn">import</span> <span class="n">GPD</span>

<span class="n">dt1</span> <span class="o">=</span> <span class="o">-</span><span class="n">df</span><span class="p">[</span><span class="o">-</span><span class="n">df</span><span class="p">[</span><span class="s">'LogRet'</span><span class="p">]</span><span class="o">&gt;</span><span class="n">thr</span><span class="p">][</span><span class="s">'LogRet'</span><span class="p">].</span><span class="n">values</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">pareto_model</span><span class="p">:</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span><span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">xi</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'xi'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">GPD</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">thr</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="n">xi</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">dt1</span><span class="p">)</span>
    <span class="n">trace_pareto</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_pareto</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/extreme_intro/trace_pareto.webp" alt="The trace of the Pareto model" /></p>

<p>Notice that in our model we fixed $\mu$ to the threshold, which is fixed.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pareto_model</span><span class="p">:</span>
    <span class="n">ppc_pareto</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_pareto</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">ppc</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="n">thr</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/extreme_intro/ppc_pareto.webp" alt="The posterior predictive of the Pareto model" /></p>

<p>In the last figure, the mean has been removed as Arviz has some issues in computing
the mean for this posterior predictive, probably because of the heavy tails or
due to the discontinuity at the threshold.
Regardless from this, the agreement between the posterior predictive and the
data looks perfect.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We introduced the Extreme Value theory, and we first applied it by
fitting the weekly minima of the Apple stocks by using the GEV distribution.
We then showed how to fit the data above a fixed threshold by using the generalized Pareto
distribution.</p>

<h2 id="suggested-readings">Suggested readings</h2>

<ul>
  <li><cite>Haan, L. d., Ferreira, A. (2006). Extreme Value Theory: An Introduction. UK: Springer New York.</cite></li>
  <li><cite>Coles, S. (2001). An Introduction to Statistical Modeling of Extreme Values. Germany: Springer London.</cite></li>
</ul>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/extreme_values_intro/" /><summary type="html"><![CDATA[Describing rare events]]></summary></entry><entry><title type="html">Application of survival analysis 1</title><link href="http://localhost:4000/statistics/survival_example" rel="alternate" type="text/html" title="Application of survival analysis 1" /><published>2024-05-12T00:00:00+00:00</published><updated>2024-05-12T00:00:00+00:00</updated><id>http://localhost:4000/statistics/survival_example</id><content type="html" xml:base="http://localhost:4000/statistics/survival_example"><![CDATA[<p>In the previous post we introduced survival
analysis and we discussed how to correctly treat
censorship.
In this post we will see an application of survival analysis.</p>

<h2 id="the-study">The study</h2>
<p>In this post we will use the “E1684” melanoma dataset available in the SurvSet python package.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">SurvSet.data</span> <span class="kn">import</span> <span class="n">SurvLoader</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">()</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">SurvLoader</span><span class="p">()</span>

<span class="n">df_melanoma</span><span class="p">,</span> <span class="n">ref_melanoma</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">ds_name</span> <span class="o">=</span> <span class="s">'e1684'</span><span class="p">).</span><span class="n">values</span><span class="p">()</span>

<span class="c1"># Dataset reference
</span>
<span class="n">ref_melanoma</span>
</code></pre></div></div>

<div class="code">
<a href="https://www.rdocumentation.org/packages/smcure/versions/2.0/topics/e1684">https://www.rdocumentation.org/packages/smcure/versions/2.0/topics/e1684</a>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_melanoma</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">pid</th>
      <th style="text-align: right">event</th>
      <th style="text-align: right">time</th>
      <th style="text-align: right">num_age</th>
      <th style="text-align: left">fac_sex</th>
      <th style="text-align: left">fac_trt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1.15068</td>
      <td style="text-align: right">-11.0359</td>
      <td style="text-align: left">M</td>
      <td style="text-align: left">IFN</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">0.62466</td>
      <td style="text-align: right">-5.12904</td>
      <td style="text-align: left">M</td>
      <td style="text-align: left">IFN</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">1.89863</td>
      <td style="text-align: right">23.186</td>
      <td style="text-align: left">F</td>
      <td style="text-align: left">Control</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">3</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">0.45479</td>
      <td style="text-align: right">11.1449</td>
      <td style="text-align: left">F</td>
      <td style="text-align: left">Control</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">4</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2.09041</td>
      <td style="text-align: right">-13.3208</td>
      <td style="text-align: left">M</td>
      <td style="text-align: left">Control</td>
    </tr>
  </tbody>
</table>

<p>There variable “time” is our regression variable, the “event” column indicates if
the event happened, and its value is 1 if the event happened (the patient died)
while it is 0 if the event is censored.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">len</span><span class="p">(</span><span class="n">df_melanoma</span><span class="p">)</span>
</code></pre></div></div>

<div class="code">
284
</div>

<p>Let us count how many entries are censored</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">df_melanoma</span><span class="p">[</span><span class="s">'event'</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
</code></pre></div></div>

<div class="code">
88
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_melanoma</span><span class="p">[</span><span class="s">'time'</span><span class="p">].</span><span class="nb">max</span><span class="p">()</span>
</code></pre></div></div>
<div class="code">
9.64384
</div>

<h2 id="the-model">The model</h2>

<p>We will use a Weibull likelihood, which is a quite flexible distribution,
which allows for fat tails, and it should thereby be more robust than
a Gamma distribution.</p>

\[Y \sim \mathcal{Weibull}(\sigma, \lambda)\]

<p>The Weibull distribution has pdf</p>

\[p(x | \alpha, \beta) = \alpha \frac{x^{\alpha-1}}{\beta^\alpha} e^{-(x/\beta)^\alpha}\]

<p>Both the parameters must be positive, and the mean of the distribution is</p>

\[\mu = \beta \Gamma\left(1+\alpha^{-1}\right)\]

<p>We want to assess the effectiveness of the treatment. The first possibility
is to fit two different models, one per treatment. Another very common
possibility is to use the treatment as a regression variable, and we will
use this method.
We define the covariate</p>

\[x =
\begin{cases}
1 &amp; treatment=IFN\\
0 &amp; treatment=Control\\
\end{cases}\]

<p>and we assume</p>

\[\begin{align}
&amp;
\lambda = \exp\left(\beta_0 + \beta_1 x \right)
\\
&amp;
\sigma = \exp\left(\alpha \right)
\\
&amp;
\alpha, \beta_i \sim \mathcal{N}(0, 100)
\\
\end{align}\]

<p>In this way we ensure that both the parameters are positive,
and the priors are very uninformative.
Let us introduce the censoring variable</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_melanoma</span><span class="p">[</span><span class="s">'censoring'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span> <span class="k">if</span> <span class="n">x</span><span class="o">==</span><span class="mi">1</span> <span class="k">else</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">df_melanoma</span><span class="p">[</span><span class="s">'event'</span><span class="p">],</span> <span class="n">df_melanoma</span><span class="p">[</span><span class="s">'time'</span><span class="p">])]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">lam</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">df_melanoma</span><span class="p">[</span><span class="s">'trt'</span><span class="p">])</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Weibull</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">alpha</span><span class="p">),</span> <span class="n">beta</span><span class="o">=</span><span class="n">lam</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Censored</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">dist</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="n">df_melanoma</span><span class="p">[</span><span class="s">'censoring'</span><span class="p">],</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_melanoma</span><span class="p">[</span><span class="s">'time'</span><span class="p">])</span>
    
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/survival_melanoma/trace.webp" alt="The trace of our model" /></p>

<h2 id="treatment-comparison">Treatment comparison</h2>

<p>By the above figure we observe that $\beta_1&gt;0\,,$
and this indicates that the test treatment is more effective than the control one.
This becomes clearer by showing the distribution of the mean $\mu$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">mu0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'beta'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">gammaln</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))))</span>
<span class="n">mu1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'beta'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'beta'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">gammaln</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">mu0</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Control'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">mu1</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'IFN'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">15</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">legend</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/survival_melanoma/mean.webp" alt="The posterior for the parameter mu" /></p>

<p>The mean for the test treatment is typically higher for the test group
than for the control group, and the peak of the mean for the IFN
treatment is roughly twice than the one for the control treatment.</p>

<p>Let us also take a look at the survival function, which is simply</p>

\[S(t, \alpha, \beta) = e^{-(t/\beta)^\alpha}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">S</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span><span class="o">/</span><span class="n">beta</span><span class="p">)</span><span class="o">**</span><span class="n">alpha</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="p">)</span>

<span class="n">t_pl</span> <span class="o">=</span>  <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="n">s0</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">S</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">alph</span><span class="p">,</span> <span class="n">b0</span><span class="p">))</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_pl</span><span class="p">]</span>

<span class="n">alph</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">b0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'beta'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'beta'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'beta'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">s0</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">S</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">alph</span><span class="p">,</span> <span class="n">b0</span><span class="p">))</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_pl</span><span class="p">]</span>
<span class="n">s0_low</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">S</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">alph</span><span class="p">,</span> <span class="n">b0</span><span class="p">),</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_pl</span><span class="p">]</span>
<span class="n">s0_high</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">S</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">alph</span><span class="p">,</span> <span class="n">b0</span><span class="p">),</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_pl</span><span class="p">]</span>

<span class="n">s1</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">S</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">alph</span><span class="p">,</span> <span class="n">b1</span><span class="p">))</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_pl</span><span class="p">]</span>
<span class="n">s1_low</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">S</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">alph</span><span class="p">,</span> <span class="n">b1</span><span class="p">),</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_pl</span><span class="p">]</span>
<span class="n">s1_high</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">S</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">alph</span><span class="p">,</span> <span class="n">b1</span><span class="p">),</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_pl</span><span class="p">]</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_pl</span><span class="p">,</span> <span class="n">s0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Control'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">t_pl</span><span class="p">,</span> <span class="n">s0_low</span><span class="p">,</span> <span class="n">s0_high</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_pl</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'IFN'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">t_pl</span><span class="p">,</span> <span class="n">s1_low</span><span class="p">,</span> <span class="n">s1_high</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'green'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">t_pl</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s">'S(t)'</span><span class="p">)</span>
<span class="n">legend</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/survival_melanoma/survival.webp" alt="The survival functions" /></p>

<p>We can safely conclude that, for the patients in this study, the IFN
treatment gives better results than the control one.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We discussed an application of survival analysis with continuous time, we explained how
to include the regressor dependence in bayesian survival analysis
and we also introduced the Weibull distribution.</p>

<h2 id="suggested-readings">Suggested readings</h2>

<ul>
  <li><cite>Ibrahim, J. G., Chen, M., Sinha, D. (2013). Bayesian Survival Analysis. Switzerland: Springer New York.</cite></li>
</ul>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/survival_continuous/" /><summary type="html"><![CDATA[Survival analysis with continouous time]]></summary></entry><entry><title type="html">Introduction to survival analysis</title><link href="http://localhost:4000/statistics/survival_analysis" rel="alternate" type="text/html" title="Introduction to survival analysis" /><published>2024-05-05T00:00:00+00:00</published><updated>2024-05-05T00:00:00+00:00</updated><id>http://localhost:4000/statistics/survival_analysis</id><content type="html" xml:base="http://localhost:4000/statistics/survival_analysis"><![CDATA[<p>There are situations where your task is to estimate the waiting time before
a certain event happens, and survival analysis is the branch of statistics
which deals with this kind of study.</p>

<p>In general, time can be either considered a continuous variable or a discrete
one. For the moment we will assume that it’s a continuous one.</p>

<p>Since we are dealing with a waiting time, our variable must be non-negative.</p>

<p>We will focus for now on parametric models, although non-parametric models
are very popular in survival analysis.</p>

<p>The analyzed event can be either the time before a component fails
or the occurrence of some biological change like the infection of one patient
or even the next eruption of a volcano.</p>

<h2 id="mathematical-background">Mathematical background</h2>

<p>Let us consider a random variable $T$ with pdf $p$ and cumulative $F\,,$ we define the <strong>survival function</strong> $S$ as:</p>

\[F(t) = P(t\leq T) = \int_0^t p(u) du = 1-S(t)\]

<p>We assume that at $t=0$ the event is not happened, so $S(0)=1$ while we
assume that we are certain that the event must occur, so $\lim_{t\rightarrow \infty} S(t)=0\,.$</p>

<p>We may alternatively assume that the event does not happen with probability $p_0\,,$
and in this case we may modify the above assumption with 
$\lim_{t\rightarrow \infty} S(t)=p_0\,.$</p>

<p>We define the <strong>hazard function</strong> as</p>

\[h(t) = \lim_{\Delta t \rightarrow 0} \frac{P(t&lt; T \leq t+\Delta t | T&gt;t)}{\Delta t} = \lim_{\Delta t \rightarrow 0} \frac{P((t&lt; T \leq t+\Delta t) \cap T&gt;t)}{P(T&gt;t)\Delta t} = \lim_{\Delta t \rightarrow 0} \frac{P(t &lt; T \leq t+\Delta t)  }{\Delta t} \frac{1}{P(T&gt;t)}  =
\frac{1}{S(t)}\lim_{\Delta t \rightarrow 0} \frac{F(t+\Delta t) - F(t)}{\Delta t }= \frac{F'(t)}{S(t)}  = \frac{f(t)}{S(t)}\]

<p>Since $h$ is the ratio of two positive quantities, it is positive itself.</p>

<p>We have that</p>

\[h(t) =  \frac{f(t)}{S(t)} = -\frac{S'(t)}{S(t)} = -\frac{d}{dt}\log S(t)\]

<p>which can be inverted by first integrating and then exponentiating:</p>

\[S(t) = \exp\left(-\int_0^t h(u) du\right)\]

<p>Notice that, if we assume that $\lim_{t\rightarrow \infty} S(t)=0\,,$ we must require that $\lim_{t\rightarrow \infty}\int_0^t h(u) du = \infty\,.$
If we otherwise assume that $\lim_{t\rightarrow \infty} S(t)=p_0\,,$ we must require that $\lim_{t\rightarrow \infty}\int_0^t h(u) du = -\log\left(p_0\right)\,.$</p>

<p>We define the <strong>cumulative hazard function</strong> as</p>

\[H(t) = \int_0^t h(u) du\]

<p>And it is related to the survival function by</p>

\[S(t) = \exp\left(-H(t)\right)\]

<h2 id="censoring">Censoring</h2>

<p>One of the main issues of survival analysis is that we are only able
to observe our system for a finite amount of time $c$, and in this
period the event may or may not occur.</p>

<p>Let us assume that we performed a study with duration $c\,,$ if we do not observe the event within the end of the study we cannot conclude that the event did not happened,
we can only conclude that it did not happened within time $c$. We assume that the event must happen at some time.
We introduce the outcome variable $y$ as \(y = \min(t, c)\) and define the
<strong>censoring status</strong> variable $\delta$ which indicates if the event was observed or not</p>

\[\delta =
\begin{cases}
1\,\,\,  if \,\,\,  t &lt; c \\
0 \,\,\,  if \,\,\,  t \geq c
\end{cases}\]

<p>and if it is not observed we say that it is <strong>censored</strong>.
If the event is not censored then its contribution to the likelihood is, as usual, $f(t)\,,$ but if we do not observe
the event within time $c$ then all we can conclude is that
the event happened after time $c\,,$ and the probability for
this is $S(c)\,.$
Thus the likelihood can be written as</p>

\[L = f(y)^\delta S(y)^{1-\delta} = (h(y) S(y))^\delta S(y)^{1-\delta}= h(y)^\delta S(y)\]

<p>and this quantity is sometimes defined as the <strong>generalized likelihood</strong>.</p>

<h2 id="wrong-methods-for-accounting-of-censoring">Wrong methods for accounting of censoring</h2>

<p>If you are new to survival analysis and you don’t know how to correctly
include censoring in your model, you may end up with a biased estimate of the 
waiting time.</p>

<p>Let us see why a naive handling of the unobserved data may end up with a wrong
estimate of the parameters.</p>

<p>Let us generate 100 fake observations, distributed according
to</p>

\[Y \sim \mathcal{Exp}(1)\]

<p>Let us also assume that our study started at $t=0$ and ended at $t=c=1.5\,.$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">123321</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="n">exponential</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">w_cp</span> <span class="o">=</span> <span class="n">w</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>

<span class="n">c</span> <span class="o">=</span> <span class="mf">1.5</span>

<span class="n">censoring</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span><span class="o">&gt;</span><span class="n">c</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

</code></pre></div></div>

<h3 id="naive-method-1-putting-a-threshold">Naive method 1: putting a threshold</h3>

<p>A first attempt could be to replace the unobserved event with the
censoring time.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">w_cp</span><span class="p">[</span><span class="n">w_cp</span><span class="o">&gt;</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">uncensored_model</span><span class="p">:</span>
    <span class="n">lam</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'lam'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="n">lam</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">w_cp</span><span class="p">)</span>
    <span class="n">trace_uncensored</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_uncensored</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/survival_intro/trace_uncensored.webp" alt="The trace of the truncated model" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace_uncensored</span><span class="p">)</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right">mean</th>
      <th style="text-align: right">sd</th>
      <th style="text-align: right">hdi_3%</th>
      <th style="text-align: right">hdi_97%</th>
      <th style="text-align: right">mcse_mean</th>
      <th style="text-align: right">mcse_sd</th>
      <th style="text-align: right">ess_bulk</th>
      <th style="text-align: right">ess_tail</th>
      <th style="text-align: right">r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">lam</td>
      <td style="text-align: right">1.328</td>
      <td style="text-align: right">0.133</td>
      <td style="text-align: right">1.083</td>
      <td style="text-align: right">1.575</td>
      <td style="text-align: right">0.001</td>
      <td style="text-align: right">0.001</td>
      <td style="text-align: right">9164</td>
      <td style="text-align: right">12834</td>
      <td style="text-align: right">1</td>
    </tr>
  </tbody>
</table>

<p>From the above summary we can observe that the $94\%$
HDI for this model does not contain the true value for the parameter.</p>

<h3 id="naive-method-2-dropping-the-unobserved-units">Naive method 2: dropping the unobserved units</h3>

<p>Another wrong method to deal with censoring is to 
only include in our dataset units which has an observation,
while excluding the remaining.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w_1</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="n">w</span><span class="o">&lt;</span><span class="n">c</span><span class="p">]</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">dropped_model</span><span class="p">:</span>
    <span class="n">lam</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'lam'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="n">lam</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">w_1</span><span class="p">)</span>
    <span class="n">trace_dropped</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_dropped</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/survival_intro/trace_dropped.webp" alt="The trace of the dropped model" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>az.summary(trace_dropped)
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right">mean</th>
      <th style="text-align: right">sd</th>
      <th style="text-align: right">hdi_3%</th>
      <th style="text-align: right">hdi_97%</th>
      <th style="text-align: right">mcse_mean</th>
      <th style="text-align: right">mcse_sd</th>
      <th style="text-align: right">ess_bulk</th>
      <th style="text-align: right">ess_tail</th>
      <th style="text-align: right">r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">lam</td>
      <td style="text-align: right">1.73</td>
      <td style="text-align: right">0.19</td>
      <td style="text-align: right">1.368</td>
      <td style="text-align: right">2.082</td>
      <td style="text-align: right">0.002</td>
      <td style="text-align: right">0.001</td>
      <td style="text-align: right">8492</td>
      <td style="text-align: right">14744</td>
      <td style="text-align: right">1</td>
    </tr>
  </tbody>
</table>

<p>This estimate is even worst than the above one.</p>

<h2 id="correct-method">Correct method</h2>

<p>Let us now show that a correct inclusion of the censoring
into the model gives a better estimate of the average lifetime.
In PyMC, censoring can be simply implemented by using the <a href="https://www.pymc.io/projects/docs/en/latest/api/distributions/censored.html">Censored class</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">censored_model</span><span class="p">:</span>
    <span class="n">lam</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'lam'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="n">lam</span><span class="o">=</span><span class="n">lam</span><span class="p">)</span>
    <span class="n">y_censored</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Censored</span><span class="p">(</span><span class="s">'y_censored'</span><span class="p">,</span> <span class="n">dist</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">w_cp</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="n">trace_censored</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_censored</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/survival_intro/trace_censored.webp" alt="The trace of the censored model" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace_censored</span><span class="p">)</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right">mean</th>
      <th style="text-align: right">sd</th>
      <th style="text-align: right">hdi_3%</th>
      <th style="text-align: right">hdi_97%</th>
      <th style="text-align: right">mcse_mean</th>
      <th style="text-align: right">mcse_sd</th>
      <th style="text-align: right">ess_bulk</th>
      <th style="text-align: right">ess_tail</th>
      <th style="text-align: right">r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">lam</td>
      <td style="text-align: right">1.08</td>
      <td style="text-align: right">0.12</td>
      <td style="text-align: right">0.863</td>
      <td style="text-align: right">1.314</td>
      <td style="text-align: right">0.001</td>
      <td style="text-align: right">0.001</td>
      <td style="text-align: right">9889</td>
      <td style="text-align: right">14108</td>
      <td style="text-align: right">1</td>
    </tr>
  </tbody>
</table>

<p>We now have that our estimate is correct within one standard deviation, and
this is a huge improvement with respect to both the naive methods.</p>

<h2 id="comparison-of-the-results">Comparison of the results</h2>

<p>In order to better understand the difference in the estimate, let us now sample
and plot the posterior predictive distributions of the three models.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">censored_model</span><span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'y_pred'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="n">lam</span><span class="p">)</span>
    <span class="n">ppc_censored</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_censored</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">])</span>

<span class="k">with</span> <span class="n">uncensored_model</span><span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'y_pred'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="n">lam</span><span class="p">)</span>
    <span class="n">ppc_uncensored</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_uncensored</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">])</span>

<span class="k">with</span> <span class="n">dropped_model</span><span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'y_pred'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="n">lam</span><span class="p">)</span>
    <span class="n">ppc_dropped</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_dropped</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">])</span>

<span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">xlim</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]</span>

<span class="n">xticks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">yticks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">ppc_uncensored</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span>  <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Uncensored'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">xticks</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">yticks</span><span class="p">)</span>

<span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">ppc_dropped</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span>  <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Dropped'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">xticks</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">yticks</span><span class="p">)</span>

<span class="n">ax3</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">ppc_censored</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span>  <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Censored'</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">xticks</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">yticks</span><span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/survival_intro/ppc_compare.webp" alt="The PPC distribution for the three models" /></p>

<p>In the above figures, the red histogram corresponds to the true (uncensored) data, while
the blue one corresponds to the posterior predictive distribution of our model.
The effect of the bias for method 1 and 2 is quite evident, while the censored
model predicts a distribution which is quite close to the true data.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We introduced survival analysis, and we introduced some main concept as
the hazard function and the survival function.
We also discussed censorship, and we showed with an example why it is important
to correctly account of censoring.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/survival_intro/" /><summary type="html"><![CDATA[Estimating waiting times]]></summary></entry></feed>