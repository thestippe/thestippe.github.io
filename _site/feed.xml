<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-06-05T21:34:11+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Data Perspectives</title><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><entry><title type="html">Splines regression</title><link href="http://localhost:4000/statistics/splines" rel="alternate" type="text/html" title="Splines regression" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>http://localhost:4000/statistics/splines</id><content type="html" xml:base="http://localhost:4000/statistics/splines"><![CDATA[<p>A common issue that any data scientist faced at some point
is how to include non-linearities into regression.
It is well known that higher order polynomials tend to overfit,
and it is therefore generally not a good idea to use this kind of
model.</p>

<p>A very flexible solution is given by the splines, which are
piecewise smooth functions.
There are many kind of splines, and recently 
<a href="https://en.wikipedia.org/wiki/B-spline">B-splines</a> gained
a lot of attention since they are very easy to implement, and they
are numerically stable.
Here we will use them to fit the “Six cities study”
of <a href="https://content.sph.harvard.edu/fitzmaur/ala2e/">this link</a>,
from the “Applied Longitudinal Analysis” textbook by Fitzmaurice, Laird
and Ware.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_stata</span><span class="p">(</span><span class="s">'fev1.dta'</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s">'id'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'id'</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'age'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'logfev1'</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/splines/data.webp" alt="Our dataset" /></p>

<p>We will try and model the relation between the Forced Expiratory Volume
(FEV) and the age.
The relation seems linear up to roughly 16 years, while above this point
it looks like the FEV saturates.
This is quite clear, since at some point the breath capacity must
saturate, since our growth stops.
Let us now implement the B-splines according to Wikipedia’s recursive algorithm</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">bspline</span><span class="p">(</span><span class="n">t</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span><span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="s">"""
    Returns a B-spline, defined as https://en.wikipedia.org/wiki/B-spline.

    Parameters:
    -----------
    t: np.array
    x: np.array
    i: int
    p: int

    Returns:
    np.array
    
    Raises:
    ------
    ValueError
       if i is not an integer between 0 and len(x)-p-1 (both included)
    """</span>
    <span class="k">if</span> <span class="n">i</span><span class="o">&gt;=</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span><span class="o">&lt;</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="n">p</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">heaviside</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">heaviside</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">t</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">fac0</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">p</span><span class="p">]</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">fac1</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">t</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">fac0</span><span class="o">*</span><span class="n">bspline</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">fac1</span><span class="o">*</span><span class="n">bspline</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s">'Got i=</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">, i must be an integer between 0 and len(x)-p-1=</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="n">p</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<p>The splines are quite fast, but our dataset contains thousands of points,
and for each point we calculate the likelihood thousands of times,
so it is better to precompute the value of the splines.
We will use second order splines in order to ensure smoothness,
and we will use 15 knots.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_fit</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">p_fit</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">splines_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_fit</span><span class="p">)</span><span class="o">-</span><span class="n">p_fit</span><span class="o">-</span><span class="mi">1</span>

<span class="n">basis</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">bspline</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'age'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">x_fit</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">p_fit</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">splines_dim</span><span class="p">)])</span>
</code></pre></div></div>

<p>Before fitting the model, let us take a look at our basis functions</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_plot</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>

<span class="n">basis_plot</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">bspline</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">x_fit</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">p_fit</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">splines_dim</span><span class="p">)])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">basis_plot</span><span class="p">:</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">elem</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/splines/basis.webp" alt="Our basis functions" /></p>

<p>We are finally ready to fit our model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">spline_model</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'w'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">splines_dim</span><span class="p">))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">df</span><span class="p">[</span><span class="s">'age'</span><span class="p">]</span> <span class="o">+</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">basis</span><span class="p">)</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'yhat'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'logfev1'</span><span class="p">])</span>

<span class="k">with</span> <span class="n">spline_model</span><span class="p">:</span>
    <span class="n">idata_spline</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_spline</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/splines/trace.webp" alt="Our dataset" /></p>

<p>We can now verify if our model is able to describe the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">spline_model</span><span class="p">:</span>
    <span class="n">mu_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'mu_pred'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">x_plot</span> <span class="o">+</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">basis_plot</span><span class="p">))</span>
    <span class="n">yhat_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'yhat_pred'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu_pred</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

<span class="k">with</span> <span class="n">spline_model</span><span class="p">:</span>
    <span class="n">ppc_spline</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata_spline</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'yhat_pred'</span><span class="p">,</span> <span class="s">'mu_pred'</span><span class="p">])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span>
<span class="n">ppc_spline</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pred'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span>
<span class="n">ppc_spline</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
<span class="n">ppc_spline</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
                <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span>
               <span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'age'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'logfev1'</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="n">x_plot</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_plot</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/splines/ppc.webp" alt="Our dataset" /></p>

<p>As we can see, our model accurately reproduces our data without
overfitting it. It has some small issue above 18, and the fact
that splines are not very stable just below the boundaries
is a known issue.</p>

<h2 id="conclusions">Conclusions</h2>

<p>You should consider using splines if you need more flexibility than
ordinary linear regression, as they allow you to smoothly add non-linearity
without overfitting.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/splines_regression/" /><summary type="html"><![CDATA[Going beyond linear models]]></summary></entry><entry><title type="html">Introduction to non-parametric models</title><link href="http://localhost:4000/statistics/nonparametric" rel="alternate" type="text/html" title="Introduction to non-parametric models" /><published>2024-06-02T00:00:00+00:00</published><updated>2024-06-02T00:00:00+00:00</updated><id>http://localhost:4000/statistics/nonparametric</id><content type="html" xml:base="http://localhost:4000/statistics/nonparametric"><![CDATA[<p>Non-parametric models are becoming more and more popular in Bayesian
statistics, as they are able to accurately reproduce complex data
and patterns.
In this section we will introduce some of the most popular
non-parametric models, namely splines and Dirichlet processes-related
models.</p>

<p>When dealing with these models, one should keep in mind that
the Bernstein-Von Mises theorem does not hold, so one cannot
approximate frequentist confidence intervals with Bayesian
credible intervals.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/nonparametric_intro/" /><summary type="html"><![CDATA[Letting the number of parameters vary]]></summary></entry><entry><title type="html">The autoregressive model</title><link href="http://localhost:4000/statistics/autoregressive" rel="alternate" type="text/html" title="The autoregressive model" /><published>2024-02-16T00:00:00+00:00</published><updated>2024-02-16T00:00:00+00:00</updated><id>http://localhost:4000/statistics/autoregressive</id><content type="html" xml:base="http://localhost:4000/statistics/autoregressive"><![CDATA[<p>In the last post we introduced the time series, and in this post
we will look more in details to the autoregressive model,
namely</p>

\[y_k = \sum_{i=1}^r y_{k-i} + \varepsilon_k\]

<p>where $\varepsilon_k$ are iid.</p>

<p>We will use the airline passengers dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">statsmodels.graphics.tsaplots</span> <span class="kn">import</span> <span class="n">plot_acf</span><span class="p">,</span> <span class="n">plot_pacf</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">from</span> <span class="nn">scipy.signal</span> <span class="kn">import</span> <span class="n">periodogram</span>
<span class="kn">import</span> <span class="nn">pymc.sampling_jax</span> <span class="k">as</span> <span class="n">pmj</span>

<span class="n">df_pass</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"https://raw.githubusercontent.com/MakrandBhandari/Time-Series-Forecasting--Airline-Passengers-in-Python/main/international-airline-passengers.csv"</span><span class="p">)</span>
<span class="n">df_pass</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">"International airline passengers: monthly totals in thousands. Jan 49 ? Dec 60"</span><span class="p">:</span> <span class="s">"Thous_pass"</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df_pass</span> <span class="o">=</span> <span class="n">df_pass</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">144</span><span class="p">]</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"Month"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"Thous_pass"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[::</span><span class="mi">24</span><span class="p">])</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/autoregressive/airline.webp" alt="" /></p>

<p>A visual inspection of the data can be very useful, but sometimes it could not be 
enough to build a starting model. 
As an example, we can clearly see that there is a (probably linear) trend in our series,
that there is a strong annual periodicity, and that the amplitude of the periodic part increases
with the time.
There are however methods which may help us in this task.
The first is the <a href="https://en.wikipedia.org/wiki/Periodogram"><strong>periodogram</strong></a>, which could help us in assessing the frequency of the
seasonal part.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f</span><span class="p">,</span> <span class="n">Pxx_spec</span> <span class="o">=</span> <span class="n">periodogram</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Thous_pass'</span><span class="p">],</span> <span class="n">detrend</span><span class="o">=</span><span class="s">'linear'</span><span class="p">,</span> <span class="n">scaling</span><span class="o">=</span><span class="s">'spectrum'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">stem</span><span class="p">(</span><span class="n">f</span><span class="o">*</span><span class="mi">12</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Pxx_spec</span><span class="p">))</span> <span class="c1"># The dataset has monthly frequency, we now convert the frequency in Y^-1
</span><span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'frequency $[Y^{-1}]$'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">35</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/autoregressive/periodogram.webp" alt="The periodogram, where the frequency is expressed in inverse years" /></p>

<p>The peak at 1 confirms us that there is a strong periodic component at one year, as well as
possibly some higher frequency components.</p>

<p>We now want to have an idea of the order of the autoregressive part, and
plotting the coefficients of the autocorrelation function can be helpful in this task.
While the periodogram automatically handles the linear component, here we must remove 
both the trend and the periodic components by hand. We can do this as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">detrended</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">diff</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Thous_pass'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
<span class="n">deperiod</span> <span class="o">=</span> <span class="n">detrended</span><span class="p">[:</span><span class="o">-</span><span class="mi">12</span><span class="p">]</span> <span class="o">-</span> <span class="n">detrended</span><span class="p">[</span><span class="mi">12</span><span class="p">:]</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">plot_acf</span><span class="p">(</span><span class="n">deperiod</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">plot_pacf</span><span class="p">(</span><span class="n">deperiod</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax1</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/autoregressive/acorr.webp" alt="The ACF and PACF plot" /></p>

<p>It looks like there is a small autoregressive component of order one.</p>

<p>We will now build a model with a trend, a periodic and a \(AR(1)\) component.
We will start by only assuming a yearly component, and in order to reduce the
trend in the amplitude of the seasonal component, we will model the logarithm
of the number of passengers.
We will only use the first 120 points to fit the model, while the remaining 2 years will
be used to assess the performances of our model for future data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pass</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Thous_pass'</span><span class="p">])).</span><span class="n">values</span>
<span class="n">x_pass</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pass</span><span class="p">))</span>

<span class="n">y_pass_fit</span> <span class="o">=</span> <span class="n">y_pass</span><span class="p">[:</span><span class="mi">120</span><span class="p">]</span>
<span class="n">x_pass_fit</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pass_fit</span><span class="p">))</span>
<span class="n">n_pred</span> <span class="o">=</span> <span class="mi">144</span> <span class="o">-</span> <span class="mi">120</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">pass_model</span><span class="p">:</span>
    <span class="n">y0</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y0'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'gamma'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">eta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'eta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_ar</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">AR</span><span class="p">(</span><span class="s">'x_ar'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="n">eta</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pass_fit</span><span class="p">))</span>
    <span class="n">muv</span> <span class="o">=</span> <span class="n">y0</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">x_pass_fit</span> <span class="o">+</span> <span class="n">x_ar</span>  <span class="o">+</span>  <span class="n">beta</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="n">x_pass_fit</span><span class="o">/</span><span class="mi">12</span><span class="p">))</span><span class="o">+</span>   <span class="n">gamma</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="n">x_pass_fit</span><span class="o">/</span><span class="mi">12</span><span class="p">))</span>
    <span class="n">yhat_pass</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'yhat_pass'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">muv</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_pass_fit</span><span class="p">)</span>
    <span class="n">trace_pass</span> <span class="o">=</span> <span class="n">pmj</span><span class="p">.</span><span class="n">sample_numpyro_nuts</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/autoregressive/trace.webp" alt="The trace plot" /></p>

<p>The trace looks OK, let us now verify if our model reproduces the fitted data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pass_model</span><span class="p">:</span>
    <span class="n">ppc_pass</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_pass</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span> <span class="n">y_pass</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pass_fit</span><span class="p">)],</span> <span class="n">ppc_pass</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'chain'</span><span class="p">,</span> <span class="s">'draw'</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pass_fit</span><span class="p">)],</span> <span class="n">ppc_pass</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'chain'</span><span class="p">,</span> <span class="s">'draw'</span><span class="p">],</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">),</span> <span class="n">ppc_pass</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'chain'</span><span class="p">,</span> <span class="s">'draw'</span><span class="p">],</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">),</span>
               <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="mi">12</span><span class="p">::</span><span class="mi">24</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/autoregressive/ppc_fitted.webp" alt="The PPC for the fit data" /></p>

<p>The agreement looks good, except for some minor issues with high frequency modes which is
however not interesting for us, since we are not interested in such a high resolution.
We can now verify is our model is also able to reproduce the observed data for the last two years.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_pass_pred</span> <span class="o">=</span> <span class="n">x_pass</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x_pass_fit</span><span class="p">):]</span>

<span class="k">with</span> <span class="n">pass_model</span><span class="p">:</span>
    <span class="n">pass_model</span><span class="p">.</span><span class="n">add_coords</span><span class="p">({</span><span class="s">"z_1"</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pass_fit</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pass</span><span class="p">),</span> <span class="mi">1</span><span class="p">)})</span>
    <span class="n">x_ar_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">AR</span><span class="p">(</span>
            <span class="s">"x_ar_pred"</span><span class="p">,</span>
            <span class="n">init_dist</span><span class="o">=</span><span class="n">pm</span><span class="p">.</span><span class="n">DiracDelta</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="n">x_ar</span><span class="p">[...,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
            <span class="n">rho</span><span class="o">=</span><span class="n">eta</span><span class="p">,</span>
            <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span>
            <span class="n">dims</span><span class="o">=</span><span class="s">"z_1"</span><span class="p">)</span>
    <span class="n">periodic</span> <span class="o">=</span>  <span class="n">beta</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="n">x_pass_pred</span><span class="o">/</span><span class="mi">12</span><span class="p">))</span><span class="o">+</span><span class="n">gamma</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="n">x_pass_pred</span><span class="o">/</span><span class="mi">12</span><span class="p">))</span>
    <span class="n">muv</span> <span class="o">=</span> <span class="n">y0</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">x_pass_pred</span> <span class="o">+</span> <span class="n">x_ar_pred</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>  <span class="o">+</span>  <span class="n">periodic</span>
    <span class="n">yhat_pass_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">"yhat_pass_pred"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">muv</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
    
    <span class="n">ppc_ar_pred_y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_pass</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'yhat_pass_pred'</span><span class="p">])</span>

<span class="n">ypass_av</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span>
<span class="n">ppc_pass</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span> <span class="n">ppc_ar_pred_y</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass_pred'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">])])</span>
<span class="n">ypass_m</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span>
<span class="n">ppc_pass</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span> <span class="n">ppc_ar_pred_y</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">])])</span>
<span class="n">ypass_M</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span>
<span class="n">ppc_pass</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span> <span class="n">ppc_ar_pred_y</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yhat_pass_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">])])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span> <span class="n">y_pass</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span> <span class="n">ypass_av</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span> <span class="n">ypass_m</span><span class="p">,</span> <span class="n">ypass_M</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="mi">12</span><span class="p">::</span><span class="mi">24</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pass_fit</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/autoregressive/ppc_all.webp" alt="" /></p>

<p>We observe some discrepancy for the yearly minimum, but except for that the data
are always included in the credible interval.</p>

<p>We can now inspect the contribution of each component</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pass_model</span><span class="p">:</span>
    <span class="n">ppc_ar_pred_ar</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_pass</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'x_ar_pred'</span><span class="p">])</span>

<span class="n">x_pass_all</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x_pass_fit</span><span class="p">,</span> <span class="n">x_pass_pred</span><span class="p">])</span>

<span class="n">y_trend</span> <span class="o">=</span> <span class="p">(</span><span class="n">trace_pass</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'y0'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]).</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">trace_pass</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]).</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">x_pass_all</span><span class="p">)</span>
<span class="n">y_seas</span> <span class="o">=</span> <span class="p">(</span><span class="n">trace_pass</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'beta'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]).</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="n">x_pass_all</span><span class="o">/</span><span class="mi">12</span><span class="p">))</span><span class="o">+</span><span class="n">trace_pass</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]).</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="n">x_pass_all</span><span class="o">/</span><span class="mi">12</span><span class="p">)))</span>
<span class="n">y_res</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">trace_pass</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'x_ar'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span> <span class="n">ppc_ar_pred_ar</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'x_ar_pred'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">])[</span><span class="mi">1</span><span class="p">:]])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">311</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span><span class="n">y_trend</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Trend'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="mi">12</span><span class="p">::</span><span class="mi">24</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x_pass_fit</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>

<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">312</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span><span class="n">y_seas</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Seasonal'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="mi">12</span><span class="p">::</span><span class="mi">24</span><span class="p">])</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x_pass_fit</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>

<span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">313</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span><span class="n">y_res</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Residual'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="mi">12</span><span class="p">::</span><span class="mi">24</span><span class="p">])</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x_pass_fit</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/autoregressive/components.webp" alt="" /></p>

<p>As expected, the trend component is the most relevant one. We can also see that
the residual component has the same order of magnitude of the seasonal one.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We have seen some tools which may help us choosing an appropriate model.
We have also seen how to implement a time series with an autoregressive component.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/time_series/" /><summary type="html"><![CDATA[How to model dependence on the past]]></summary></entry><entry><title type="html">Introduction to time series modelling</title><link href="http://localhost:4000/statistics/time_series" rel="alternate" type="text/html" title="Introduction to time series modelling" /><published>2024-02-15T00:00:00+00:00</published><updated>2024-02-15T00:00:00+00:00</updated><id>http://localhost:4000/statistics/time_series</id><content type="html" xml:base="http://localhost:4000/statistics/time_series"><![CDATA[<p>Up to now we assumed that our samples were iid according to some
probability distribution. We will now modify this assumption
ad assume that each observation depends on the previous
observations.
We will assume that we are dealing with discrete time process,
and leave continuous time ones to a future post.</p>

<p>Let us take a look at some example of time series.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">statsmodels</span> <span class="k">as</span> <span class="n">stats</span>
<span class="kn">import</span> <span class="nn">yfinance</span> <span class="k">as</span> <span class="n">yf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">statsmodels.graphics.tsaplots</span> <span class="kn">import</span> <span class="n">plot_acf</span><span class="p">,</span> <span class="n">plot_pacf</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="n">sm</span>
<span class="kn">import</span> <span class="nn">pyreadr</span>

<span class="n">df_co2</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"https://raw.githubusercontent.com/gahjelle/data-analysis-with-python/master/data/co2-ppm-mauna-loa-19651980.csv"</span><span class="p">)</span>
<span class="n">df_co2</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'CO2 (ppm) mauna loa, 1965-1980'</span><span class="p">:</span><span class="s">"CO2_ppm"</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df_stock</span> <span class="o">=</span> <span class="n">yf</span><span class="p">.</span><span class="n">download</span><span class="p">(</span><span class="s">'RB=F'</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="s">'2021-01-01'</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s">'2023-01-01'</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="s">'1wk'</span><span class="p">)</span>

<span class="n">df_lynx</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'http://people.whitman.edu/~hundledr/courses/M250F03/LynxHare.txt'</span><span class="p">,</span> <span class="n">delim_whitespace</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">df_lynx</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="s">'Year'</span>
<span class="n">df_lynx</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Hare'</span><span class="p">,</span> <span class="s">'Lynx'</span><span class="p">]</span>

<span class="n">df_pass</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"https://raw.githubusercontent.com/MakrandBhandari/Time-Series-Forecasting--Airline-Passengers-in-Python/main/international-airline-passengers.csv"</span><span class="p">)</span>
<span class="n">df_pass</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">"International airline passengers: monthly totals in thousands. Jan 49 ? Dec 60"</span><span class="p">:</span> <span class="s">"Thous_pass"</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_pass</span> <span class="o">=</span> <span class="n">df_pass</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">144</span><span class="p">]</span>

<span class="n">df_ssp</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://www.sidc.be/SILSO/INFO/snmtotcsv.php'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">';'</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">df_sunspot</span> <span class="o">=</span> <span class="n">df_ssp</span><span class="p">.</span><span class="n">set_axis</span><span class="p">([</span><span class="s">'YEAR'</span><span class="p">,</span> <span class="s">'month'</span><span class="p">,</span> <span class="s">'ym'</span><span class="p">,</span> <span class="s">'SUNACTIVITY'</span><span class="p">,</span> <span class="s">'activity_sd'</span><span class="p">,</span> <span class="s">'n_obs'</span><span class="p">,</span> <span class="s">'is_definitive'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df_sunspot</span> <span class="o">=</span> <span class="n">df_sunspot</span><span class="p">[</span><span class="n">df_sunspot</span><span class="p">[</span><span class="s">'YEAR'</span><span class="p">]</span><span class="o">&lt;</span><span class="mi">2024</span><span class="p">]</span>

<span class="k">with</span> <span class="n">requests</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'https://github.com/cran/TSA/raw/master/data/larain.rda'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'./larain.rda'</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">g</span><span class="p">:</span>
        <span class="n">g</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">content</span><span class="p">)</span>

<span class="n">df_rain</span> <span class="o">=</span> <span class="n">pyreadr</span><span class="p">.</span><span class="n">read_r</span><span class="p">(</span><span class="s">'./larain.rda'</span><span class="p">)</span>

<span class="n">df_larain</span> <span class="o">=</span> <span class="n">df_rain</span><span class="p">[</span><span class="s">'larain'</span><span class="p">]</span>

<span class="n">df_larain</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1878</span><span class="p">,</span><span class="mi">1993</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_co2</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"Month"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"CO2_ppm"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_co2</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[::</span><span class="mi">24</span><span class="p">])</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_stock</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"Date"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"Close"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_lynx</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"Year"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"Lynx"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_pass</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"Month"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"Thous_pass"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">].</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_pass</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[::</span><span class="mi">24</span><span class="p">])</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_sunspot</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"YEAR"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"SUNACTIVITY"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_larain</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"year"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"larain"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/time_series_intro/ts_examples.webp" alt="Some time series example
" /></p>

<p>Each time series has different characteristics, and each of them encodes different
features.</p>

<p>First of all, we can classify the time series depending on its <strong>trend</strong>, that is
the presence of a monotone growth or decrease of the mean.
This component is clearly visible in the airline dataset (4), in the $CO_2$ one (1) and in the stock
(number 2, where we can see a growth followed by a decrease).</p>

<p>Another kind of common component is the <strong>seasonal</strong> (or periodic) one,
which is present if there’s some recurrent pattern.
This component is evident in the $CO_2$ and in the airline dataset, but also possibly in the lynx one
as well as in the sunspot data.</p>

<p>A common approach is to decompose the time series into a sum of trend, seasonality and residual random component,
and not all of them are always needed.
As an example, the $CO_2$ time series can be well reproduced without its random component,
while the inclusion of this part will the core topic of the future posts in this section.</p>

<h2 id="the-co_2-series">The \(CO_2\) series</h2>

<p>We will only use a subset of our sample to fit the model, and use the remaining
point to check the performance of our model for this subset of points.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cut_df</span> <span class="o">=</span> <span class="mi">150</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">df_co2</span><span class="p">[</span><span class="s">'CO2_ppm'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[:</span><span class="n">cut_df</span><span class="p">]</span><span class="o">/</span><span class="mi">1000</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="n">y_all</span> <span class="o">=</span> <span class="n">df_co2</span><span class="p">[</span><span class="s">'CO2_ppm'</span><span class="p">]</span><span class="o">/</span><span class="mi">1000</span>
<span class="n">X_all</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_all</span><span class="p">))</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">co2_model</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'gamma'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">Xp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">X</span><span class="o">/</span><span class="mi">12</span><span class="p">)</span>  <span class="c1"># The regressor for the periodic component
</span>    <span class="n">mu0</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">+</span><span class="n">beta</span><span class="o">*</span><span class="n">X</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">Xp</span>
    <span class="c1"># X is the regressor for the trend component
</span>    <span class="n">y_co2</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y_co2'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="k">with</span> <span class="n">co2_model</span><span class="p">:</span>
    <span class="n">trace_co2</span> <span class="o">=</span> <span class="n">pmj</span><span class="p">.</span><span class="n">sample_numpyro_nuts</span><span class="p">(</span><span class="n">tune</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_co2</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/time_series_intro/trace_co2.webp" alt="The trace of the CO2 model
" /></p>

<p>Let us now extent the model above the fitted data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">co2_model</span><span class="p">:</span>
    <span class="n">Xp_all</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">2.0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">X_all</span><span class="o">/</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">mu0_all</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">+</span><span class="n">beta</span><span class="o">*</span><span class="n">X_all</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">Xp_all</span>
    <span class="n">y_co2_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y_co2_pred'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu0_all</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
    <span class="n">ppc_co2</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_co2</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y_co2_pred'</span><span class="p">])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">df_co2</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span>
<span class="n">ppc_co2</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_co2_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
<span class="n">ppc_co2</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_co2_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
    <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span>
<span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">df_co2</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span>
<span class="n">ppc_co2</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_co2_pred'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]))</span>

<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_co2</span><span class="p">[</span><span class="s">'Month'</span><span class="p">],</span> <span class="n">y_all</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_co2</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="n">cut_df</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_co2</span><span class="p">[</span><span class="s">'Month'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[::</span><span class="mi">24</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/time_series_intro/co2.webp" alt="The posterior predictive of the CO2 model" /></p>

<p>We are using only the data below the black dotted line to fit the model.
As we can see, our model reproduces with quite a good accuracy also the data above
this line for many months.
When you have enough data it can be really useful to perform this check, in order to ensure
that your model is able to reproduce the observed data for the future,
at least with the current knowledge.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We have seen a possible decomposition of a time series, and we saw that a possible
way to deal with the seasonal and trend components is to treat them as we would
do in a normal inference problem.
We saw how to split our data in order to check the performance of our model for unobserved data.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/time_series/" /><summary type="html"><![CDATA[How to model time-dependent processes]]></summary></entry><entry><title type="html">Synthetic control</title><link href="http://localhost:4000/statistics/synthetic_control" rel="alternate" type="text/html" title="Synthetic control" /><published>2024-02-12T00:00:00+00:00</published><updated>2024-02-12T00:00:00+00:00</updated><id>http://localhost:4000/statistics/synthetic_control</id><content type="html" xml:base="http://localhost:4000/statistics/synthetic_control"><![CDATA[<p>The <strong>synthetic control</strong> method recently became a very popular method
among economists (although I honestly can’t see the same enthusiasm in
the statistics community).
This method has been widely (and a little bit wildly) used to assess
the effects on a quantity \(Y^{\bar{s}}_t\) of the introduction of a new policy into a country $s$
(or other geographical region) at a time $t=t_1$.
Assuming that you have the same quantity for a set of similar countries $s_i$
as well as for the target country $\bar{s}\,,$
you assume that the time behavior of \(Y_{\bar{s}} = (Y_{t_0}^{\bar{s}}, \dots, Y^{\bar{s}}_{t_1})\) before the intervention is given by a weighted
average of $Y^{s_i}\,.$</p>

<p>You moreover assume, as control, the same weighted average
\(\bar{Y}^{\bar{s}}\) after the intervention.</p>

<p>A very detailed discussion of this method can be found on <a href="https://juanitorduz.github.io/synthetic_control_pymc/">Juan Camilo Orduz’ page</a>.
We will use the same model, but we will apply it to a different dataset.</p>

<p>While in fact he uses PyMC to reproduce <a href="https://matheusfacure.github.io/python-causality-handbook/landing-page.html">this example</a>,
we will use it to perform a simplified re-analysis of <a href="https://link.springer.com/article/10.1007/s10584-021-03111-2">this article</a>, where the authors analyze the impact of the introduction
of a policy for the reduction of the $CO_2$ emissions in the UK.
The dataset used in this work can be found <a href="https://zenodo.org/records/4566804">on Zenodo</a>.</p>

<p>The authors of the original work, in fact, performed a careful analysis
of the control set, while we will limit ourself to the set of countries
who were in the OECD organization in 2001 and who had not adopted any 
$CO_2$ reduction policy before that year.
We will assume</p>

\[Y^{\bar{s}} \sim \mathcal{N}(\mu, \sigma)\]

<p>In order to ensure that the behavior before the intervention is carefully
reproduced, we assume a small variance</p>

\[\sigma \sim \mathcal{Exp}(100)\]

<p>As anticipated, $\mu$ is given by</p>

\[\mu = \sum_{i=1}^n \omega_{i} Y^i\]

<p>We assume that the weights sum up to one, so we assume</p>

\[\omega \sim \mathcal{Dir}(1/n,\dots,1/n)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc.sampling_jax</span> <span class="k">as</span> <span class="n">pmjax</span>

<span class="n">df_dt</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./data/climate_policies.csv'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">';'</span><span class="p">)</span>
<span class="n">df_carb</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./data/nation.1751_2014.csv'</span><span class="p">)</span>

<span class="n">df_red</span> <span class="o">=</span> <span class="n">df_carb</span><span class="p">[</span><span class="n">df_carb</span><span class="p">[</span><span class="s">'Year'</span><span class="p">]</span><span class="o">&gt;=</span><span class="mi">1990</span><span class="p">][[</span><span class="s">'Nation'</span><span class="p">,</span> <span class="s">'Year'</span><span class="p">,</span> <span class="s">'Per capita CO2 emissions (metric tons of carbon)'</span><span class="p">]]</span>

<span class="n">df_co2</span> <span class="o">=</span> <span class="n">df_red</span><span class="p">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s">'Year'</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s">'Nation'</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s">'Per capita CO2 emissions (metric tons of carbon)'</span><span class="p">)</span>

<span class="c1"># Taken from the repo
</span>
<span class="n">oecd</span> <span class="o">=</span> <span class="p">[</span><span class="s">"AUSTRALIA"</span><span class="p">,</span><span class="s">"AUSTRIA"</span><span class="p">,</span><span class="s">"BELGIUM"</span><span class="p">,</span><span class="s">"CANADA"</span><span class="p">,</span><span class="s">"CZECH REPUBLIC"</span><span class="p">,</span>
        <span class="s">"DENMARK"</span><span class="p">,</span><span class="s">"FINLAND"</span><span class="p">,</span><span class="s">"FRANCE (INCLUDING MONACO)"</span><span class="p">,</span><span class="s">"GERMANY"</span><span class="p">,</span>
        <span class="s">"GREECE"</span><span class="p">,</span><span class="s">"HUNGARY"</span><span class="p">,</span><span class="s">"ICELAND"</span><span class="p">,</span><span class="s">"IRELAND"</span><span class="p">,</span><span class="s">"ITALY (INCLUDING SAN MARINO)"</span><span class="p">,</span>
        <span class="s">"JAPAN"</span><span class="p">,</span><span class="s">"LUXEMBOURG"</span><span class="p">,</span><span class="s">"MEXICO"</span><span class="p">,</span><span class="s">"NETHERLANDS"</span><span class="p">,</span><span class="s">"NEW ZEALAND"</span><span class="p">,</span><span class="s">"NORWAY"</span><span class="p">,</span>
        <span class="s">"POLAND"</span><span class="p">,</span><span class="s">"PORTUGAL"</span><span class="p">,</span><span class="s">"SLOVAKIA"</span><span class="p">,</span><span class="s">"REPUBLIC OF KOREA"</span><span class="p">,</span><span class="s">"SPAIN"</span><span class="p">,</span><span class="s">"SWEDEN"</span><span class="p">,</span>
        <span class="s">"SWITZERLAND"</span><span class="p">,</span><span class="s">"TURKEY"</span><span class="p">,</span><span class="s">"UNITED KINGDOM"</span><span class="p">,</span><span class="s">"UNITED STATES OF AMERICA"</span><span class="p">]</span>

<span class="n">to_exclude</span> <span class="o">=</span> <span class="p">[</span><span class="s">'DENMARK'</span><span class="p">,</span> <span class="s">'ESTONIA'</span><span class="p">,</span> <span class="s">'FINLAND'</span><span class="p">,</span> <span class="s">'NETHERLANDS'</span><span class="p">,</span> <span class="s">'NORWAY'</span><span class="p">,</span>
       <span class="s">'SLOVENIA'</span><span class="p">,</span> <span class="s">'SWEDEN'</span><span class="p">,</span> <span class="s">"UNITED KINGDOM"</span><span class="p">]</span>

<span class="n">donors</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">oecd</span><span class="p">)</span><span class="o">-</span><span class="nb">set</span><span class="p">(</span><span class="n">to_exclude</span><span class="p">))</span>

<span class="n">df_in</span> <span class="o">=</span> <span class="n">df_co2</span><span class="p">[</span><span class="n">donors</span><span class="p">].</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">sc_model</span><span class="p">:</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="s">'w'</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">)))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">):</span>
        <span class="n">mu</span> <span class="o">+=</span> <span class="n">w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">df_in</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">loc</span><span class="p">[</span><span class="mi">1990</span><span class="p">:</span><span class="mi">2001</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_co2</span><span class="p">[</span><span class="s">'UNITED KINGDOM'</span><span class="p">].</span><span class="n">loc</span><span class="p">[</span><span class="mi">1990</span><span class="p">:</span><span class="mi">2001</span><span class="p">])</span>

<span class="k">with</span> <span class="n">sc_model</span><span class="p">:</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pmjax</span><span class="p">.</span><span class="n">sample_numpyro_nuts</span><span class="p">(</span><span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>

</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/synthetic_control/trace.webp" alt="The trace plot" /></p>

<p>We ran quite a large number of draws as the number of parameters is quite large
and rather unconstrained. However, the trace looks fine.
An important thing that one should always verify when using
a synthetic control, is that the weights must be sparse (only few should
dominate, while the remaining should be close to 0).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'w'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/synthetic_control/weights.webp" alt="The forest plot of the weights" /></p>

<p>The requirement seems fulfilled, as only few dominate the entire fit.
We can now compute the posterior predictive before and after the intervention.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">sc_model</span><span class="p">:</span>
    <span class="n">mu1</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df_in</span><span class="p">.</span><span class="n">columns</span><span class="p">):</span>
        <span class="n">mu1</span> <span class="o">+=</span> <span class="n">w</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">df_in</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">loc</span><span class="p">[</span><span class="mi">2002</span><span class="p">:].</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">y1</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y1'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu1</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y'</span><span class="p">,</span> <span class="s">'y1'</span><span class="p">])</span>

<span class="n">yv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">ppc</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">20000</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
                     <span class="n">ppc</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y1'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">20000</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))],</span>
                     <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[[</span><span class="s">'right'</span><span class="p">,</span> <span class="s">'top'</span><span class="p">]].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
<span class="n">uk</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">df_co2</span><span class="p">[</span><span class="s">'UNITED KINGDOM'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'UK'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">yv</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">yv</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">synth</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">yv</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'Synthetic UK'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">2001</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Year"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">"Per capita $CO_2$ $m^3/Year$"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">'Synthetic'</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">yv</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">color</span><span class="o">=</span><span class="n">synth</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">get_color</span><span class="p">()</span> <span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">'UK'</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">df_co2</span><span class="p">.</span><span class="n">index</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">df_co2</span><span class="p">[</span><span class="s">'UNITED KINGDOM'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">color</span><span class="o">=</span><span class="n">uk</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">get_color</span><span class="p">()</span> <span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'/home/stippe/thestippe.github.io/docs/assets/images/statistics/synthetic_control/posterior_predictive.webp'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/synthetic_control/posterior_predictive.webp" alt="The comparison between the true and the synthetic UK" /></p>

<p>As we can see, the behavior is very similar up to 2001, while after this date
the synthetic UK $CO_2$ consumption is larger than one of the true $UK\,.$
You can verify yourself that, by only fitting up to 2000, the result doesn’t
change, and the lines still diverge starting from 2002.
This is another important check that you should always perform when using the
synthetic control method.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We have seen how to implement the synthetic control method, together with
some of the most important checks that you should always do in order to
exclude major problems in your model.
We also re-analyzed <a href="https://link.springer.com/article/10.1007/s10584-021-03111-2">this article</a>, obtaining the same conclusions.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/discontinuity_regression/" /><summary type="html"><![CDATA[Building a doppleganger from the control group]]></summary></entry><entry><title type="html">Regression discontinuity</title><link href="http://localhost:4000/statistics/discontinuity_regression" rel="alternate" type="text/html" title="Regression discontinuity" /><published>2024-02-11T00:00:00+00:00</published><updated>2024-02-11T00:00:00+00:00</updated><id>http://localhost:4000/statistics/discontinuity_regression</id><content type="html" xml:base="http://localhost:4000/statistics/discontinuity_regression"><![CDATA[<p>Regression discontinuity recently became a popular way to assess the effect
of an intervention $I$ which depends on some confounder $X$ via</p>

\[I=
\begin{cases}
0 &amp; x &lt; x_0 \\
1 &amp; x \geq x_0
\end{cases}\]

<p>where in general the effect of $Y$ on $X$ varies smoothly.
Since the dependence can be, in principle, arbitrary, many authors
discuss both the linear as well as higher order polynomials (See Angrist’ textbook below).
However, higher order polynomial regression should in principle be avoided,
as it may lead to artificial discontinuities, as extensively explained
in <a href="http://www.stat.columbia.edu/~gelman/research/published/2018_gelman_jbes.pdf">this work by Gelman and Imbens</a>.</p>

<p>We will perform a bayesian version of <a href="https://lfoswald.github.io/2021-spring-stats2/materials/session-7/07-online-tutorial/">this analysis</a> and, for the reasons explained
above, we will limit ourself to the linear dependence.
The dataset uses is the MADD dataset, which collects the
mortality rate of young people in the USA.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">df_madd</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"https://raw.githubusercontent.com/seramirezruiz/stats-ii-lab/master/Session%206/data/mlda.csv"</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_madd</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="s">'forcing'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'outcome'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/discontinuity_regression/data.webp" alt="The input data" /></p>

<p>Here “outcome” is the mortality rate from motor vehicle (per 100000),
while “forcing” is age minus 21 (we recall that, in the US, 21 is the age
where it is legally possible to drink alcoholic beverages).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">madd_model</span><span class="p">:</span>
  <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
  <span class="n">gamma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'gamma'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
  <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
  <span class="n">mu_0</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">df_madd</span><span class="p">[</span><span class="s">'forcing'</span><span class="p">].</span><span class="n">values</span>
  <span class="n">mu</span> <span class="o">=</span> <span class="n">mu_0</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">heaviside</span><span class="p">(</span><span class="n">df_madd</span><span class="p">[</span><span class="s">'forcing'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
  <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> 
                <span class="n">observed</span><span class="o">=</span><span class="n">df_madd</span><span class="p">[</span><span class="s">'outcome'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
  <span class="n">trace_madd</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_madd</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/discontinuity_regression/trace.webp" alt="The trace plot" /></p>

<p>The trace looks good, let us verify if our model is able to reproduce the data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">madd_model</span><span class="p">:</span>
    <span class="n">x_pl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">x_pl</span><span class="o">+</span><span class="n">gamma</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">heaviside</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="n">y_pl</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y_pl'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
    <span class="n">pp_plot</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="o">=</span><span class="n">trace_madd</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y_pl'</span><span class="p">])</span>

<span class="n">pp_madd</span> <span class="o">=</span> <span class="n">pp_plot</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">.</span><span class="n">y_pl</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_pl</span><span class="p">)))</span>

<span class="n">madd_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pp_madd</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">madd_qqmax</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">pp_madd</span><span class="p">,</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">madd_qqmin</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">pp_madd</span><span class="p">,</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_madd</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="s">'forcing'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'outcome'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">madd_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">madd_qqmin</span><span class="p">,</span> <span class="n">madd_qqmax</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/discontinuity_regression/posterior_predictive.webp" alt="The posterior predictive plot" /></p>

<p>The posterior predictive seems in good agreement with the observed data, and
the threshold effect seems quite evident. In fact it is quite large:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">trace_madd</span><span class="p">,</span> <span class="n">filter_vars</span><span class="o">=</span><span class="s">'like'</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="s">'gamma'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/discontinuity_regression/effect.webp" alt="The boxplot of the effect size" /></p>

<h2 id="conclusions">Conclusions</h2>

<p>We have discussed how to implement the regression discontinuity,
together with some recommendations on how to implement it, and we applied
it to the MADD dataset.</p>

<h2 id="recommended-readings">Recommended readings</h2>
<ul>
  <li><cite>Angrist, J. D., Pischke, J. (2009). Mostly harmless econometrics : an empiricist’s companion. UK: Princeton University Press.
</cite></li>
</ul>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/discontinuity_regression/" /><summary type="html"><![CDATA[Using jumps to estimate effects]]></summary></entry><entry><title type="html">Difference in difference</title><link href="http://localhost:4000/statistics/difference_in_differences" rel="alternate" type="text/html" title="Difference in difference" /><published>2024-02-10T00:00:00+00:00</published><updated>2024-02-10T00:00:00+00:00</updated><id>http://localhost:4000/statistics/difference_in_differences</id><content type="html" xml:base="http://localhost:4000/statistics/difference_in_differences"><![CDATA[<p>Difference in differences is a very old technique,
and one of the first applications of
this method was done by John Snow, who’s also
popular due to the cholera outbreak data visualization.</p>

<p>In his study, he used the <strong>Difference in Difference</strong>
(DiD) method to provide some evidence that,
during the London cholera epidemic of 1866,
the cholera was caused by drinking from a water
pump.
This method has been more recently used <a href="https://davidcard.berkeley.edu/papers/njmin-aer.pdf">by 
Card and Krueger in this work</a>
to analyze the causal relationship between
minimum wage and employment.
In 1992, the New Jersey increased the minimum wage
from 4.25 dollars to 5.00 dollars.
They compared the employment in Pennsylvania
and New Jersey before and after the minimum wage increase
to assess if it caused a decrease in the New Jersey
occupation, as supply and demand theory would predict.</p>

<p>DiD assumes that, before the intervention $I$,
the untreated group and the treated one
both evolve linearly with the time $t$ with the
same slope,
while after the intervention the treated group
changes slope.
Assuming, that the intervention was applied at time
$t=0$</p>

\[\begin{align}
&amp;
Y_{P}^0 = \alpha_{P} 
\\
&amp;
Y_{P}^1 = \alpha_{P} +\beta
\\
&amp;
Y_{NJ}^0 = \alpha_{NJ} 
\\
&amp;
Y_{NJ}^1 = \alpha_{NJ} +\beta + \gamma
\end{align}\]

<p>In the above formulas, the intervention effect
is simply $\gamma\,.$</p>

<h2 id="the-implementation">The implementation</h2>

<p>We downloaded the dataset from <a href="https://www.kaggle.com/code/harrywang/difference-in-differences-in-python/input">this page</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">df_employment</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/employment.csv'</span><span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_employment</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'state'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/difference_in_difference/pairplot.webp" alt="The dataset pairplot" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_before</span> <span class="o">=</span> <span class="n">df_employment</span><span class="p">[[</span><span class="s">'state'</span><span class="p">,</span> <span class="s">'total_emp_feb'</span><span class="p">]]</span>
<span class="n">df_after</span> <span class="o">=</span> <span class="n">df_employment</span><span class="p">[[</span><span class="s">'state'</span><span class="p">,</span> <span class="s">'total_emp_nov'</span><span class="p">]]</span>

<span class="c1"># We will assign t=0 data before treatment and t=1 after the treatment
# Analogously g=0 will be the control group, g=1 will be the test group
</span>
<span class="n">df_before</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">df_after</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">df_before</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'total_emp_feb'</span><span class="p">:</span> <span class="s">'Y'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_after</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'total_emp_nov'</span><span class="p">:</span> <span class="s">'Y'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df_before</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'state'</span><span class="p">:</span> <span class="s">'g'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_after</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'state'</span><span class="p">:</span> <span class="s">'g'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df_reg</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_before</span><span class="p">,</span> <span class="n">df_after</span><span class="p">])</span>

<span class="c1">## Let us build the interaction term
</span>
<span class="n">df_reg</span><span class="p">[</span><span class="s">'gt'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_reg</span><span class="p">[</span><span class="s">'g'</span><span class="p">]</span><span class="o">*</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span>

<span class="n">df_reg</span> <span class="o">=</span> <span class="n">df_reg</span><span class="p">[[</span><span class="s">'g'</span><span class="p">,</span> <span class="s">'t'</span><span class="p">,</span> <span class="s">'gt'</span><span class="p">,</span> <span class="s">'Y'</span><span class="p">]]</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">did_model</span><span class="p">:</span>
    <span class="n">beta_0</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_0'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">beta_g</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_g'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">beta_t</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_t'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">beta_gt</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_gt'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s">'nu'</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="n">beta_g</span><span class="o">*</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'g'</span><span class="p">]</span><span class="o">+</span> <span class="n">beta_t</span><span class="o">*</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span><span class="o">+</span> <span class="n">beta_gt</span><span class="o">*</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'gt'</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">,</span>
                  <span class="n">observed</span><span class="o">=</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'Y'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
    <span class="n">trace_did</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_did</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/difference_in_difference/trace.webp" alt="The model trace" /></p>

<p>The trace looks fine, let us now verify the posterior
predictive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">did_model</span><span class="p">:</span>
    <span class="n">y00</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y00'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">beta_0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    <span class="n">y10</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y10'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">beta_0</span><span class="o">+</span><span class="n">beta_g</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    <span class="n">y01</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y01'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">beta_0</span><span class="o">+</span><span class="n">beta_t</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    <span class="n">y11</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y11'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">beta_0</span><span class="o">+</span><span class="n">beta_g</span><span class="o">+</span><span class="n">beta_t</span><span class="o">+</span><span class="n">beta_gt</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>
    
    <span class="n">ppc_check</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span>
        <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y00'</span><span class="p">,</span><span class="s">'y01'</span><span class="p">,</span><span class="s">'y10'</span><span class="p">,</span><span class="s">'y11'</span><span class="p">],</span> <span class="n">trace</span><span class="o">=</span><span class="n">trace_did</span><span class="p">)</span>


<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]:</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">].</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">80</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
            <span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">az</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">ppc_check</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s">"y</span><span class="si">{</span><span class="n">g</span><span class="si">}{</span><span class="n">t</span><span class="si">}</span><span class="s">"</span><span class="p">],</span> <span class="n">group</span><span class="o">=</span><span class="s">'posterior_predictive'</span><span class="p">,</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">],</span>
                       <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">)</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">az</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">ppc_check</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s">"y</span><span class="si">{</span><span class="n">g</span><span class="si">}{</span><span class="n">t</span><span class="si">}</span><span class="s">"</span><span class="p">],</span> <span class="n">group</span><span class="o">=</span><span class="s">'posterior_predictive'</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">])</span>
        <span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">df_reg</span><span class="p">[(</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'g'</span><span class="p">]</span><span class="o">==</span><span class="n">g</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">df_reg</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span><span class="o">==</span><span class="n">t</span><span class="p">)][</span><span class="s">'Y'</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">g</span><span class="p">][</span><span class="n">t</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s">"g=</span><span class="si">{</span><span class="n">g</span><span class="si">}</span><span class="s">, t=</span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="n">legend</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">legend</span><span class="p">(</span><span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/difference_in_difference/posterior_predictives.webp" alt="The comparison between the predicted
and observed distributions of Y" /></p>

<p>The posterior predictive distributions agree with the observed data. We extracted some random sub-sample to
provide an estimate of the uncertainties.</p>

<p>We can finally verify if there is any effect:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">trace_did</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'beta_gt'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/difference_in_difference/effect_estimate.webp" alt="Our estimate for the minimum wage increase effect
" /></p>

<p>As you can see, the effect is compatible with 0, therefore there is no evidence
that by increasing the minimum salary there is an effect on the occupation.</p>

<p>Our model has a small issue: it allows for negative values of the occupation,
which doesn’t make sense. This problem can be easily circumvented by using 
the <a href="https://www.pymc.io/projects/docs/en/v4.4.0/api/distributions/generated/pymc.Truncated.html">truncated PyMC class</a>.</p>

<p>I suggest you to try it and verify yourself if there is any effect.
Remember that in that case $\mu$ is no more the mean for $Y$,
so you can’t use it to estimate the average effect.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We have seen how to implement the DiD method with PyMC, and we used to
re-analyze the Krueger and Card article on the relation between the minimum
salary and the occupation.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/causal_intro/" /><summary type="html"><![CDATA[Causal inference from 1850]]></summary></entry><entry><title type="html">Instrumental variable regression</title><link href="http://localhost:4000/statistics/instrumental_variable" rel="alternate" type="text/html" title="Instrumental variable regression" /><published>2024-02-09T00:00:00+00:00</published><updated>2024-02-09T00:00:00+00:00</updated><id>http://localhost:4000/statistics/instrumental_variable</id><content type="html" xml:base="http://localhost:4000/statistics/instrumental_variable"><![CDATA[<p>In many circumstances you cannot randomize, either because it is unethical
or simply because it’s too expensive.
There are however methods which, if appropriately applied, may provide
you some convincing causal evidence.</p>

<p>Let us consider the case where you cannot randomly assign the treatment $T\,,$
and in this case it could be affected by any confounder $X$
leading you to a biased estimate of the treatment effect.
However, if you have a variable $Z$ that only affects $T$
and does not affect your outcome in any other way other than via $T\,,$
than you can apply <strong>Instrumental Variable Regression</strong>.</p>

<p><img src="/docs/assets/images/statistics/instrumental_variable/causal_structure.webp" alt="The assumed causal flow" /></p>

<p>Of course, the above causal assumption is quite strong, but it holds
in quite a good approximation in some circumstance.</p>

<p>This method has been applied to analyze the effect of school years ($T$)
on earning ($Y$).
In this case the variable $Z$ was the assignment of some economical assistance
(a voucher) to go to school.</p>

<p>One would be tempted to simply use linear regression to fit this model:</p>

\[Y = \alpha + \beta T + \gamma Z + \varepsilon\]

<p>However, linear regression assumes independence between the regressors,
while in our case we have that $T$ is determined by $Z\,.$
This has an impact on the variance estimate of $Y\,,$ as we do not
correctly propagate the uncertainty due to the $T$ dependence on $Z\,.$
In fact, linear regression always predicts homoscedastic variance,
while IV can also reproduce heteroscedasticity.</p>

<h2 id="application-to-the-cigarettes-sales">Application to the cigarettes sales</h2>

<p>We will use IV to see if an increase in the cigarettes price ($T$)
causes a decrease in the cigarettes sales ($Y$), and we will use the
tobacco taxes as instrumental variable $Z$.
In order to linearize the dependence between the variables,
instead of the value of each quantity, we will consider the
difference between the 1995 log value and the 1985 log value.</p>

\[\begin{pmatrix}
T \\
Y \\
\end{pmatrix}
\sim 
\mathcal{t}
\left(
\left(
\alpha_0 + \beta_0 Z
\atop
\alpha_1 + \beta_1 T
\right),
\Sigma, \nu
\right)\]

<p>where $t$ represents the 2 dimensional Student-T distribution and $\Sigma$ is the $2\times2$ covariance matrix.
If $Z$ has a causal effect on $Y$ via $T\,,$ then the correlation
between $Y$ and $T$ is different from zero.</p>

<p>We will assume</p>

\[\alpha_i, \beta_i \sim \mathcal{N}(0, 10^3)\]

<p>and</p>

\[\nu \sim \mathcal{HalfNormal}(100)\]

<p>\(\Sigma\) must be a positive semi-defined matrix, and an easy way to
provide it a prior is using the
<a href="https://en.wikipedia.org/wiki/Lewandowski-Kurowicka-Joe_distribution">Lewandowski-Kurowicka-Joe distribution
</a>.
This distribution takes a shape parameter $\eta\,,$
and we will take $\eta=1\,,$ which implies that we will take a uniform
prior over $[-1, 1]$ for the correlation matrix.
We will moreover assume that the standard deviations are distributed according to</p>

\[\sigma_i \sim \mathcal{HalfCauchy}(20)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">pytensor.tensor.extra_ops</span> <span class="kn">import</span> <span class="n">cumprod</span>
<span class="kn">import</span> <span class="nn">pymc.sampling_jax</span> <span class="k">as</span> <span class="n">pmjx</span>

<span class="n">random_seed</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">df_iv</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/AER/CigarettesSW.csv'</span><span class="p">)</span>

<span class="n">X_iv</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1995</span><span class="p">][</span><span class="s">'price'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'price'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
       <span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'price'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
<span class="n">Y_iv</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1995</span><span class="p">][</span><span class="s">'packs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'packs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
       <span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'packs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
<span class="n">Z_iv</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1995</span><span class="p">][</span><span class="s">'taxs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'taxs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
       <span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_iv</span><span class="p">[</span><span class="n">df_iv</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span><span class="o">==</span><span class="mi">1985</span><span class="p">][</span><span class="s">'taxs'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">instrumental_variable</span><span class="p">:</span>
    <span class="n">sd_dist</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfCauchy</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">20.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'nu'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">100.0</span><span class="p">)</span>
    <span class="n">chol</span><span class="p">,</span> <span class="n">corr</span><span class="p">,</span> <span class="n">sigmas</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">LKJCholeskyCov</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sd_dist</span><span class="o">=</span><span class="n">sd_dist</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">Z_iv</span><span class="p">,</span> <span class="n">X_iv</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">X_iv</span><span class="p">,</span> <span class="n">Y_iv</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">w</span><span class="p">)</span>  <span class="c1"># so we will recover it easily
</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">MvStudentT</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">chol</span><span class="o">=</span><span class="n">chol</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_iv</span><span class="p">)),</span> <span class="n">observed</span><span class="o">=</span><span class="n">u</span><span class="p">)</span>
    <span class="c1"># We directly compute the posterior predictive
</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">MvStudentT</span><span class="p">(</span><span class="s">'y_pred'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">chol</span><span class="o">=</span><span class="n">chol</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>

<span class="k">with</span> <span class="n">instrumental_variable</span><span class="p">:</span>
    <span class="n">trace_instrumental_variable</span> <span class="o">=</span> <span class="n">pmjx</span><span class="p">.</span><span class="n">sample_numpyro_nuts</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_instrumental_variable</span> <span class="p">,</span>

              <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">,</span> <span class="s">'beta'</span><span class="p">,</span> <span class="s">'sigma'</span><span class="p">,</span> <span class="s">'nu'</span><span class="p">],</span>
              <span class="n">coords</span><span class="o">=</span><span class="p">{</span><span class="s">'sigma_corr_dim_0'</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s">'sigma_corr_dim_1'</span><span class="p">:</span><span class="mi">1</span><span class="p">})</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/instrumental_variable/trace.webp" alt="The trace plot of the above model" /></p>

<p>As we can see, there is no signal of problems in thee trace plot.</p>

<p>A few remarks on the above code. Since the model is not very fast,
we used the numpyro sampler, which hundred of times
faster than the standard PyMC sampler.
Moreover, we instructed arviz to only plot the off-diagonal elements
of the correlation matrix. We must do this because the diagonal elements
are always one, as they must be, but this causes an error in arviz
(which assumes a random behavior in all the variables of the trace).</p>

<p>We can now verify the posterior predictive distribution.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a0</span> <span class="o">=</span> <span class="n">trace_instrumental_variable</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">])[</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span>
<span class="n">b0</span> <span class="o">=</span> <span class="n">trace_instrumental_variable</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'beta'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">])[</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span>

<span class="n">x_min</span> <span class="o">=</span> <span class="mf">0.06</span>
<span class="n">x_max</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="n">x_pl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mf">0.0002</span><span class="p">)</span>

<span class="n">xiv_0</span> <span class="o">=</span> <span class="n">trace_instrumental_variable</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">2</span><span class="p">))[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">xiv_1</span> <span class="o">=</span> <span class="n">trace_instrumental_variable</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">2</span><span class="p">))[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">sampled_index</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">sampled_index</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">a0</span><span class="o">+</span><span class="n">b0</span><span class="o">*</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">sampled_index</span><span class="p">:</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xiv_0</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">xiv_1</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'x'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_iv</span><span class="p">,</span> <span class="n">Y_iv</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'steelblue'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'t'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/instrumental_variable/posterior_predictive.webp" alt="The posterior predictive distribution" /></p>

<p>Our model also looks capable to reproduce the observed data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="n">kdeplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">az</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">trace_instrumental_variable</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">"sigma_corr"</span><span class="p">])[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">"IV Model - Posterior Distribution Correlation"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$\sigma$'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">''</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/instrumental_variable/correlation.webp" alt="The off-diagonal component of the correlation matrix" /></p>

<h2 id="conclusions">Conclusions</h2>

<p>We have seen how IV allows us to make causal inference in absence of randomization,
but making some rather strong assumptions about the causal structure of the problem.
We have also seen how to implement it in PyMC.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/causal_intro/" /><summary type="html"><![CDATA[Making causal inference without randomization]]></summary></entry><entry><title type="html">Randomized controlled trials</title><link href="http://localhost:4000/statistics/randomized" rel="alternate" type="text/html" title="Randomized controlled trials" /><published>2024-02-08T00:00:00+00:00</published><updated>2024-02-08T00:00:00+00:00</updated><id>http://localhost:4000/statistics/randomized</id><content type="html" xml:base="http://localhost:4000/statistics/randomized"><![CDATA[<p>As we anticipated in the last post, when we have randomization, association
implies causation.
In this case we can use a simple regression model to assess if the treatment
causes an effect.</p>

<p>Randomized controlled trials are the golden standards in clinical studies,
but they are widely used in other fields like industry or marketing
campaigns.
Thanks to their popularity, even marketing providers such as Mailchimp allow you
to easily implement this kind of studies, and in this post we will see how
to analyze them by using Bayesian regression.
In this experiment we we will analyze the data from a newsletter, and what we will
determine is whether the presence of the first name (which is required
in the login form) in the mail preview increases the probability of opening the
email.
When we programmed the newsletter, we divided the total audience into
two blocks, and each recipient has been randomly assigned to one block.
In the control block (t=0) we sent the email without the first name in the mail
preview, while to the other recipients we sent the email with the first name
in the mail preview.</p>

<p>Some mails were bounced, but at the end $n_t = 2326$ users received the test mail
and $n_c = 2347$ received the control mail.
$y_t = 787$ users out of 2326 opened the test email, while $y_c=681$ users out
of 2347 opened the control one.</p>

<p>Since the opening action is a binary variable, we will take
a binomial likelihood.
We will therefore use a logistic regression to estimate the ATE.</p>

\[\begin{align}
&amp;
y_{c} \sim \mathcal{Binom}(p_c, n_n)
\\
&amp;
y_{t} \sim \mathcal{Binom}(p_t, n_t)
\\
&amp;
p_c = \frac{1}{1+e^{-\beta_0}}
\\
&amp;
p_t = \frac{1}{1+e^{-(\beta_0+ \beta_1)}}
\end{align}\]

<p>We will take a non-informative prior for both the parameters</p>

\[\beta_i \sim \mathcal{N}(0, 10^3)\]

<p>We can now easily implement our model in PyMC</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">random_seed</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">n_t</span> <span class="o">=</span> <span class="mi">2326</span>
<span class="n">n_c</span> <span class="o">=</span> <span class="mi">2347</span>

<span class="n">k_t</span> <span class="o">=</span> <span class="mi">787</span>
<span class="n">k_c</span> <span class="o">=</span> <span class="mi">681</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">pt</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'pt'</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]))))</span>
    <span class="n">pc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'pc'</span><span class="p">,</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]))))</span>
    <span class="n">ate</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'ate'</span><span class="p">,</span> <span class="n">pt</span><span class="o">-</span><span class="n">pc</span><span class="p">)</span>
    <span class="n">y_t</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">'y_t'</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_t</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pt</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">k_t</span><span class="p">)</span>
    <span class="n">y_c</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">'y_c'</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_c</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pc</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">k_c</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/randomized/trace.webp" alt="The trace of our model" /></p>

<p>The average treatment effect is greater than 0 with a probability
approximately equal to 1,
therefore we are almost sure that, in our test,
using the first name in the mail preview increased the opening
probability of the newsletter.</p>

<p>Notice that we restricted our discussion to one single newsletter, and we
avoided more general claims regarding future newsletters we will send.
However, we have some indication that our audience may prefer more
personal newsletters.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We saw an example of how to perform causal inference in Bayesian statistics for randomized controlled experiments
by using regression models in PyMC. We also discussed the proper interpretation of the results.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/randomized/" /><summary type="html"><![CDATA[When association implies causation]]></summary></entry><entry><title type="html">Causal inference</title><link href="http://localhost:4000/statistics/causal_intro" rel="alternate" type="text/html" title="Causal inference" /><published>2024-02-07T00:00:00+00:00</published><updated>2024-02-07T00:00:00+00:00</updated><id>http://localhost:4000/statistics/causal_intro</id><content type="html" xml:base="http://localhost:4000/statistics/causal_intro"><![CDATA[<p>In this post we will try and clarify when it is possible to make statements
about causation rather than sticking to statistical association,
and we will do so on the basis of Rubin’s potential outcomes.</p>

<p>The main reference for this part will be the material in
<a href="https://www.bradyneal.com/Introduction_to_Causal_Inference-Dec17_2020-Neal.pdf">these</a>
notes by Brady Neal, but I strongly recommend to read the textbook by Guido Imbens
(who, in 2021, shared the Nobel prize for economics with Joshua Angrist and David Card for their works on causal inference) and Carl Rubin
(who first developed the potential outcomes framework).</p>

<h2 id="the-counterfactual-definition-of-causality">The counterfactual definition of causality</h2>

<p>You may have heard the mantra “association is not causation” or the more colloquial 
(but less accurate) “correlation is not causation”.
Correlation is a statistical measure linear dependence,
while association generally means statistical dependence.
However the exact meaning of causation is never given,
and the first part of these notes will be devoted to clarify what do we mean with causation.</p>

<p>Let us assume that we took a medicine because we had a headache,
what do we mean when we say that the medicine
caused the headache to disappear? It means that, if we hadn’t taken the medicine,
the headache wouldn’t have gone away.</p>

<p>We will therefore stuck to the counterfactual definition of causation,
so we will say that an event causes an outcome if,
by removing the event then the outcome disappears.
The above definition works for binary outcomes, but has some
problems when we want to investigate causes which can take any real value.
More generally, we can say that an event causes an outcome if, by modifying
the cause, the outcome changes.
This definition already puts a strong constrain on what we can investigate,
since it requires that we must be able, at least in principle, to modify the
cause.</p>

<div class="emphbox">
There's no causation without manipulation.
</div>

<p>While the meaning of the above sentence may seem obvious at a first sight,
you should carefully think about it when making causal inference.
If you want to assess the effect of the ethnicity on the probability of being hired,
you may not be able to manipulate someone’s ethnicity,
but you can still manipulate people’s perception of ethnicity by modifying 
the CV.</p>

<p>When talking about causality, one can be either interested in the determination
of the effect of a cause (e.g. does my headache disappears when I take medicine?)
or the cause of an effect (e.g. is my headache gone because I took the medicine?).</p>

<p>Within the counterfactual framework, and in science in general, one generally wants
to assess the effect of a specific cause.
Determining the cause of an effect, in fact, is an ill-posed question, as one could
find a cause of the cause, a cause of the cause of the cause and so on.</p>

<p>A relevant aspect which we must keep in mind is that there could be more than one
cause. We know that, in order to light a fire, we need oxygen, heat and fuel,
and all the above are necessary conditions for fire.</p>

<p>Let’s assume that we want to assess if heat causes fire ignition,
and we perform an experiment to determine it.
If we first provide both three the elements,
and we then remove oxygen and heat, we can’t conclude anything about the
causal relation between heat and fire, since we also removed the oxygen.
The counterfactual definition of causality requires that
only the cause must change, while all the other elements must be unchanged.</p>

<h2 id="potential-outcomes">Potential outcomes</h2>

<p>But how can we measure the effect of an event? Let us indicate with $T=1$
the case where the event happens, as an example we take a therapy,
while $T=0$ means that we do not take the therapy.
Suppose that the outcome of the event $T=0$ is $y_0$ while the outcome of $T=1$ is
$y_1\,.$ We define</p>

\[Y(t) = t y_1 + (1-t) y_0\,.\]

<p>From a counterfactual point of view, a natural way to assess the causal
effect is via the <strong>Individual Treatment Effect</strong> ITE</p>

\[\tau = Y(1)-Y(0)\]

<p>The definition of $\tau$ is of course arbitrary, but quite general.
As an example, one could prefer taking
the ratio of the two, but then taking the logarithm we recover the above definition.</p>

<p>Despite on the exact definition, $\tau$ of course cannot be measured, as either we take the treatment and
we observe $Y(1)$ or we don’t and we observe $Y(0)\,,$
and any reasonable definition of $\tau$ involves both the quantities.
This implies that,</p>

<div class="emphbox">
while our definition of effect may be individual,
its quantification can only be done on a larger sample.
</div>

<p>We must therefore do an experiment in order to estimate it:
we collect $2N$ individuals and divide them into 2 groups.
Half individuals are treated with $T=1$ (the treatment group)
and half of them with $T=0$ (the control group).</p>

<p>In order to proceed, we will stick to the definition $\tau = Y(1)-Y(0)\,.$
The simplest estimate that we may do is the <strong>Average Treatment Effect</strong> ATE</p>

\[ATE = \mathbb{E}[Y_i(1) - Y_i(0)] =  \mathbb{E}[Y(1) - Y(0)] = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]\]

<p>where the average is meant both on the individual and on any other possible source
of randomness.</p>

<p>To clarify what we are doing, we can put the collected data as</p>

<table>
  <thead>
    <tr>
      <th>i</th>
      <th>T</th>
      <th>Y</th>
      <th>Y(0)</th>
      <th>Y(1)</th>
      <th>X</th>
      <th>Y(1) - Y(0)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>$y^1$</td>
      <td>$y^1$</td>
      <td>?</td>
      <td>$x^1$</td>
      <td>?</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0</td>
      <td>$y^2$</td>
      <td>$y^2$</td>
      <td>?</td>
      <td>$x^2$</td>
      <td>?</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0</td>
      <td>$y^3$</td>
      <td>$y^3$</td>
      <td>?</td>
      <td>$x^3$</td>
      <td>?</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1</td>
      <td>$y^4$</td>
      <td>?</td>
      <td>$y^4$</td>
      <td>$x^4$</td>
      <td>?</td>
    </tr>
    <tr>
      <td>5</td>
      <td>1</td>
      <td>$y^5$</td>
      <td>?</td>
      <td>$y^5$</td>
      <td>$x^5$</td>
      <td>?</td>
    </tr>
    <tr>
      <td>6</td>
      <td>1</td>
      <td>$y^6$</td>
      <td>?</td>
      <td>$y^6$</td>
      <td>$x^6$</td>
      <td>?</td>
    </tr>
  </tbody>
</table>

<p>where $X$ represents other possibly relevant quantity where we must take
into account to estimate the averages or, in other words, any quantity we may suspect could
affect the outcome.
The estimate of the ATE is the so-called <strong>fundamental problem of causal inference</strong>,
and since the question marks can be seen as missing values,</p>

<div class="emphbox">
the fundamental problem of causal inference is a missing value problem. 
</div>

<p>We did a step further, but still we don’t know how to compute that quantity.</p>

<p>In writing the above table, we implicitly made the <strong>no interference</strong> assumptions, namely that</p>

\[Y_i(t_1, t_2, ..., t_{i-1}, t_i, t_{i+1}, ..., t_n) = Y_i(t_i)\]

<p>So each unit’s outcome only depends on his own treatment and not on the treatment of other individuals.
This implies that, if we are checking the effect of a product in some tomato field,
we must be sure that the product does not goes in another studied field by mistake.
Another case can be a study where we are studying an experimental study program in a class.
If a student is selected in the treatment group and a friend of his is not,
the latter could be sad for not being selected and his outcome could be lowered.</p>

<p>Generally, a good strategy to enforce this requirement is to take well separated units
and not letting them communicate during the experiment.</p>

<p>Notice that this is not a necessary requirement, but it greatly simplifies the discussion,
as it allows us to threat each unit independently on the others.</p>

<p>A quantity that is closely related to the ATE is the <strong>associational difference</strong></p>

\[\mathbb{E}[Y|T=1] - \mathbb{E}[Y|T=0]\]

<p>When are we allowed to replace the ATE with the associational difference?
In other words, when are we allowed to compute the average only over the observed 
values and replace the question marks with the appropriate average?</p>

<p>The assumptions that the observed data do not depend on the missing ones
is called <strong>ignorability</strong>, and it is one of the most important assumptions
in causal inference.
Ignorability can be written in mathematical language as</p>

\[Y(0), Y(1) \perp\!\!\!\!\perp T\]

<p>where \(A \perp\!\!\!\!\perp B\) means that $A$ and $B$ are independent one on
the other one.</p>

<p>If ignorability holds, we are allowed to estimate the average of $Y(0)$
by only using the $T=0$ group and replace it in the $T=1$ group and vice versa,
and this is why this assumption is often named <strong>exchangeability</strong>.</p>

<p>We can mathematically write the exchangeability assumption as</p>

\[\mathbb{E}[Y(0) | T=0] = \mathbb{E}[Y(0) | T=1] = \mathbb{E}[Y(0)]\]

<p>and</p>

\[\mathbb{E}[Y(1) | T=0] = \mathbb{E}[Y(1) | T=1] = \mathbb{E}[Y(1)]\]

<p>The above assumption is almost equivalent to <strong>identifiability</strong> assumption:
a causal quantity $\mathbb{E}[Y(t)]$ is identifiable if it can be computed from a pure statistical quantity $\mathbb{E}[Y | T=t]$.</p>

<p>There are cases where exchangeability does not hold.
As an example, assume that you are testing a medicine, and that this medicine
is more effective on patients with a severe version of the disease you are treating.
If in the $T=1$ group we have
people with a more severe version of the disease than in the $T=0$
group we may not be allowed to exchange the two groups,
as we have no guarantee that the result would be invariant under the group exchange.</p>

<p>Let us decompose the associational difference as</p>

\[\mathbb{E}[Y(1) | T=1] - \mathbb{E}[Y(0) | T=0]
=
(\mathbb{E}[Y(1) | T=1] - \mathbb{E}[Y(0) | T=1])
+(\mathbb{E}[Y(0) | T=1] - \mathbb{E}[Y(0) | T=0])\]

<p>The associational difference can be decomposed as
the average treatment effect on the treated (the first parenthesis)
plus the sampling bias (the second parenthesis).</p>

<p>Consider the case where $Y$ is the health status of a person and the treatment is
the hospitalization.
The associational difference is simply the difference between the health status
of hospitalized patients and the health status of non-hospitalized people.
This is simply the sum between the effect of the hospitalization on hospitalized
patients plus the baseline health difference between hospitalized and non-hospitalized individuals.
In general, even if hospitalization improves health, the health status of those who go to
the hospital is generally worst than the other individuals.
Therefore, in absence of randomization, if we simply use the associational difference to assess the effect of
hospitalization, we may end up with the conclusion that health get worst due to hospitalization
simply because only sick people goes to the hospital.</p>

<p>If exchangeability does not hold, then \(\mathbb{E}[Y \vert T=0]\) is different from
\(\mathbb{E}[Y \vert T=1]\,,\) therefore the associational quantity \(\mathbb{E}[Y \vert T=t]\) is a biased estimator for \(\mathbb{E}[Y(t)]\,.\)</p>

<p>One possible way to ensure that exchangeability holds is to ensure that the missing
terms are randomly distributed.
This can be experimentally done by randomly assigning the
treatment $T$ to each unit, and in this case we are dealing with a randomized experiment.</p>

<p>In a randomized experiment, the treatment assignment does not depend on anything other
other than the result of a coin toss, therefore</p>

\[\mathbb{E}[Y(1)]-\mathbb{E}[Y(0)] = \mathbb{E}[Y(1)|T=1]-\mathbb{E}[Y(0)|T=0] = \mathbb{E}[Y | T=1]-\mathbb{E}[Y | T=0]\]

<p>We stress that this is only a statistical property, and it doesn’t guarantee that the outcome
estimate of an experiment will be correct.</p>

<p>In other words, as explained in <a href="http://www.fsb.muohio.edu/lij14/420_paper_Rubin74.pdf">the breaktrhough 1974 Rubin paper</a>:</p>
<blockquote cite="https://hedibert.org/wp-content/uploads/2015/10/causality-meeting2.pdf">
Whether treatments are randomly assigned or not, no matter how carefully
matched the trials, and no matter how large N, a skeptical observer could always
eventually find some variable that systematically differs in the E (T=1) trials and C (T=0) trials.
<br />
Within the experiment there can be no refutation of this claim; only a logical
argument explaining that the variable cannot causally affect the dependent
variable or additional data outside the study can be used to counter it.
</blockquote>

<p>Generally exchangeability is an unrealistic assumption, as it would impossible to verify
that $X$ and $Y$ are equally distributed with respect to all the 
relevant variables except for the treatment.
A weaker assumption is that the assigned treatment only depends on some
relevant quantity $X$ while the two groups are exchangeable
with respect to any other quantity.
This condition is called <strong>conditional exchangeability</strong> or <strong>unconfoundedness</strong> and it is indicated as</p>

\[Y(0), Y(1) \perp\!\!\!\!\perp T | X\]

<p>If conditional exchangeability holds, we have that</p>

\[\begin{align}
 \mathbb{E}[Y(1)-Y(0)|X] 
 &amp; = \mathbb{E}[Y(1)|X] - \mathbb{E}[Y(0)|X] \\
 &amp; = \mathbb{E}[Y(1)| T=1, X] - \mathbb{E}[Y(0)|T=0, X] \\
 &amp; = \mathbb{E}[Y| T=1, X] - \mathbb{E}[Y|T=0, X] \\
 \end{align}\]

<p>In order to get the ATE we must simply take the expectation value over $X$</p>

\[\mathbb{E}[Y(1) - Y(0)] = \mathbb{E}_X[ \mathbb{E}[Y(1) - Y(0) | X] ] 
 = \mathbb{E}_X[ \mathbb{E}[Y |T=1, X] ] - \mathbb{E}_X[ \mathbb{E}[Y |T=0, X] ]\]

<p>And the equality between the first and the last term of this equation is called the <strong>adjustment formula</strong>.</p>

<p>In the above equation we assumed <strong>consistency</strong>, which can be written as</p>

\[T=t \Longrightarrow Y(T) = Y(t)\]

<p>This means that the treatment must be well specified: the treatment must not be “get some medicine” but should rather be “take 15 mg of medicine every 8 hours for 7 days”.
Only thanks to this hypothesis we can replace</p>

\[\mathbb{E}[Y(T=t) | T=t] = \mathbb{E}[Y | T=t] \,.\]

<p>This is not a necessary requirement, but it greatly simplifies the discussion, otherwise we would be forced to model
this random aspect too.
Notice that the concept of consistency is not a mathematical requirement, but rather a conceptual one,
and only agreement among domain experts can assess whether it holds or not.</p>

<p>In the literature it is often required the <strong>Stable Unit Treatment Value Assumption</strong> SUTVA, which is simply requiring consistency and no interference.</p>

<p>Let us now write explicitly the adjustment formula for $X$ discrete:</p>

\[\begin{align}
&amp;
\mathbb{E}_X[ \mathbb{E}[Y |T=1, X] ] - \mathbb{E}_X[ \mathbb{E}[Y |T=0, X] ]  
\\
&amp; =  \sum_{x}P(X=x) \sum_{y} y \left(P(Y=y|T=1, X=x) - P(Y=y| T=0, X=x) \right) \\
&amp; =   \sum_{x}P(X=x) \sum_{y} y \left(\frac{P(Y=y,T=1, X=x)}{P(T=1, X=x)} - \frac{P(Y=y,T=0, X=x)}{P(T=0, X=x)}\right) \\
= &amp; \sum_{x}P(X=x) \sum_{y} y \left(\frac{P(Y=y,T=1, X=x)}{P(T=1| X=x) P(X=x)} - \frac{P(Y=y,T=0, X=x)}{P(T=0| X=x) P(X=x)}\right) 
\\
&amp; = 
\sum_{x}\sum_{y} y \left(\frac{P(Y=y,T=1, X=x)}{P(T=1| X=x)} - \frac{P(Y=y,T=0, X=x)}{P(T=0| X=x)}\right) 
\end{align}\]

<p>where the first equivalence comes from the definition of conditional probability,
the second one from the hypothesis that \(P(T, X) = P(T | X) P(X)\)
so that $T$ causally depends on $X\,.$</p>

<p>In order for this quantity to be finite we must require that both the denominators are
strictly positive, and since $P(T=0|X) = 1 - P(T=1, X)$ we can write this requirement,
named the <strong>positivity</strong> assumption, as</p>

\[0 &lt; P(T=t | X) &lt; 1 \, \forall t\]

<p>In other words, for each value of X we must have some representative individual in
each group.
If, for some group, everyone receives the treatment or everyone receives the control,
we are not able to estimate the treatment versus control effect.</p>

<!--
We can better see why randomization is important from the adjustment formula.
If the treatment assignment is not random, one cannot assume that $P(X)$ is the same for the two
groups, and therefore one should replace it with $P(X=x | T=t)\,.$
A careful observer could therefore always find some confounder which could differ
between the two groups, and claim that the difference in the effect is due to the
different distribution of that confounder.
Within randomization, this cannot happen, as the probability distribution
of $X$ is independent on $T\,.$
-->

<h2 id="some-remarks">Some remarks</h2>

<p>Our discussion on causality both applies to frequentist statistics
and to bayesian one. However, as pointed out by Rubin himself in his 1990
article “Formal mode of statistical inference for causal effects”,
it is straightforward to apply fully bayesian methods to causal inference.
However, it is very easy to misuse it, as</p>

<blockquote cite="https://www.sciencedirect.com/science/article/abs/pii/0378375890900778">
there appears to be no formal requirement to make sure
that the models conform at all to reality. 
In practice, careful model monitoring is
needed, and for this purpose, the randomization-based approaches we have presented
can be regarded as providing useful guidelines.
</blockquote>

<p>It is therefore crucial both to ensure that experimental setup fulfills the
above mentioned assumptions and that the statistical model is appropriate
in describing it.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We introduced the counterfactual definition of causality, and we introduced
Rubin’s potential outcomes. We also discussed under which conditions
we can compute the average treatment effect.</p>

<h2 id="suggested-readings">Suggested readings</h2>

<ul>
  <li><cite>Imbens, G. W., Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. US: Cambridge University Press.<cite></cite></cite></li>
  <li><cite><a href="https://arxiv.org/pdf/2206.15460.pdf">Li, Ding, Mealli (2022). Bayesian Causal Inference: A Critical Review</a></cite></li>
</ul>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/causal_intro/" /><summary type="html"><![CDATA[When association implies causation]]></summary></entry></feed>