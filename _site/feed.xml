<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-08-18T07:50:59+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Just another Bayesian enthusiast</title><author><name>Stippe</name><email>smaurizio87@protonmail</email></author><entry><title type="html">Dealing with count data</title><link href="http://localhost:4000/count_data/" rel="alternate" type="text/html" title="Dealing with count data" /><published>2023-08-14T00:00:00+02:00</published><updated>2023-08-14T00:00:00+02:00</updated><id>http://localhost:4000/poisson</id><content type="html" xml:base="http://localhost:4000/count_data/"><![CDATA[<p>In <a href="/beta_binom/">the previous post</a> we have seen how we can model a two-valued variable
by using the binomial distribution. In this one we will illustrate how we can 
model count variables, that is variables which can take any non-negative
integer value $0, 1, 2,\dots$</p>

<h2 id="the-poisson-model">The Poisson model</h2>

<p>A classical example is given by the number of hurricanes in the North Atlantic ocean by year.
The corresponding dataset can be downloaded from
 <a href="https://ourworldindata.org/grapher/frequency-north-atlantic-hurricanes">https://ourworldindata.org/grapher/frequency-north-atlantic-hurricanes</a>,
 and we saved the downloaded dataset in the data subfolder.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">poisson</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="nf">use</span><span class="p">(</span><span class="sh">"</span><span class="s">seaborn-v0_8-darkgrid</span><span class="sh">"</span><span class="p">)</span>

<span class="n">df_hurricanes</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">data/frequency-north-atlantic-hurricanes.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_hurricanes</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">Number of US Hurricanes (HUDRAT, NOAA)</span><span class="sh">"</span><span class="p">,</span>
                <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/hurricanes_count.jpg" alt="Hurricanes count" /></p>

<p>We can assume that the number of hurricanes for each year is independent
on the number of hurricanes of the other years and that the average
number of hurricanes per year is constant.
This suggests us to use the Poisson distribution.</p>

\[y \sim Poisson(\mu)\]

<p>where $\mu$ is the average, and the Poisson distribution has probability mass function
given by</p>

\[P(y | \mu) = \frac{e^{-\mu} \mu^y}{ y! }\]

<p><img src="/docs/assets/images/poisson_example.jpg" alt="Poisson example" />
<em>The Poisson distribution</em></p>

<p>and $\mu$ must be a positive real quantity.</p>

<p>As usual, we must now provide a prior for the parameter $\mu\,,$
and our prior must be able to easily accommodate the data.
A flexible enough family of distributions is given by the Gamma distribution:</p>

\[\mu \sim Gamma(\alpha, \beta)\]

<p>where the Gamma distribution has pdf</p>

\[P(y | \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} y^{\alpha - 1}  e^{-\beta y}\]

<p>A special case of the Gamma distribution is the Exponential distribution,
which corresponds to $Gamma(1, \lambda)\,,$ and $\lambda$ is the inverse of the mean.</p>

<p><img src="/docs/assets/images/gamma_example.jpg" alt="Gamma example" />
<em>The gamma distribution</em></p>

<p>We don’t want to use our prior to force the result toward a particular
region, we’d rather prefer to use our prior to <strong>regularize</strong> our estimate.
So we will use a weakly informative prior:</p>

\[\mu \sim Gamma(1, 1/10)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_obs</span> <span class="o">=</span> <span class="n">df_hurricanes</span><span class="p">[</span><span class="sh">"</span><span class="s">Number of US Hurricanes (HUDRAT, NOAA)</span><span class="sh">"</span><span class="p">].</span><span class="nf">dropna</span><span class="p">().</span><span class="n">values</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_hurricanes</span><span class="p">:</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Gamma</span><span class="p">(</span><span class="sh">'</span><span class="s">mu</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Poisson</span><span class="p">(</span><span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_obs</span><span class="p">)</span>
    <span class="n">trace_hurricanes</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span> 

</code></pre></div></div>

<p>We used PyMC to simulate 4 chains for $\mu\,,$ each chain contains
2000+500 draws, and the first 500 draws are discarded.
This is because the initial part of the simulation
may depend on the initial value and it is used by PyMC to infer the
best parameters for the simulation (we will discuss this more in detail in
a future post).</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_hurricanes</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/trace_hurricanes.jpg" alt="Hurricanes traces" /></p>

<p>The four traces have a stationary appearance, and it looks like
the distribution is the same across the traces.
These are good signals that we had no issues during the simulations.
We can now take a look at the most important statistics regarding the trace on $\mu$:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">summary</span><span class="p">(</span><span class="n">trace_hurricanes</span><span class="p">)</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>mu</td>
      <td>1.748</td>
      <td>0.102</td>
      <td>1.566</td>
      <td>1.951</td>
      <td>0.002</td>
      <td>0.001</td>
      <td>3171</td>
      <td>5132</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>The results show that:</p>
<ul>
  <li>The samples have a mean of 1.748 and a standard deviation of 0.102</li>
  <li>The $95\%$ Credible Interval has lower boundary 1.566 and upper boundary 1.951</li>
  <li>The uncertainty in the estimate of the mean due to the Monte Carlo procedure (Monte Carlo Standard Error) has been estimated to 0.002 with an uncertainty of 0.001</li>
  <li>The Effective Sample Size is of the same order of magnitude of the sample size (3000-5000 against 4000)</li>
  <li>$\hat{R} = 1$ indicates that the four traces show similar properties, and this is another indicator that there are no issues in the simulation.</li>
</ul>

<p>Now that we are quite confident about the sampling procedure, we can compare
the prediction of our model for the distribution of the number of hurricanes
per year with the true distribution:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model_hurricanes</span><span class="p">:</span>
        <span class="n">ppc_hurricanes</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_hurricanes</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="nf">plot_ppc</span><span class="p">(</span><span class="n">ppc_hurricanes</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/poisson_ppc.jpg" alt="Hurricanes ppc" /></p>

<p>The true values are lying well inside our error bands, and the mean estimate
is close to the observed one. We can thus consider our model satisfactory for our purposes.
We can thus use our model to make predictions.
As an example, we could ask what is the probability that, in one year, one gets
at least four hurricanes:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">ppc_hurricanes</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">&gt;=</span><span class="mi">4</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div>
<blockquote>
  <p>0.10078</p>
</blockquote>

<p>We can compare it with the raw historical estimate:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">df_hurricanes</span><span class="p">[</span><span class="sh">"</span><span class="s">Number of US Hurricanes (HUDRAT, NOAA)</span><span class="sh">"</span><span class="p">].</span><span class="nf">dropna</span><span class="p">().</span><span class="n">values</span><span class="o">&gt;=</span><span class="mi">4</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div>
<blockquote>
  <p>0.08982</p>
</blockquote>

<h2 id="the-negative-binomial-model">The negative-binomial model</h2>

<p>The Poisson model is a one-parameter model, and this implies that the
variance is uniquely determined by the mean (in fact they are equal).
This could be a too restrictive requirement, as one would like to treat them
independently. In these cases a very common choice is the negative
binomial distribution, which represents the number of subsequent failures $k$ in a
Bernoulli process with success probability $p$ before a given number of subsequent successes $r$ occur:</p>

\[P(k \vert p, r) \propto (1-p)^k p^r\]

<p>As an example, I used this model to investigate the number of retweets of a particular
twitter account:</p>

<p><img src="/docs/assets/images/twitter_data.jpg" alt="Hurricanes ppc" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_twitter</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">data/data_twitter.csv</span><span class="sh">'</span><span class="p">)</span>
<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">negbin</span><span class="p">:</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Uniform</span><span class="p">(</span><span class="sh">'</span><span class="s">p</span><span class="sh">'</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Exponential</span><span class="p">(</span><span class="sh">'</span><span class="s">n</span><span class="sh">'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">NegativeBinomial</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_twitter</span><span class="p">[</span><span class="sh">'</span><span class="s">retweets_count</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">trace_negbin</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_negbin</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/negbin_trace.jpg" alt="Hurricanes ppc" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">negbin</span><span class="p">:</span>
   <span class="n">ppc_negbin</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_negbin</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">()</span>
<span class="n">az</span><span class="p">.</span><span class="nf">plot_ppc</span><span class="p">(</span><span class="n">ppc_negbin</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">df_count</span><span class="p">[</span><span class="sh">'</span><span class="s">retweets_count</span><span class="sh">'</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
         <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/negbin_ppc.jpg" alt="Alt text" /></p>]]></content><author><name>Stippe</name><email>smaurizio87@protonmail</email></author><summary type="html"><![CDATA[In the previous post we have seen how we can model a two-valued variable by using the binomial distribution. In this one we will illustrate how we can model count variables, that is variables which can take any non-negative integer value $0, 1, 2,\dots$]]></summary></entry><entry><title type="html">Beer and the Beta-Binomial model</title><link href="http://localhost:4000/beta_binom/" rel="alternate" type="text/html" title="Beer and the Beta-Binomial model" /><published>2023-08-11T00:00:00+02:00</published><updated>2023-08-11T00:00:00+02:00</updated><id>http://localhost:4000/beta-binom</id><content type="html" xml:base="http://localhost:4000/beta_binom/"><![CDATA[<p>I love beer, and whenever I have a free day I brew. As you probably know, beer is made
with water, malt, hop and yeast. One of the most important things to do in order
to produce a good beer is to have a good quality yeast, and one of the metrics
used to quantify the goodness of the yeast is the <strong>yeast viability</strong>, which corresponds to the percentage of alive cells in your yeast.
However, measuring the viability is a time consuming process, as you must
count the number of dead and alive cells in your yeast by hand.
Because of this, often the estimate is done with small samples, and due to this
it is important to quantify the uncertainties in your estimate.
Unfortunately, most home-brew textbooks will only give you very poor models to
estimate the yeast viability, and you may get fooled by your count and think that
you are working with a good yeast while you simply overestimated the yeast viability.
If you want to know more about how to experimentally count the yeast cells,
you can take a look to <a href="https://escarpmentlabs.com/blogs/resources/crop-pray-count-yeast-counting-guide">this</a>
link, where the procedure to count the yeast cells is illustrated.</p>

<p>In the standard procedure, one has a $5\times 5$ grid and one counts the alive
cells and the death ones, where one can distinguish the cells thanks to the
Trypan Blu which will color the death cells.
A simulated example of what one will see is shown below:</p>

<p><img src="/docs/assets/images/yeast_count.jpg" alt="Alt text" /></p>

<p>Since counting all the cells would require a lot of time, one usually counts
five well separated squares, usually the four corner squares and the center one.
In the figure shown above:</p>

<table>
  <thead>
    <tr>
      <th>square</th>
      <th>alive</th>
      <th>death</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>top left</td>
      <td>16</td>
      <td>2</td>
    </tr>
    <tr>
      <td>top right</td>
      <td>17</td>
      <td>3</td>
    </tr>
    <tr>
      <td>bottom left</td>
      <td>18</td>
      <td>3</td>
    </tr>
    <tr>
      <td>bottom right</td>
      <td>11</td>
      <td>0</td>
    </tr>
    <tr>
      <td>center</td>
      <td>8</td>
      <td>1</td>
    </tr>
    <tr>
      <td><strong>total</strong></td>
      <td>70</td>
      <td>9</td>
    </tr>
  </tbody>
</table>

<p>Let us see how can we estimate the viability.
In the following, we will indicate with $n_a$ the number of alive cells (which is 70)
and with $n_d$ the number of death cells
In order to do this, let us first open our Jupyter notebook, import some libraries
and define the data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>

<span class="c1"># Let us improve the graphics a little bit
</span><span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="nf">use</span><span class="p">(</span><span class="sh">"</span><span class="s">seaborn-v0_8-darkgrid</span><span class="sh">"</span><span class="p">)</span>

<span class="n">cmap</span> <span class="o">=</span> <span class="n">sns</span><span class="p">.</span><span class="nf">color_palette</span><span class="p">(</span><span class="sh">"</span><span class="s">rocket</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># For reproducibility
</span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">alive</span> <span class="o">=</span> <span class="mi">70</span>
<span class="n">death</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">total</span> <span class="o">=</span> <span class="n">alive</span> <span class="o">+</span> <span class="n">death</span>
</code></pre></div></div>

<h2 id="the-home-brewers-textbook-way">The home-brewer’s textbook way</h2>
<p>The home-brewer’s solution is fast and simple: if we have 70
alive cells out of 79 cells, then the probability of having
and alive cell is simply</p>

\[p = \frac{n_a}{n_a + n_d}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p_naive</span> <span class="o">=</span> <span class="n">alive</span> <span class="o">/</span> <span class="n">total</span>
</code></pre></div></div>
<blockquote>
  <p>0.886</p>
</blockquote>

<p>This is a quick-and-dirty solution, it has however as a drawback that we have
no idea about what is the associated uncertainty to this number.</p>

<h2 id="the-frequentist-statistician">The frequentist statistician</h2>

<p>A frequentist statistician would first of all setup a model for this problem.
The state of each cell can take two values:</p>

\[y = 
\begin{cases}
1 \text{ (alive) } &amp; \text{ with probability } p \\
0 \text{ (death) } &amp; \text{ with probability } q=1-p
\end{cases}\]

<p>If we assume that the probability of being alive of each cell is independent on the probability of the remaining cells
of being alive and that the probability is the same for each cell, we have that the probability of finding $y$ alive cells out of $n$ total counted cells must follow a binomial distribution:</p>

\[P(y|p, n) \propto p^{y} (1-p)^{n-y}\]

<p>which can be written as</p>

\[y \sim Binomial(p, n)\]

<p>where the binomial distribution has probability mass</p>

\[P(y | p, n) = \binom{n}{y} p^y (1-p)^{n-y}\]

<p>and $ \binom{n}{y} = \frac{n!}{y!(n-y)!}$ is a multiplicative normalization factor.
Once the model is built, we want to find $p$ such that the $P(y | p, n)$ is maximum, namely the <em>Maximum Likelihood Estimator</em> or MLE for the
sake of brevity.
$P(y | p, n)$ is a positive quantity for $p \in (0, 1)$, and this allows us to take its logarithm, which is a monotone increasing function, and 
this implies that the maximum of $\log P$ is the maximum of $P\,.$</p>

\[\log P(y | p, n) \propto y \log p + (n-y) \log(1-p)\]

\[\frac{\partial \log P(y | p, n)}{\partial p} = \frac{y}{p} + \frac{n-y}{\hat{p}-1}\]

\[\left. \frac{\partial \log P(y | p, n)}{\partial p}\right|_{\hat{p}} = 0 \Rightarrow \frac{y}{\hat{p}} = \frac{n-y}{1-\hat{p}} \Rightarrow \hat{p}(n-y) = (1-\hat{p}) y
\Rightarrow \hat{p} n = y\]

<p>Which gives us, again, $\hat{p} = \frac{y}{n}$</p>

<p>The frequentist statistician, however, knows that his estimate for the alive cell
fraction is not exact, and he would like to provide an uncertainty interval
associated to the estimate.
He can use the central limit theorem, which says that, if $n$ is large, then the binomial distribution can be approximated with the normal distribution
with the same mean and variance of the binomial distribution, which corresponds to $\mu = n\hat{p}$ and $\sigma^2= n\hat{p}(1-\hat{p})\,.$
He would use this theorem to provide the $95\%$ Confidence Interval for this distribution, which is given by
\(\hat{p} \pm  z_{1-0.05/2} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}} 
 = \hat{p} \pm  1.96 \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}  = [0.81, 0.96]\)</p>

<p>where $z_{1-0.05/2}=1.96$ is the $0.975$ normal quantile.</p>

<p>The calculation is quite straightforward, but one should pay a lot of attention in giving the correct interpretation to this interval.
In the frequentist paradigm, one imagines to repeat the experiment many times, and what one can say is that, by doing this,
if the confidence interval is constructed with the procedure given above, in the $95\%$ of the repetitions it will contain the true
fraction of alive cells.
However, it doesn’t tell us anything about the confidence we have that the fraction of alive cells is in the interval $[0.81, 0.96]\,.$</p>

<p><strong>For the frequentist statistician, the probability that the true value lies inside [0.81, 0.96] is either 1 if it is inside 
or 0 if it is not inside, but he cannot say which one is correct!</strong></p>

<p>This fact is often misinterpreted, even by many researchers and data scientists.</p>

<h2 id="the-bayesian-rookie">The Bayesian rookie</h2>

<p>The Bayesian statistician would take the same likelihood for the model, however in his framework the parameter $p$ is
simply another random variable, and it is described by some other probability distribution $P(p)$ namely by the <strong>prior</strong> associated
to the parameter $p\,.$</p>

<p>$p$ can take any value between 0 and 1, but he has no preference about any value, so he assumes that $p$ is distributed
according to the uniform distribution over $[0, 1]\,.$</p>

\[p \sim Uniform(0, 1)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">beta_binom_model</span><span class="p">:</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Uniform</span><span class="p">(</span><span class="sh">'</span><span class="s">p</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">total</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">alive</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/trace_bb.jpg" alt="Alt text" /></p>

<p>He used PyMC to sample $p$ many times according to its posterior probability distribution,
obtained by using the Bayes theorem</p>

\[P(p | y, n) \propto P(y | p, n) P(p)\]

<p>and the sampled values are those shown in the figure.
The details about how does PyMC’s sampler works will be explained in a future post,
as well as the main methods to exclude problems in the sampling procedure.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_posterior</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/posterior_bb.jpg" alt="Alt text" /></p>

<p>We can see that the mean is very close to the MLE value, and the (Bayesian)
$95\%$ CI (which corresponds to the two printed numbers) is close to
 the frequentist one too.
However in this case the interpretation is straightforward:</p>

<p><strong>the Bayesian statistic simply updated his/her initial guess for $p$ by means of Bayes’ theorem.</strong></p>

<p>For the Bayesian statistician there is the $95\%$ of chance that the true
value of $p$ lies inside the $95\%$ CI associated to $p$.</p>

<p>Another major advantage of the Bayesian approach is that we did not had to rely
on the Central Limit Theorem, which only holds if the sample is large enough.
The Bayesian approach is always valid, regardless on the size of the sample.</p>

<h2 id="the-wise-bayesian">The wise Bayesian</h2>

<p>The wise Bayesian would follow an analogous procedure, he would however
take the less informative prior as possible, where a strongly informative
prior is a prior which influences a lot the posterior probability distribution.
The uniform distribution is not a very informative distribution.
However, as we will show, we can even choose a less informative prior, namely
the <strong>Jeffreys’ prior</strong> for the binomial distribution</p>

\[p \sim Beta(1/2, 1/2)\]

<p>where the Beta has pdf</p>

\[P(p | \alpha, \beta) = \frac{1}{B(\alpha, \beta) } p^\alpha (1-p)^\beta\]

<p>and $B(x, y)$ is the Beta function.
However, he knows he knows he must pay a lot of attention, as often -but not
in this case- the Jeffreys’ prior is not a proper prior
(it cannot be integrated to one) <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">beta_binom_model_wise</span><span class="p">:</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Beta</span><span class="p">(</span><span class="sh">'</span><span class="s">p</span><span class="sh">'</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">total</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">alive</span><span class="p">)</span>
    <span class="n">trace_wise</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_wise</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/trace_bb_wise.jpg" alt="Alt text" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_posterior</span><span class="p">(</span><span class="n">trace_wise</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/posterior_bb_wise.jpg" alt="Alt text" /></p>

<p>As we can see, this result is almost identical to the previous one.
In the Bayesian framework one can, and should, investigate the goodness
of his results by trying out different priors and assess how
much does the results on his/her inference depend on the choice of the priors.</p>

<h2 id="conclusions-and-take-home-message">Conclusions and take-home message</h2>

<ul>
  <li>PyMC allows you to easily implement Bayesian models</li>
  <li>In many cases Bayesian statistics offers results which are more transparent than their frequentist counterparts. We have seen this for a very simple model, but this becomes even more evident as the complexity of the model grows.</li>
  <li>You can apply Bayesian statistics to any kind of problem, even home-brewing!</li>
</ul>

<p>In the <a href="/count_data/">next</a> example we will apply Bayesian statistics to study
data which can take more than two values.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>This topic will be discussed in a future post. For the moment, if you are curious, you can take a look at the <a href="https://en.wikipedia.org/wiki/Jeffreys_prior#">Wikipedia</a> page. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Stippe</name><email>smaurizio87@protonmail</email></author><summary type="html"><![CDATA[I love beer, and whenever I have a free day I brew. As you probably know, beer is made with water, malt, hop and yeast. One of the most important things to do in order to produce a good beer is to have a good quality yeast, and one of the metrics used to quantify the goodness of the yeast is the yeast viability, which corresponds to the percentage of alive cells in your yeast. However, measuring the viability is a time consuming process, as you must count the number of dead and alive cells in your yeast by hand. Because of this, often the estimate is done with small samples, and due to this it is important to quantify the uncertainties in your estimate. Unfortunately, most home-brew textbooks will only give you very poor models to estimate the yeast viability, and you may get fooled by your count and think that you are working with a good yeast while you simply overestimated the yeast viability. If you want to know more about how to experimentally count the yeast cells, you can take a look to this link, where the procedure to count the yeast cells is illustrated.]]></summary></entry><entry><title type="html">Why (and when) should you go for Bayesian</title><link href="http://localhost:4000/2023/08/09/intro-when-go-bayesian.html" rel="alternate" type="text/html" title="Why (and when) should you go for Bayesian" /><published>2023-08-09T00:00:00+02:00</published><updated>2023-08-09T00:00:00+02:00</updated><id>http://localhost:4000/2023/08/09/intro-when-go-bayesian</id><content type="html" xml:base="http://localhost:4000/2023/08/09/intro-when-go-bayesian.html"><![CDATA[<p>I feel quite a pragmatic person, so I think that one should choose the tool depending on the needs rather than by relying on some personal believes.
Bayesian statistics allows to build custom and structured models by simply specifying the data generating process.
The model can be divided into two parts:</p>
<ul>
  <li>The likelihood $P(y \vert \theta)$, which determines how the data we want to model $y$ are generated given the parameter(s) $\theta$.</li>
  <li>The priors $P(\theta)$, which specifies our hypothesis about the distribution of the parameters of the model.</li>
</ul>

<p>The only mathematical requirements for both the likelihood and for the priors is that they are non-negative and sum up to one.
There is a huge literature about the model building, and one can easily start by using one of the already available models and adapt it
to the problem under study.</p>

<p>Once that the model is built one samples the entire posterior probability distribution,
which is determined by means of Bayes theorem.</p>

\[P(\theta \vert y) = \frac{P(y \vert \theta) P(\theta)}{P(y)} \propto  P(y \vert \theta) P(\theta)\]

<p>Here $\propto$ means proportional to, which means equal up to some irrelevant
multiplicative positive constant, which is the inverse of the probability of the data
$P(y)\,.$</p>

<p>This makes Bayesian statistics very attractive if you are building a statistical
model to make a decision, as you can easily make inference about any kind of quantity
regarding your model, while this is not true if you only have a point estimate 
or an interval estimate, as it happens in Machine Learning.</p>

<p>Moreover, Bayesian statistics is easily interpretable: what you are doing
is simply to use the data to update your initial believes.</p>

<p>So why is not everyone using it? There are many reasons for this, some of them
are historical, other are more pragmatic.</p>

<p>On the one hand, the possibility to easily implement a numerical simulation
and to run it within a reasonable amount of time is relatively recent and not
yet spread outside the statistical community. However, things are changing,
as the misuse and misinterpretation of tools of frequentist statistics is considered 
one of the main reason for the so-called <em>reproducibility crisis in science</em>
and a <strong>proper</strong> use of Bayesian methods is considered a valid alternative to those
tools <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> <sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> <sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>.</p>

<p>The major practical drawback of Bayesian statistics is that you need to run your
simulation thousands of times, and this may take some time if the number
of parameters in your model is large.
Thus, I do not reccomend you to use Bayesian statistics if your task is a simple
and fast fit-predict problem.</p>

<p>There is another drawback, and this is where these notes come into play:
building a model without a basic knowledge about model building and assessment
is not an easy task. There are many beautiful online courses about Bayesian
statistics in R, but either you already know R, or you need to learn Bayesian
statistics <em>and</em> R.
This blog is written to make this task simpler for anyone who has a basic knowledge
about Python, and since Python is the most widely spread programming language
in the World, I hope I will help a lot of people.</p>

<p>I hope you enjoyed,</p>

<p>Stefano</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>This is somehow a misleading name, as this crisis is not only affecting academia, but it is a problem in industry too. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>See <a href="https://www.nature.com/articles/533452a">this article on Nature</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>See <a href="https://www.nature.com/articles/520612a">this other article of Nature</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>Of course, Bayesian statistics can be misused too, but there are few very clear guidelines from the academic community which will make this less likely to happen. Moreover, in most cases, a problem in your model will show up in a problem in your simulation, and this makes Bayesian inference less error-prone than frequentist inference. In fact, when talking about frequentist statistics or machine learning, most of the time what you are computing is either an optimization problem or the average of some quantity, and an eventual problem in this kind of task may be quite hard to spot. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Stippe</name><email>smaurizio87@protonmail</email></author><summary type="html"><![CDATA[I feel quite a pragmatic person, so I think that one should choose the tool depending on the needs rather than by relying on some personal believes. Bayesian statistics allows to build custom and structured models by simply specifying the data generating process. The model can be divided into two parts: The likelihood $P(y \vert \theta)$, which determines how the data we want to model $y$ are generated given the parameter(s) $\theta$. The priors $P(\theta)$, which specifies our hypothesis about the distribution of the parameters of the model.]]></summary></entry></feed>