<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-11-21T19:49:51+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Data Perspectives</title><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><entry><title type="html">Poisson regression</title><link href="http://localhost:4000/statistics/poisson_regression" rel="alternate" type="text/html" title="Poisson regression" /><published>2024-11-20T00:00:00+00:00</published><updated>2024-11-20T00:00:00+00:00</updated><id>http://localhost:4000/statistics/poisson_regression</id><content type="html" xml:base="http://localhost:4000/statistics/poisson_regression"><![CDATA[<p>In the last post we introduced the Generalized Linear Models,
and we explained how to perform regression on data types which are
not appropriate for a Gaussian likelihood.
We also saw a concrete example of logistic regression, and here we will
discuss another type of GLM, the Poisson regression.</p>

<h2 id="poisson-regression">Poisson regression</h2>

<p>In the Poisson regression one assumes that</p>

\[Y_i \sim \mathcal{Poisson}(\theta_i)\]

<p>where $\theta_i$ must be a non-negative variable. One can 
use the exponential function to map any real number on the
positive axis, we therefore assume that</p>

\[\theta_i = \exp\left(\alpha + \beta X_i\right)\]

<p>We will use this model to estimate the average number of
bear attacks in North America.
The original data can be found on this <a href="https://data.world/ajsanne/north-america-bear-killings/workspace/file?filename=north_america_bear_killings.csv">data.world
</a>
page, where there are listed all human killing by a black, brown, or polar bear from 1900-2018 in North America.
We will limit ourself to black and brown bears, as attacks by polar bears are very rare.
We will also limit our dataset to the years after 1999, as we want to assume that the attack probability
is constant within the entire time range, and we will neglect attacks by captive animals.
We want to assess the attack probability by bear type, and to do this we will use the bear type
as a regressor.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./data/north_america_bear_killings.csv'</span><span class="p">)</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">df_red</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">groupby</span><span class="p">([</span><span class="s">'Year'</span><span class="p">,</span> <span class="s">'Type'</span><span class="p">,</span> <span class="s">'Type of bear'</span><span class="p">]).</span><span class="n">count</span><span class="p">().</span><span class="n">reset_index</span><span class="p">()[[</span><span class="s">'Year'</span><span class="p">,</span> <span class="s">'Type'</span><span class="p">,</span> <span class="s">'Type of bear'</span><span class="p">,</span> <span class="s">'Hikers'</span><span class="p">]]</span>

<span class="n">df_clean</span> <span class="o">=</span> <span class="n">df_red</span><span class="p">[(</span><span class="n">df_red</span><span class="p">[</span><span class="s">'Year'</span><span class="p">]</span><span class="o">&gt;=</span><span class="mi">2000</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df_red</span><span class="p">[</span><span class="s">'Type of bear'</span><span class="p">]</span> <span class="o">!=</span> <span class="s">'Polar Bear'</span><span class="p">)</span><span class="o">&amp;</span> <span class="p">(</span><span class="n">df_red</span><span class="p">[</span><span class="s">'Type'</span><span class="p">]</span> <span class="o">!=</span> <span class="s">'Captive'</span><span class="p">)]</span>

<span class="n">df_fit</span> <span class="o">=</span> <span class="n">df_clean</span><span class="p">.</span><span class="n">set_index</span><span class="p">([</span><span class="s">'Year'</span><span class="p">,</span> <span class="s">'Type of bear'</span><span class="p">]).</span><span class="n">unstack</span><span class="p">(</span><span class="n">fill_value</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">stack</span><span class="p">().</span><span class="n">reset_index</span><span class="p">()[[</span><span class="s">'Year'</span><span class="p">,</span> <span class="s">'Type of bear'</span><span class="p">,</span> <span class="s">'Hikers'</span><span class="p">]]</span>

<span class="n">df_fit</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'Hikers'</span><span class="p">:</span> <span class="s">'Count'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df_fit</span><span class="p">[</span><span class="s">'is_black'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_fit</span><span class="p">[</span><span class="s">'Type of bear'</span><span class="p">].</span><span class="nb">str</span><span class="p">.</span><span class="n">contains</span><span class="p">(</span><span class="s">'Black'</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">poisson_regr</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">df_fit</span><span class="p">[</span><span class="s">'is_black'</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Poisson</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">z</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_fit</span><span class="p">[</span><span class="s">'Count'</span><span class="p">])</span>

<span class="k">with</span> <span class="n">poisson_regr</span><span class="p">:</span>
    <span class="n">idata</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span>
                     <span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/poisson_glm/trace.webp" alt="The trace of the Poisson model" /></p>

<p>The trace seems fine, we can now verify if the model is compatible with the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">poisson_regr</span><span class="p">:</span>
    <span class="n">y_brown</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Poisson</span><span class="p">(</span><span class="s">'y_brown'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">y_black</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Poisson</span><span class="p">(</span><span class="s">'y_black'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">alpha</span><span class="o">+</span><span class="n">beta</span><span class="p">))</span>
    
<span class="k">with</span> <span class="n">poisson_regr</span><span class="p">:</span>
    <span class="n">idata</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y'</span><span class="p">,</span> <span class="s">'y_brown'</span><span class="p">,</span> <span class="s">'y_black'</span><span class="p">],</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">))</span>
<span class="c1"># Let us estimate the probability that, in one year, there are k brown/black bear attacks 
</span>
<span class="n">n_brown</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">idata</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_brown'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>
<span class="n">n_black</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">idata</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_black'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">df_fit</span><span class="p">[</span><span class="n">df_fit</span><span class="p">[</span><span class="s">'is_black'</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">][</span><span class="s">'Count'</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'crimson'</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">+</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">n_brown</span><span class="o">/</span><span class="n">n_brown</span><span class="p">.</span><span class="nb">sum</span><span class="p">(),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">df_fit</span><span class="p">[</span><span class="n">df_fit</span><span class="p">[</span><span class="s">'is_black'</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">][</span><span class="s">'Count'</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'crimson'</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">+</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">n_black</span><span class="o">/</span><span class="n">n_black</span><span class="p">.</span><span class="nb">sum</span><span class="p">(),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/poisson_glm/posterior_predictive.webp" alt="The posterior predictive of the Poisson model" /></p>

<p>The data seems compatible with the average estimate of our model.
We can now verify if the average number of attacks by black bears is statistically
compatible with the average number of attacks by brown bears.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">,</span> <span class="s">'beta'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/poisson_glm/posterior.webp" alt="The forest plot of our parameters" /></p>

<p>As we can see, $\beta$ is compatible with 0, so we can consider the average attack number
by black bears is compatible with the average attack number by brown bears.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We discussed a second kind of GLM, namely the Poisson regression,
and we applied this model to estimate the average number of lethal
attacks by wild bears in North America.</p>

<h2 id="suggested-readings">Suggested readings</h2>
<ul>
  <li><cite><a href="http://www.stat.columbia.edu/~gelman/book/BDA3.pdf">Gelman, A. (2014). Bayesian Data Analysis, Third Edition. Taylor &amp; Francis.</a></cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">pytensor</span>
</code></pre></div></div>

<div class="code">
Last updated: Wed Nov 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.7
<br />
IPython version      : 8.24.0
<br />

<br />
xarray  : 2024.9.0
<br />
pytensor: 2.25.5
<br />

<br />
numpy     : 1.26.4
<br />
arviz     : 0.20.0
<br />
pymc      : 5.17.0
<br />
matplotlib: 3.9.2
<br />
pandas    : 2.2.3
<br />
seaborn   : 0.13.2
<br />

<br />
Watermark: 2.4.3
<br />
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/logistic_regression/" /><summary type="html"><![CDATA[Regression on count data]]></summary></entry><entry><title type="html">Logistic regression</title><link href="http://localhost:4000/statistics/logistic_regression" rel="alternate" type="text/html" title="Logistic regression" /><published>2024-11-13T00:00:00+00:00</published><updated>2024-11-13T00:00:00+00:00</updated><id>http://localhost:4000/statistics/logistic_regression</id><content type="html" xml:base="http://localhost:4000/statistics/logistic_regression"><![CDATA[<p>In the last posts we discussed how to build
the simplest regression model for a real variable
with the linear model.
This model can be used as a starting block
to perform regression on many other types of data,
and this can be done by building
a <strong>Generalized Linear Model</strong> (GLM).</p>

<p>GLMs can be constructed by starting
from any likelihood for the
data \(P(y | \theta)\,.\)</p>

<p>The parameter $\theta$ usually is bounded to
some specific range \(D\): we have
\(\theta \in [0, 1]\) for the Binomial likelihood,
while we have $\theta &gt; 0$ for the Poisson model.
On the other hand, the variable</p>

\[Z \sim \alpha + \beta X\]

<p>can generally take any real value.
However, by choosing a suitable function</p>

\[f : \mathbb{R} \rightarrow D\]

<p>we can map our random variable \(Z\) to
the desired domain \(D\,.\)</p>

<p>The general GLM can therefore be written as</p>

\[\begin{align}
Y &amp; \sim P(\theta) \\
\theta &amp; = f\left(\alpha + \beta X\right)
\end{align}\]

<p>Of course $\alpha$ and $\beta$ and any other parameter
$\phi$ will be described by a suitable prior distribution.</p>

<p>Let us now see how to do this in practice.</p>

<h2 id="the-logistic-model">The logistic model</h2>

<p>The logistic model can be applied when there is a single binary dependent variable
which depends on one or more independent variables, which can be binary, integer or continuous.
In the logistic model the likelihood is taken as the binomial one,
while the mapping function $f$ is taken as the logistic function, plotted below:</p>

\[f(x) = \frac{1}{1+e^{-x}}\]

<p><img src="/docs/assets/images/statistics/logistic/logistic.webp" alt="The logistic function" /></p>

<p>We will apply the logistic regression to the Challenger O-ring dataset. 
On January 28th 1986 the shuttle broke during the launch, killing several people,
and the USA president formed a commission to investigate on the causes of the incident.
One of the member of the commission was the physicist Richard Feynman,
who proved that the incident was caused by a loss of flexibility of the shuttle
O-rings caused by the low temperature 
(see the <a href="https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disaster">Wikipedia page</a>)
Here we will take the data on the number of O-rings damaged in each mission of the Challenger,
and we will provide an estimate on the probability that one o-ring becomes damaged as a function of the temperature.
The original data can be found <a href="https://archive.ics.uci.edu/dataset/92/challenger+usa+space+shuttle+o+ring">here</a>,
and we provide here the dataset grouped by temperature for completeness (the temperature is expressed in °F).</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">temperature</th>
      <th style="text-align: right">damaged</th>
      <th style="text-align: right">undamaged</th>
      <th style="text-align: right">count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">53</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: right">57</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">58</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">63</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">66</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">5</td>
      <td style="text-align: right">67</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">18</td>
      <td style="text-align: right">18</td>
    </tr>
    <tr>
      <td style="text-align: right">6</td>
      <td style="text-align: right">68</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">7</td>
      <td style="text-align: right">69</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">8</td>
      <td style="text-align: right">70</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">22</td>
      <td style="text-align: right">24</td>
    </tr>
    <tr>
      <td style="text-align: right">9</td>
      <td style="text-align: right">72</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">10</td>
      <td style="text-align: right">73</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">11</td>
      <td style="text-align: right">75</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">11</td>
      <td style="text-align: right">12</td>
    </tr>
    <tr>
      <td style="text-align: right">12</td>
      <td style="text-align: right">76</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">12</td>
      <td style="text-align: right">12</td>
    </tr>
    <tr>
      <td style="text-align: right">13</td>
      <td style="text-align: right">78</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">14</td>
      <td style="text-align: right">79</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">15</td>
      <td style="text-align: right">81</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
  </tbody>
</table>

<p>The dataset contains all the information collected before the Challenger disaster.
The logistic model is already implemented into PyMC, but to see how it works we will implement it from scratch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc.sampling_jax</span> <span class="k">as</span> <span class="n">pmjax</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>


<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">df_oring</span><span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./data/orings.csv'</span><span class="p">)</span>

<span class="c1"># Convert it to Celsius
</span>
<span class="n">df_oring</span><span class="p">[</span><span class="s">'deg'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_oring</span><span class="p">[</span><span class="s">'temperature'</span><span class="p">]</span><span class="o">-</span><span class="mi">32</span><span class="p">)</span><span class="o">*</span><span class="mi">5</span><span class="o">/</span><span class="mi">9</span>
</code></pre></div></div>

<p>I converted the temperature to Celsius degree because it is easier for me to 
reason in terms of Celsius degree.
Let us write down our model</p>

\[\begin{align}
Y_i \sim &amp; \mathcal{Binom}(p_i, n_i)\\
p_i = &amp; 
\frac{1}{1+e^{-\alpha - \beta X_i}} 
\\
\end{align}\]

<p>The <strong>odds ratio</strong> is defined as</p>

\[\begin{align}
\frac{p}{1-p} 
&amp;
=
\frac{1}{1+e^{-\alpha - \beta X}}\frac{1}{1-\frac{1}{1+e^{-\alpha - \beta X}}}
\\
&amp;
=
\frac{1}{1+e^{-\alpha - \beta X}}\frac{ 1+e^{-\alpha - \beta X} }{e^{-\alpha - \beta X}}
\\
&amp;
= e^{\alpha + \beta X}
\end{align}\]

<p>therefore</p>

\[\log\left(\frac{p}{1-p}\right) = \alpha + \beta X\]

<p>We can therefore identify $\alpha$ with the log odds at $T=0°C$
It doesn’t really make sense to assume either a too big number or a too small one,
so we will take</p>

\[\alpha \sim \mathcal{N}(0, 15)\]

<p>On the other hand, $\beta$ represents the variation of the log odds with an increase of $1°C\,.$
We do expect a meaningful variation on a scale of $10°C\,,$ 
so we can generously take</p>

\[\beta \sim \mathcal{N}(0, 2)\]

<p>We are now ready to implement our model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">log_theta</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">df_oring</span><span class="p">[</span><span class="s">'deg'</span><span class="p">]</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">log_theta</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">df_oring</span><span class="p">[</span><span class="s">'count'</span><span class="p">],</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_oring</span><span class="p">[</span><span class="s">'undamaged'</span><span class="p">])</span>


<span class="k">with</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">idata_logistic</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                               <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span> <span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">)</span>


<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_logistic</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/logistic/trace.webp" alt="The trace of the logistic model" /></p>

<p>The trace looks fine, we can now take a look at the posterior predictive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_pl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

<span class="k">with</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">x_pl</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'p'</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">mu</span><span class="p">)))</span>

<span class="k">with</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">idata_logistic</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata_logistic</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y'</span><span class="p">,</span> <span class="s">'mu'</span><span class="p">,</span> <span class="s">'p'</span><span class="p">],</span>
                                                        <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span>
                <span class="n">idata_logistic</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">.</span><span class="n">p</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
                <span class="mi">1</span><span class="o">-</span>
                <span class="n">idata_logistic</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">.</span><span class="n">p</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
                <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span>
        <span class="n">idata_logistic</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">.</span><span class="n">p</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
        <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_oring</span><span class="p">[</span><span class="s">'deg'</span><span class="p">],</span> <span class="n">df_oring</span><span class="p">[</span><span class="s">'damaged'</span><span class="p">]</span><span class="o">/</span><span class="n">df_oring</span><span class="p">[</span><span class="s">'count'</span><span class="p">],</span>
           <span class="n">marker</span><span class="o">=</span><span class="s">'x'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'raw data estimate'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">1.01</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">"T $\degree$C"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">"Fraction of damaged O-rings"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/logistic/posterior_predictive.webp" alt="The trace of the logistic model" /></p>

<p>As we can see, the more we approach $0°\,,$ the more it is likely that an O-ring gets damaged.
The forecasted temperature for the launch day was $26-29 °F\,,$ corresponding to a range between
$-1.6$ °C and $-3.3$ °C.</p>

<p>We must however consider that one broken O-ring is not enough to create serious issues.
We can therefore estimate the probability as a function of the number of undamaged rings.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tm</span> <span class="o">=</span> <span class="p">(</span><span class="mi">26</span><span class="o">-</span><span class="mi">32</span><span class="p">)</span><span class="o">*</span><span class="mi">5</span><span class="o">/</span><span class="mi">9</span>
<span class="n">tM</span> <span class="o">=</span> <span class="p">(</span><span class="mi">29</span><span class="o">-</span><span class="mi">32</span><span class="p">)</span><span class="o">*</span><span class="mi">5</span><span class="o">/</span><span class="mi">9</span>

<span class="k">with</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">theta_m</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">tm</span><span class="o">*</span><span class="n">beta</span><span class="p">)))</span>
    <span class="n">ym</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">'ym'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta_m</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
    <span class="n">theta_M</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">tm</span><span class="o">*</span><span class="n">beta</span><span class="p">)))</span>
    <span class="n">yM</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">'yM'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta_M</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

<span class="k">with</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">ppc_t</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_logistic</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'ym'</span><span class="p">,</span> <span class="s">'yM'</span><span class="p">])</span>

<span class="c1"># We count how many O-rings are undamaged for each draw
</span>
<span class="n">hm</span> <span class="o">=</span> <span class="p">[(</span><span class="n">ppc_t</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'ym'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="n">k</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">)]</span>
<span class="n">hM</span> <span class="o">=</span> <span class="p">[(</span><span class="n">ppc_t</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yM'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="n">k</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">)]</span>
<span class="n">h_0</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">)]</span>

<span class="c1"># And we now estimate the corresponding probability
</span>
<span class="n">df_h</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'n'</span><span class="p">:</span> <span class="n">h_0</span><span class="p">,</span> <span class="s">'count_m'</span><span class="p">:</span> <span class="n">hm</span><span class="p">,</span> <span class="s">'count_M'</span><span class="p">:</span> <span class="n">hM</span><span class="p">})</span>
<span class="n">df_h</span><span class="p">[</span><span class="s">'prob_m'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_h</span><span class="p">[</span><span class="s">'count_m'</span><span class="p">]</span><span class="o">/</span><span class="n">df_h</span><span class="p">[</span><span class="s">'count_m'</span><span class="p">].</span><span class="nb">sum</span><span class="p">()</span>
<span class="n">df_h</span><span class="p">[</span><span class="s">'prob_M'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_h</span><span class="p">[</span><span class="s">'count_M'</span><span class="p">]</span><span class="o">/</span><span class="n">df_h</span><span class="p">[</span><span class="s">'count_M'</span><span class="p">].</span><span class="nb">sum</span><span class="p">()</span>

<span class="n">df_h</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'n'</span><span class="p">:</span> <span class="n">h_0</span><span class="p">,</span> <span class="s">'count_m'</span><span class="p">:</span> <span class="n">hm</span><span class="p">,</span> <span class="s">'count_M'</span><span class="p">:</span> <span class="n">hM</span><span class="p">})</span>

<span class="c1"># Let us take a look at the best-case scenario
</span><span class="n">sns</span><span class="p">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">df_h</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'n'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'prob_M'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/logistic/best_case.webp" alt="The probaility as a function of the undamaged rings" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_h</span><span class="p">[</span><span class="n">df_h</span><span class="p">[</span><span class="s">'n'</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">][</span><span class="s">'prob_M'</span><span class="p">]</span>
</code></pre></div></div>

<div class="code">
0    0.9469
<br />
Name: prob_M, dtype: float64
</div>
<p>The most probable scenario is that all O-rings get damaged, and this 
is scenario has, according to our model, the $95\%$ or probability to happen.</p>

<p>We can conclude that, with the available information,
it was not safe to perform the launch.
This is however a <em>post-hoc</em> analysis (an analysis performed on some
data once the outcome is known), and one should be really careful
to draw conclusions based on this kind of analysis, as this easily results
into false positive errors (see <a href="https://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data">the Wikipedia page on this topic</a>).</p>

<h2 id="conclusions">Conclusions</h2>

<p>We introduced the Generalized Linear Model, and we analyzed the Challenger dataset
by means of a logistic regression.
We have seen how, by means of the GLM, we can easily extend the linear regression
to binary data.
In the next post we will discuss the Poisson regression.</p>

<h2 id="suggested-readings">Suggested readings</h2>
<ul>
  <li><cite><a href="http://www.stat.columbia.edu/~gelman/book/BDA3.pdf">Gelman, A. (2014). Bayesian Data Analysis, Third Edition. Taylor &amp; Francis.</a></cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">pytensor</span>
</code></pre></div></div>
<div class="code">
Last updated: Wed Nov 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.7
<br />
IPython version      : 8.24.0
<br />

<br />
xarray  : 2024.9.0
<br />
pytensor: 2.25.5
<br />

<br />
pandas    : 2.2.3
<br />
numpy     : 1.26.4
<br />
seaborn   : 0.13.2
<br />
arviz     : 0.20.0
<br />
pymc      : 5.17.0
<br />
matplotlib: 3.9.2
<br />

<br />
Watermark: 2.4.3
<br />
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/logistic_regression/" /><summary type="html"><![CDATA[How to perform regression on binary data]]></summary></entry><entry><title type="html">Robust linear regression</title><link href="http://localhost:4000/statistics/robust_regression" rel="alternate" type="text/html" title="Robust linear regression" /><published>2024-11-06T00:00:00+00:00</published><updated>2024-11-06T00:00:00+00:00</updated><id>http://localhost:4000/statistics/robust_regression</id><content type="html" xml:base="http://localhost:4000/statistics/robust_regression"><![CDATA[<p>In some case your data may be not good enough to provide you reliable estimates with normal linear regression,
and this is the case of the conclusions drawn from
<a href="https://www.cambridge.org/core/journals/american-political-science-review/article/abs/political-institutions-and-voter-turnout-in-the-industrial-democracies/D6725BBF93F2F90F03A69B0794728BF7">this</a>
article, where the author concludes that there is a significant correlation between
the voter turnout in a country and its average income inequality.
This example is a classical example of misleading result of a regression,
where the author does not provide a plot of the data, taken from
<a href="https://www.google.it/books/edition/Data_Visualization/3XOYDwAAQBAJ?hl=it&amp;gbpv=1&amp;dq=Data+visualization,+a+practical+introduction&amp;printsec=frontcover">Healy, “Data visualization, a practical introduction”</a>.
The data below is extracted the data from the figure of Healy’s book.
South Africa corresponds to the last point.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">turnout</th>
      <th style="text-align: right">inequality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0.85822</td>
      <td style="text-align: right">1.95745</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: right">0.837104</td>
      <td style="text-align: right">1.95745</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">0.822021</td>
      <td style="text-align: right">2.41135</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">0.87632</td>
      <td style="text-align: right">2.76596</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">0.901961</td>
      <td style="text-align: right">2.95035</td>
    </tr>
    <tr>
      <td style="text-align: right">5</td>
      <td style="text-align: right">0.776772</td>
      <td style="text-align: right">3.21986</td>
    </tr>
    <tr>
      <td style="text-align: right">6</td>
      <td style="text-align: right">0.72549</td>
      <td style="text-align: right">3.14894</td>
    </tr>
    <tr>
      <td style="text-align: right">7</td>
      <td style="text-align: right">0.72549</td>
      <td style="text-align: right">2.92199</td>
    </tr>
    <tr>
      <td style="text-align: right">8</td>
      <td style="text-align: right">0.61991</td>
      <td style="text-align: right">2.93617</td>
    </tr>
    <tr>
      <td style="text-align: right">9</td>
      <td style="text-align: right">0.574661</td>
      <td style="text-align: right">2.31206</td>
    </tr>
    <tr>
      <td style="text-align: right">10</td>
      <td style="text-align: right">0.880845</td>
      <td style="text-align: right">3.60284</td>
    </tr>
    <tr>
      <td style="text-align: right">11</td>
      <td style="text-align: right">0.803922</td>
      <td style="text-align: right">3.5461</td>
    </tr>
    <tr>
      <td style="text-align: right">12</td>
      <td style="text-align: right">0.778281</td>
      <td style="text-align: right">3.47518</td>
    </tr>
    <tr>
      <td style="text-align: right">13</td>
      <td style="text-align: right">0.739065</td>
      <td style="text-align: right">3.68794</td>
    </tr>
    <tr>
      <td style="text-align: right">14</td>
      <td style="text-align: right">0.819005</td>
      <td style="text-align: right">4.41135</td>
    </tr>
    <tr>
      <td style="text-align: right">15</td>
      <td style="text-align: right">0.645551</td>
      <td style="text-align: right">3.91489</td>
    </tr>
    <tr>
      <td style="text-align: right">16</td>
      <td style="text-align: right">0.669683</td>
      <td style="text-align: right">5.64539</td>
    </tr>
    <tr>
      <td style="text-align: right">17</td>
      <td style="text-align: right">0.14178</td>
      <td style="text-align: right">9.30496</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc.sampling_jax</span> <span class="k">as</span> <span class="n">pmjax</span>

<span class="n">df_turnout</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/inequality.csv'</span><span class="p">)</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">df_turnout</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/inequality_pairplot.webp" alt="The dataset pairplot" /></p>

<p>By simply plotting the data we can clearly see that there is one point,
the South Africa, which is far away from the other,
and this may have a huge impact on the fit.
Let us see this, and how one may avoid this kind of error.</p>

<h2 id="the-normal-linear-regression">The normal linear regression</h2>

<p>Let us start by assuming that the inequality is distributed
according to a normal linear model,
analogous to the one already discussed in the <a href="/linear_regression">regression post</a>.</p>

\[Y \sim \mathcal{N}(\mu, \sigma)\]

<p>where</p>

\[\mu = \alpha + \beta X\]

<p>We will assume that the precision $\tau = 1/\sigma$ is distributed according to a Half Normal
distribution. Since the inequality goes from 0 to 10, assuming a
standard deviation of $5$ for $\tau$ should be sufficient.
On the other hand, we will make the quite generous assumption that</p>

\[\alpha \sim \mathcal{N}(0, 20)\]

\[\beta \sim \mathcal{N}(0, 20)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_norm</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'tau'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'turnout'</span><span class="p">].</span><span class="n">values</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'inequality'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_norm</span><span class="p">:</span>
    <span class="n">idata_norm</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span>
                           <span class="n">idata_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s">'log_likelihood'</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_norm</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/trace_norm.webp" alt="The trace of the normal model" /></p>

<p>The trace doesn’t show any relevant issue, and for our purposes it is
sufficient this check.
Let us check our fit</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_plt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_norm</span><span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y_pred'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">x_plt</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>
    

<span class="k">with</span> <span class="n">model_norm</span><span class="p">:</span>
    <span class="n">idata_norm</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata_norm</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y'</span><span class="p">,</span> <span class="s">'y_pred'</span><span class="p">],</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">))</span>

    
    
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">idata_norm</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">idata_norm</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
                <span class="n">idata_norm</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'turnout'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">df_turnout</span><span class="p">[</span><span class="s">'inequality'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/ppc_norm.webp" alt="The posterior preditcive distribution of our model" /></p>

<p>The error bands correctly reproduce almost all the data. However,
since the South Africa is far away from the other countries,
it may happen that its behavior strongly influences the fit.</p>

<p>Let us now use a more robust model.
In order to make it more robust, which in this context means
less sensitive to isolated data, let us take a t-Student likelihood
instead of a normal one.</p>

<p>We will leave the parameters $\alpha\,, \beta$ and $\tau = \frac{1}{\sigma}$
unchanged, but we must choose a prior for the number of degrees of
freedom $\nu\,.$</p>

<p>We want a robust estimate, so we want a prior with a small
number of degrees of freedom. However, $\nu \approx 0$
can be hard to handle from a numeric perspective,
since the resulting distribution decreases very slowly 
as one steps away from the peak.
For the above reason, we choose a Gamma prior with $\alpha=4$
and $\beta=2\,.$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_robust</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'tau'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Gamma</span><span class="p">(</span><span class="s">'nu'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'turnout'</span><span class="p">].</span><span class="n">values</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'inequality'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">tau</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_robust</span><span class="p">:</span>
    <span class="n">idata_robust</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> 
                             <span class="n">idata_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s">'log_likelihood'</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span> 
                             <span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_robust</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/trace_robust.webp" alt="The trace of the robust model" /></p>

<p>The trace doesn’t show relevant issues, so we can compute the posterior predictive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model_robust</span><span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y_pred'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">x_plt</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">tau</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_robust</span><span class="p">:</span>
    <span class="n">idata_robust</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata_robust</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y'</span><span class="p">,</span> <span class="s">'y_pred'</span><span class="p">],</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">))</span>


<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">idata_robust</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">median</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">idata_robust</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
                <span class="n">idata_robust</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'turnout'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">df_turnout</span><span class="p">[</span><span class="s">'inequality'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/ppc_robust.webp" alt="The PPC of the robust model" /></p>

<p>This distribution does a better job in reproducing the data, but
it tells a very different story from the normal model.</p>

<p>While in fact in the above model an increase of the turnout
translated into a reduction of the average inequality,
with this robust model this conclusion does not appear so clearly.</p>

<p>Let us try and see what does the LOO can tell us.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_compare</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">compare</span><span class="p">({</span><span class="s">'Normal model'</span><span class="p">:</span> <span class="n">idata_norm</span><span class="p">,</span> <span class="s">'Robust model'</span><span class="p">:</span> <span class="n">idata_robust</span><span class="p">})</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_compare</span><span class="p">(</span><span class="n">df_compare</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/loo.webp" alt="The plot of the LOO cross-validation" /></p>

<p>The LOO is slightly better for the normal model,
they are however very similar. Let us try and understand why.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_compare</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right">rank</th>
      <th style="text-align: right">elpd_loo</th>
      <th style="text-align: right">p_loo</th>
      <th style="text-align: right">elpd_diff</th>
      <th style="text-align: right">weight</th>
      <th style="text-align: right">se</th>
      <th style="text-align: right">dse</th>
      <th style="text-align: left">warning</th>
      <th style="text-align: left">scale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Normal model</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">-30.9967</td>
      <td style="text-align: right">4.92972</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0.889385</td>
      <td style="text-align: right">4.39811</td>
      <td style="text-align: right">0</td>
      <td style="text-align: left">True</td>
      <td style="text-align: left">log</td>
    </tr>
    <tr>
      <td style="text-align: left">Robust model</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">-32.3574</td>
      <td style="text-align: right">6.88334</td>
      <td style="text-align: right">1.36077</td>
      <td style="text-align: right">0.110615</td>
      <td style="text-align: right">4.66233</td>
      <td style="text-align: right">1.855</td>
      <td style="text-align: left">False</td>
      <td style="text-align: left">log</td>
    </tr>
  </tbody>
</table>

<p>The difference is $1.65\,,$ and the difference due to the number
of the degrees of freedom is the difference of the $p_{loo}\,,$
which is approximately 2.2, so the entire preference is due
to the lower number of degrees of freedom of the normal distribution.</p>

<p>We can see, however, that the LOO estimate for the normal
model has a warning. This generally happens because the ELPD
estimate is not exact, and it’s only reliable when 
removing one point does not affect too much log predictive density.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loo_normal</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">loo</span><span class="p">(</span><span class="n">idata_norm</span><span class="p">)</span>

<span class="n">loo_normal</span>
</code></pre></div></div>

<div class="code">
Computed from 16000 posterior samples and 18 observations log-likelihood matrix.
<br />

<br />
         Estimate       SE
<br />
elpd_loo   -31.00     4.40
<br />
p_loo        4.93        -
<br />

<br />
There has been a warning during the calculation. Please check the results.
<br />
------
<br />

<br />
Pareto k diagnostic values:
<br />
                         Count   Pct.
<br />
(-Inf, 0.5]   (good)       16   88.9%
<br />
 (0.5, 0.7]   (ok)          1    5.6%
<br />
   (0.7, 1]   (bad)         0    0.0%
<br />
   (1, Inf)   (very bad)    1    5.6%
<br />
</div>

<p>There are two points which strongly affect our parameters,
and one reasonable assumption is that one of those is the South Africa.</p>

<p>Let us try and see what does it happen once we remove it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_norm_red</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'tau'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'turnout'</span><span class="p">].</span><span class="n">values</span><span class="p">[:</span><span class="mi">17</span><span class="p">]</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'inequality'</span><span class="p">].</span><span class="n">values</span><span class="p">[:</span><span class="mi">17</span><span class="p">],</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_norm_red</span><span class="p">:</span>
    <span class="n">idata_norm_red</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                               <span class="n">idata_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s">'log_likelihood'</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span>
                               <span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span>
                               <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    
<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_norm_red</span><span class="p">)</span>

</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/trace_norm_red.webp" alt="The trace for the new normal model" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model_norm_red</span><span class="p">:</span>
    <span class="n">y_pred_red</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y_pred'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">x_plt</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>


<span class="k">with</span> <span class="n">model_norm_red</span><span class="p">:</span>
    <span class="n">idata_norm_red</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata_norm_red</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y'</span><span class="p">,</span> <span class="s">'y_pred'</span><span class="p">],</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">))</span>
    
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">idata_norm_red</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">idata_norm_red</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
                <span class="n">idata_norm_red</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'turnout'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">df_turnout</span><span class="p">[</span><span class="s">'inequality'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/ppc_norm_red.webp" alt="The PPC for the new model" /></p>

<p>This result looks much more to the robust estimate than to 
the full normal estimate.
While in the full normal model the parameter beta was
not compatible with 0, both for the robust and for the reduced
normal model it is.
This implies that those models contradict the full normal model,
which shows a negative association between the turnover
and the average income inequality.
Since the conclusion of the full normal model are heavily 
affected by the South Africa, before drawing
any conclusion one should carefully assess whether it
makes sense or not. Is the South Africa really representative or
is it a special case?</p>

<h2 id="conclusions">Conclusions</h2>

<p>We have discussed how to perform a robust linear regression,
and we have shown with an example that using it instead of a normal
linear regression makes our model more stable to the presence
of non-representative items.</p>

<h2 id="suggested-readings">Suggested readings</h2>
<ul>
  <li><cite>Healy, K. (2019). Data Visualization: A Practical Introduction. Princeton University Press.
</cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">pytensor</span>
</code></pre></div></div>
<div class="code">
Last updated: Wed Nov 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.7
<br />
IPython version      : 8.24.0
<br />

<br />
xarray  : 2024.9.0
<br />
pytensor: 2.25.5
<br />

<br />
pandas    : 2.2.3
<br />
seaborn   : 0.13.2
<br />
matplotlib: 3.9.2
<br />
pymc      : 5.17.0
<br />
numpy     : 1.26.4
<br />
arviz     : 0.20.0
<br />

<br />
Watermark: 2.4.3
<br />
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/robust_regression/" /><summary type="html"><![CDATA[Reducing sensitivity to large deviations]]></summary></entry><entry><title type="html">Multi-linear regression</title><link href="http://localhost:4000/statistics/multivariate_regression" rel="alternate" type="text/html" title="Multi-linear regression" /><published>2024-10-30T00:00:00+00:00</published><updated>2024-10-30T00:00:00+00:00</updated><id>http://localhost:4000/statistics/multivariate_regression</id><content type="html" xml:base="http://localhost:4000/statistics/multivariate_regression"><![CDATA[<p>When dealing with real-world datasets, you will often only
have to deal with more than one independent variable.
Here we will see how to adapt our framework to this case.
As you will see, doing so is straightforward, at least in theory.
In practice, this is not true, as deciding how to improve your model may be a tricky question,
and only practice and domain knowledge will, sometimes, help you in this task.</p>

<h2 id="the-dataset">The dataset</h2>

<p>In this post, we will use the dataset provided in <a href="https://www.tandfonline.com/doi/full/10.1080/10691898.2001.11910659">this</a>
very nice article, where the aim of the author is to show some of the difficulties
one faces when dealing with real World datasets.
The aim is to predict the price of a set of diamonds, given their carat numbers,
their color, their clarity and their certification.
I found this dataset in <a href="https://vincentarelbundock.github.io/Rdatasets/datasets.html">this amazing repo</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Diamond.csv'</span><span class="p">)</span>

<span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">rownames</th>
      <th style="text-align: right">carat</th>
      <th style="text-align: left">colour</th>
      <th style="text-align: left">clarity</th>
      <th style="text-align: left">certification</th>
      <th style="text-align: right">price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">0.3</td>
      <td style="text-align: left">D</td>
      <td style="text-align: left">VS2</td>
      <td style="text-align: left">GIA</td>
      <td style="text-align: right">1302</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">0.3</td>
      <td style="text-align: left">E</td>
      <td style="text-align: left">VS1</td>
      <td style="text-align: left">GIA</td>
      <td style="text-align: right">1510</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">3</td>
      <td style="text-align: right">0.3</td>
      <td style="text-align: left">G</td>
      <td style="text-align: left">VVS1</td>
      <td style="text-align: left">GIA</td>
      <td style="text-align: right">1510</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">4</td>
      <td style="text-align: right">0.3</td>
      <td style="text-align: left">G</td>
      <td style="text-align: left">VS1</td>
      <td style="text-align: left">GIA</td>
      <td style="text-align: right">1260</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">0.31</td>
      <td style="text-align: left">D</td>
      <td style="text-align: left">VS1</td>
      <td style="text-align: left">GIA</td>
      <td style="text-align: right">1641</td>
    </tr>
  </tbody>
</table>

<p>It is known that white, clear diamonds look brighter, and because of this they
have higher price than more opaque or colorful diamonds.
When considering colour and clarity, one should however keep in mind
that these values are assigned by experts, and two experts might provide
different values for the same diamond.</p>

<p>Let us now take a look at the relation between carat and price.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'carat'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'price'</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/multilinear/scatter.webp" alt="" /></p>

<p>From the above figure, we can see that it is unlikely that a linear fit would
work, since the dataset shows a very pronounced heteroskedasticity.
In order to improve the homoscedasticity, we can try the following transformation</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="s">'log_price'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'price'</span><span class="p">])</span>
<span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'carat'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'log_price'</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/multilinear/scatter_log.webp" alt="" /></p>

<p>The above transformation improved the homoscedasticity, so we now have higher chances
to be able to properly fit the dataset.</p>

<p>Let us now take a look at the categorical columns.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="n">certification</span><span class="p">.</span><span class="n">drop_duplicates</span><span class="p">()</span>
</code></pre></div></div>

<div class="code">
0      GIA
<br />
151    IGI
<br />
229    HRD
<br />
Name: certification, dtype: object
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="n">colour</span><span class="p">.</span><span class="n">drop_duplicates</span><span class="p">()</span>
</code></pre></div></div>

<div class="code">
0    D <br />
1    E <br />
2    G <br />
6    F <br />
8    H <br />
9    I <br />
Name: colour, dtype: object
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="n">clarity</span><span class="p">.</span><span class="n">drop_duplicates</span><span class="p">()</span>
</code></pre></div></div>

<div class="code">
0      VS2 <br />
1      VS1 <br />
2     VVS1 <br />
7     VVS2 <br />
83      IF <br />
Name: clarity, dtype: object
</div>

<p>These columns encode categories, and we should treat them by making an
indicator variable for each possible value of the three variables.</p>

<p>Taking the color as an example, we will take one value as baseline (say “D”) such that
all the indicator variables are zero for it.
We will then define four indicator variables “E”, “F”, “G” and “H”
with value 0 if the color is not the one corresponding to the variable,
1 otherwise.
This can be easily done with pandas as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_col</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'colour'</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="n">df_clar</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'clarity'</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="n">df_cert</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'certification'</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="n">df_cat</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_col</span><span class="p">,</span> <span class="n">df_clar</span><span class="p">,</span> <span class="n">df_cert</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>We can now try and fit the data with a linear model.
We will use two additional features which PyMC provides us, namely the
“coords” option and the “Data” class.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">coords</span> <span class="o">=</span> <span class="p">{</span><span class="s">'ind'</span><span class="p">:</span> <span class="n">df_cat</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="s">'col'</span><span class="p">:</span> <span class="n">df_cat</span><span class="p">.</span><span class="n">columns</span><span class="p">}</span>

<span class="n">yobs</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'log_price'</span><span class="p">].</span><span class="n">values</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">coords</span><span class="o">=</span><span class="n">coords</span><span class="p">)</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">X_cat</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Data</span><span class="p">(</span><span class="s">'X_cat'</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">df_cat</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="p">[</span><span class="s">'obs_idx'</span><span class="p">,</span> <span class="s">'feature'</span><span class="p">])</span>
    <span class="n">X_carat</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Data</span><span class="p">(</span><span class="s">'X_carat'</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'carat'</span><span class="p">]</span><span class="o">-</span><span class="n">df</span><span class="p">[</span><span class="s">'carat'</span><span class="p">].</span><span class="n">mean</span><span class="p">())</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">df</span><span class="p">[</span><span class="s">'carat'</span><span class="p">].</span><span class="n">std</span><span class="p">()),</span> <span class="n">dims</span><span class="o">=</span><span class="p">[</span><span class="s">'obs_idx'</span><span class="p">])</span>
    <span class="n">beta_cat</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_cat'</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="p">[</span><span class="s">'feature'</span><span class="p">],</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">beta_carat</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_carat'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">beta_cat</span><span class="p">,</span> <span class="n">X_cat</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">beta_carat</span><span class="o">*</span><span class="n">X_carat</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">yobs</span><span class="p">)</span>

<span class="n">pm</span><span class="p">.</span><span class="n">model_to_graphviz</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/multilinear/model.webp" alt="" /></p>

<p>In our model $\alpha$ is the intercept for the baseline diamond, $\beta_{cat}$ the average log-price
difference associated to the categorical variables and $\beta_{carat}$ the slope,
while $sigma$ is the standard deviation of our model.</p>

<p>As explained by Gelman, it is often suitable to replace a continuous regressor
$X$ with its standardized  version, as we did in our model,
in order to simplify the comparison between the corresponding
variable and the ones associated to discrete variables.
We divided by two standard deviations so that a difference between $-\sigma$
and $\sigma$ is not mapped into a $\Delta X = 1\,.$</p>

<p>By using the standardized  regressor, we also have two more advantages.
The first one is that the parameter $\alpha$ is now associated with an observable quantity,
namely the value of the log price when the carat number is equal to the average
carat number, and we don’t need to relate it to the extrapolated log price
when the carat number is 0.
The second advantage is that it is now easier to guess a prior for $\beta_{carat}\,,$
while it might not be so easy to do the same for the un-standardized regressor.</p>

<p>We can now fit our model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">idata</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">,</span> <span class="s">'beta'</span><span class="p">,</span> <span class="s">'sigma'</span><span class="p">],</span> <span class="n">filter_vars</span><span class="o">=</span><span class="s">'like'</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/multilinear/trace_model.webp" alt="" /></p>

<p>The traces look fine, let us now take a look at the posterior predictive</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">idata</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">))</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">idata</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/multilinear/ppc_model.webp" alt="" /></p>

<p>It doesn’t look like our model is appropriate to describe the data:
the log-price is overestimated close to the tails, while it is underestimated
close to the center of the distribution.</p>

<p>If we look again at the carat vs log-price scatterplot,
it looks like close to 0 the behavior is not polynomial, and this
suggests us that we should use, as independent variable,
a function of $x$ which is not analytic in 0.
The two most common choices are the logarithm and the square root.
We will follow the linked article and use the square root.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">coords</span><span class="o">=</span><span class="n">coords</span><span class="p">)</span> <span class="k">as</span> <span class="n">model_s</span><span class="p">:</span>
<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">coords</span><span class="o">=</span><span class="n">coords</span><span class="p">)</span> <span class="k">as</span> <span class="n">model_s</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">X_cat</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Data</span><span class="p">(</span><span class="s">'X_cat'</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">df_cat</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="p">[</span><span class="s">'obs_idx'</span><span class="p">,</span> <span class="s">'feature'</span><span class="p">])</span>
    <span class="n">X_carat</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Data</span><span class="p">(</span><span class="s">'X_carat'</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'carat'</span><span class="p">])</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'carat'</span><span class="p">])))</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'carat'</span><span class="p">]))),</span> <span class="n">dims</span><span class="o">=</span><span class="p">[</span><span class="s">'obs_idx'</span><span class="p">])</span>
    <span class="n">beta_cat</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_cat'</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="p">[</span><span class="s">'feature'</span><span class="p">],</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">beta_carat</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta_carat'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">beta_cat</span><span class="p">,</span> <span class="n">X_cat</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">beta_carat</span><span class="o">*</span><span class="n">X_carat</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">yobs</span><span class="p">)</span>
    <span class="n">idata_s</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_s</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">,</span> <span class="s">'beta'</span><span class="p">,</span> <span class="s">'sigma'</span><span class="p">],</span> <span class="n">filter_vars</span><span class="o">=</span><span class="s">'like'</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/multilinear/trace_model_s.webp" alt="" /></p>

<p>Also in this case the trace looks fine. Let us now look at the posterior predictive
distribution</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model_s</span><span class="p">:</span>
    <span class="n">idata_s</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata_s</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">))</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">idata_s</span><span class="p">)</span>

</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/multilinear/ppc_model_s.webp" alt="" /></p>

<p>It looks like the result slightly improved.
Let us try and compare the two models.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">pm</span><span class="p">.</span><span class="n">compute_log_likelihood</span><span class="p">(</span><span class="n">idata</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_s</span><span class="p">:</span>
    <span class="n">pm</span><span class="p">.</span><span class="n">compute_log_likelihood</span><span class="p">(</span><span class="n">idata_s</span><span class="p">)</span>

<span class="n">df_comp</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">compare</span><span class="p">({</span><span class="s">'linear'</span><span class="p">:</span> <span class="n">idata</span><span class="p">,</span> <span class="s">'square root'</span><span class="p">:</span> <span class="n">idata_s</span><span class="p">})</span>

<span class="n">df_comp</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right">rank</th>
      <th style="text-align: right">elpd_loo</th>
      <th style="text-align: right">p_loo</th>
      <th style="text-align: right">elpd_diff</th>
      <th style="text-align: right">weight</th>
      <th style="text-align: right">se</th>
      <th style="text-align: right">dse</th>
      <th style="text-align: left">warning</th>
      <th style="text-align: left">scale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">square root</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">363.968</td>
      <td style="text-align: right">13.2115</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">9.61651</td>
      <td style="text-align: right">0</td>
      <td style="text-align: left">False</td>
      <td style="text-align: left">log</td>
    </tr>
    <tr>
      <td style="text-align: left">linear</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">164.762</td>
      <td style="text-align: right">13.7875</td>
      <td style="text-align: right">199.206</td>
      <td style="text-align: right">6.12772e-10</td>
      <td style="text-align: right">9.96487</td>
      <td style="text-align: right">8.65212</td>
      <td style="text-align: left">False</td>
      <td style="text-align: left">log</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_compare</span><span class="p">(</span><span class="n">df_comp</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/multilinear/compare_1.webp" alt="" /></p>

<p>There are no warnings, we can therefore consider the estimate
as reliable, and there is no doubt that the latter model greatly improved the result.</p>

<p>Let us also take a look at the LOO-PIT</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">az</span><span class="p">.</span><span class="n">plot_loo_pit</span><span class="p">(</span><span class="n">idata_s</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'y'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">az</span><span class="p">.</span><span class="n">plot_loo_pit</span><span class="p">(</span><span class="n">idata_s</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'y'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ecdf</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/multilinear/loo_pit_s.webp" alt="" /></p>

<p>There is still margin for improvement, as it doesn’t really look like
the LOO-PIT is compatible with the uniform distribution.
We won’t however improve our model for now.</p>

<p>Let us instead inspect the impact of the categorical variables on the log price</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">idata_s</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="s">'beta_cat'</span><span class="p">,</span> <span class="n">filter_vars</span><span class="o">=</span><span class="s">'like'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">combined</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">df_cat</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/multilinear/forest_s.webp" alt="" /></p>

<p>All the categorical variables has an important effect on the log price,
as we expected.
In this case, we already knew which variables to include in our model,
in general it won’t be this case, as you might have more variables than needed.</p>

<p>Gelman, in the textbook “Data Analysis Using Regression and Multilevel/Hierarchical Models”.
suggests the following method to decide which variables are relevant:</p>

<p>Keep all variables that you expect might be relevant in the outcome prediction.
If you also have additional variables:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">A parameter</th>
      <th>has the expected sign</th>
      <th>does not have the expected sign</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>is not significant</strong></td>
      <td>Keep it</td>
      <td>Don’t keep it</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>is significant</strong></td>
      <td>Keep it</td>
      <td>You should ask yourself why this is happening. Are you not considering a variable?</td>
    </tr>
  </tbody>
</table>

<p>Finally, if an independent variable has a large effect, consider including an interaction
term.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The regression with multiple variables is a deep topic, and we barely introduced
the main concepts and gave few hints on how to work with this kind of model.
We did so by using a real-World dataset, and we also showed some of the issues one 
might face when dealing with problematic datasets.</p>

<h2 id="suggested-readings">Suggested readings</h2>
<ul>
  <li><cite>Gelman, A., Hill, J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models. CUP.
</cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">pytensor</span>
</code></pre></div></div>

<div class="code">
Last updated: Wed Nov 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.7
<br />
IPython version      : 8.24.0
<br />

<br />
xarray  : 2024.9.0
<br />
pytensor: 2.25.5
<br />

<br />
matplotlib: 3.9.2
<br />
arviz     : 0.20.0
<br />
seaborn   : 0.13.2
<br />
pandas    : 2.2.3
<br />
pymc      : 5.17.0
<br />
numpy     : 1.26.4
<br />

<br />
Watermark: 2.4.3
<br />
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/linear_regression/" /><summary type="html"><![CDATA[Including many covariates]]></summary></entry><entry><title type="html">Linear regression with binary input</title><link href="http://localhost:4000/statistics/regression_binary_input" rel="alternate" type="text/html" title="Linear regression with binary input" /><published>2024-10-23T00:00:00+00:00</published><updated>2024-10-23T00:00:00+00:00</updated><id>http://localhost:4000/statistics/regression_binary_input</id><content type="html" xml:base="http://localhost:4000/statistics/regression_binary_input"><![CDATA[<p>Linear regression can be straightforwardly applied when the independent variable
$X$ is discrete. One should only pay attention to the interpretation
of the parameters in this case, as the interpretation provided in the 
previous post may not apply.</p>

<h2 id="the-model">The model</h2>

<p>Here we will use the “Student alcohol consumption”
dataset, available on <a href="https://data.world/databeats/student-alcohol-consumption/">this data.world page</a>.
In this study, the authors analyzed the relationship between
alcohol consumption and many aspects of the student’s life, including
the school behavior.
We will analyze the difference in the math grades between male and female
students.</p>

<p>Our model will be the following</p>

\[y_i = \mathcal{N}(\mu_i, \sigma)\]

<p>where</p>

\[\mu_i = \beta_0 + \beta_1 x_i\]

<p>and</p>

\[x_i =
\begin{cases}
0 &amp; if\, x=F\\
1 &amp; if\, x=M
\end{cases}\]

<p>In this case we have that, for female students,
the average grade is $\beta_0\,,$
while for male students it is $\beta_0 + \beta_1\,.$</p>

<p>The female group and, more generally, the group 
with average dependent variable $\beta_0\,,$ is called the <strong>reference group</strong>.</p>

<p>The parameter $\beta_1$ can be now interpreted as the difference
between the average male and female grade.</p>

<h2 id="the-implementation">The implementation</h2>

<p>Let us now see how to implement this model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://query.data.world/s/qz5sf27veajjivl3bpa5npazsxzn7z?dws=00000'</span><span class="p">)</span>

</code></pre></div></div>

<p>Let us now only keep the columns we are interested in</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">df_red</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s">'sex'</span><span class="p">,</span> <span class="s">'G3'</span><span class="p">]]</span>

<span class="n">df_red</span><span class="p">[</span><span class="s">'sex'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_red</span><span class="p">[</span><span class="s">'sex'</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="s">'category'</span><span class="p">)</span>

<span class="n">df_red</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: left">sex</th>
      <th style="text-align: right">G3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: left">F</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: left">F</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: left">F</td>
      <td style="text-align: right">10</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: left">F</td>
      <td style="text-align: right">15</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: left">F</td>
      <td style="text-align: right">10</td>
    </tr>
  </tbody>
</table>

<p>We can now easily build our model.
We could easily do this as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_red</span><span class="p">[</span><span class="s">'sex'</span><span class="p">]</span><span class="o">==</span><span class="s">'M'</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</code></pre></div></div>

<p>This method works fine for a binary category, but this method becomes
cumbersome as the number of categories grows.
Fortunately, pandas provides a builtin function to do this job,
namely the “factorize” function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">,</span> <span class="n">cat_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">factorize</span><span class="p">(</span><span class="n">df_red</span><span class="p">[</span><span class="s">'sex'</span><span class="p">])</span>

<span class="n">cat_data</span>
</code></pre></div></div>
<div class="code">
CategoricalIndex(['F', 'M'], categories=['F', 'M'], ordered=False, dtype='category')
</div>

<p>Since the first category is “F”, the females will be associated 
to the 0 values in x, while males will be associated to 1.</p>

<p>We can now implement the model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_red</span><span class="p">[</span><span class="s">'G3'</span><span class="p">])</span>
    <span class="n">idata</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/regression_binary/trace.webp" alt="The trace for our simple model" /></p>

<p>Let us take a better look to the beta parameters</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="s">'beta'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/regression_binary/forest.webp" alt="The forest plot for the beta parameters" /></p>

<p>It looks like, in this study, the male students 
have on average a slightly higher math grade than the female students.</p>

<h2 id="posterior-predictive-checks">Posterior predictive checks</h2>

<p>Let us now verify if the observed data are included within the predicted
uncertainties</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="k">for</span> <span class="n">yt</span> <span class="ow">in</span> <span class="n">az</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span><span class="n">ppc</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s">'posterior_predictive'</span><span class="p">)[</span><span class="s">'y'</span><span class="p">].</span><span class="n">T</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">s</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">df_red</span><span class="p">[</span><span class="s">'G3'</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s">'F'</span><span class="p">,</span> <span class="s">'M'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/regression_binary/ppc.webp" alt="The posterior predictive for our model" /></p>

<p>In the above code block, we “jittered” (added a small random number
to the x variable) in order to be able to distinguish the points.</p>

<p>The posterior predictive looks good, so we can conclude that
on average the male students performed slightly better than the female
ones.
We could of course improve our model by imposing that the grade
is non-negative, but if we simply want to investigate the mean
difference, this model is enough for our purposes.
We want to stress that this does not imply that being males makes
you perform better at math with respect to females or vice versa,
this would be wrong for many reasons.
First of all, as we will discuss in the section about causal inference, this
statement has no meaning in the counterfactual definition of causality,
as you cannot manipulate someone’s biological sex.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We extended the linear regression model to a binary outcome,
and discussed the interpretation of the parameter’s models when
the binary regressor encodes a categorical variable.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">pytensor</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>

<div class="code">
Last updated: Wed Nov 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.7
<br />
IPython version      : 8.24.0
<br />

<br />
xarray  : 2024.9.0
<br />
pytensor: 2.25.5
<br />
numpyro : 0.15.0
<br />
jax     : 0.4.28
<br />
jaxlib  : 0.4.28
<br />

<br />
arviz     : 0.20.0
<br />
pandas    : 2.2.3
<br />
pymc      : 5.17.0
<br />
numpy     : 1.26.4
<br />
seaborn   : 0.13.2
<br />
matplotlib: 3.9.2
<br />

<br />
Watermark: 2.4.3
<br />
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/binary_linear_regression/" /><summary type="html"><![CDATA[Extending regression to discrete variables]]></summary></entry><entry><title type="html">Introduction to the linear regression</title><link href="http://localhost:4000/statistics/regression" rel="alternate" type="text/html" title="Introduction to the linear regression" /><published>2024-10-16T00:00:00+00:00</published><updated>2024-10-16T00:00:00+00:00</updated><id>http://localhost:4000/statistics/regression</id><content type="html" xml:base="http://localhost:4000/statistics/regression"><![CDATA[<p>So far we discussed how to model one variable. With this
post we will start a discussion on how to model the dependence of one
variable on other variables, named <strong>covariates</strong>,  <strong>regressors</strong>,
<strong>predictors</strong>
or <strong>risk factors</strong> depending on the research area we are dealing with.</p>

<h2 id="regression">Regression</h2>

<p>In regression, we want to model the dependence of one variable
\(Y\) on one or more external variables \(X\).
In other words, we are trying to determine an $f$ such that</p>

\[Y_i = f(X_i, \theta) + \varepsilon_i\]

<p>where $\varepsilon_i$ is some random noise and $\theta$ represents a set of parameters.
In the case of regression, we are not interested in modelling $X_i$.
Notice that the distribution of the $Y_i$ is now different among
different elements, as the parameters are assumed to depend on $X_i\,.$
In other words, the $Y_i$ are no more identically distributed.
What we want to model is the <strong>statistical dependence</strong>, which is not an exact one, since
we assume that there is some noise which makes our dependence inaccurate.
This fact makes statistical dependence different from the mathematical dependence,
where the relation is exactly fulfilled.
We must also draw a distinction between statistical dependence and causal dependence,
since a common misconception is that finding a statistical dependence 
implies a causal relation between $X$ and $Y$.</p>

<div class="emphbox">
Causal inference requires much stronger
assumptions than statistical inference.
</div>

<p>We are only allowed to draw conclusions about causality
when these assumptions are satisfied, as we will discuss later in this blog.
Notice that referring to $X$ as the risk factor is usually done in the context
of causal inference, and we will therefore avoid this term for now.</p>

<p>As pointed out by John Carlin in <a href="https://arxiv.org/pdf/2309.06668.pdf">this paper</a>, there are two main purposes for regression
other than causal inference: we may either want to <strong>describe</strong> a relation between $X$ and $Y\,,$
or we may desire to use our model to <strong>predict</strong> the value of $Y$ one $X$ has been measured.</p>

<p>By Taylor expanding $f$ around $X=0$ we have that the simplest
dependence we can assume is</p>

\[Y_i = \theta_0 + \theta_1 X_i + \varepsilon_i\,,\]

<p>we are therefore assuming the <strong>additivity</strong> of $Y_i$ with respect to $X_i\,.$</p>

<p>The assumption which is by far the most common for $\varepsilon_i$ is</p>

\[\varepsilon_i \sim \mathcal{N}(0, \sigma)\]

<p>We are therefore assuming that the errors are normally distributed, and that the
variance is independent on $X_i\,.$
The constant variance assumption is named <strong>homoscedasticity</strong>,
while the condition of variable variance is named <strong>heteroskedasticity</strong>.</p>

<p>Let us now take a look at our parameters:</p>
<ul>
  <li>$\theta_1$ is the average $Y$ difference of two groups with $\Delta Y = 1\,.$</li>
  <li>$\theta_0$ is the intercept of the model. If our data includes $X=0$ we can interpret $\theta_0$ as the value of $Y$ when $X=0\,.$</li>
  <li>$\sigma$ is the average variance.</li>
</ul>

<h2 id="gdp-life-expectancy-relation">GDP-Life expectancy relation</h2>

<p>In order to understand our model, we will apply it to investigate the relation between the gross domestic product of
a country and its life expectancy.</p>

<p>First of all, let us import the relevant libraries.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">zipfile</span> <span class="kn">import</span> <span class="n">ZipFile</span>
<span class="kn">import</span> <span class="nn">io</span>
</code></pre></div></div>

<p>We can now download the GDP data from the IMF rest API as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">requests</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'https://www.imf.org/external/datamapper/api/v1/NGDPDPC'</span><span class="p">)</span> <span class="k">as</span> <span class="n">gdp</span><span class="p">:</span>
    <span class="n">data_gdp</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">gdp</span><span class="p">.</span><span class="n">content</span><span class="p">)</span>

<span class="n">dt</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="p">[</span><span class="n">data_gdp</span><span class="p">[</span><span class="s">'values'</span><span class="p">][</span><span class="s">'NGDPDPC'</span><span class="p">][</span><span class="n">key</span><span class="p">][</span><span class="s">'2021'</span><span class="p">]]</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">data_gdp</span><span class="p">[</span><span class="s">'values'</span><span class="p">][</span><span class="s">'NGDPDPC'</span><span class="p">]</span> <span class="k">if</span> <span class="s">'2021'</span> <span class="ow">in</span> <span class="n">data_gdp</span><span class="p">[</span><span class="s">'values'</span><span class="p">][</span><span class="s">'NGDPDPC'</span><span class="p">][</span><span class="n">key</span><span class="p">]}</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">dt</span><span class="p">).</span><span class="n">transpose</span><span class="p">().</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s">'gdp'</span><span class="p">})</span>

</code></pre></div></div>

<p>We also download the country names from the same API, and we combine the two tables</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">requests</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'https://www.imf.org/external/datamapper/api/v1/countries'</span><span class="p">)</span> <span class="k">as</span> <span class="n">countries</span><span class="p">:</span>
    <span class="n">data_countries</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">countries</span><span class="p">.</span><span class="n">content</span><span class="p">)</span>

<span class="n">df_countries</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="n">from_dict</span><span class="p">({</span><span class="n">key</span><span class="p">:</span> <span class="p">[</span><span class="n">data_countries</span><span class="p">[</span><span class="s">'countries'</span><span class="p">][</span><span class="n">key</span><span class="p">][</span><span class="s">'label'</span><span class="p">]]</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">data_countries</span><span class="p">[</span><span class="s">'countries'</span><span class="p">]}).</span><span class="n">transpose</span><span class="p">().</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s">'name'</span><span class="p">})</span>

<span class="n">df_n</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">df_countries</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s">'inner'</span><span class="p">)</span>
</code></pre></div></div>

<p>The table containing the life expectancy can be downloaded from <a href="https://data.worldbank.org/indicator/SP.DYN.LE00.IN">this page of the World Bank website</a>.
Rather than clicking on the website, we will download it automatically as follows</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">requests</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'https://api.worldbank.org/v2/en/indicator/SP.DYN.LE00.IN?downloadformat=csv'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">content</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">ZipFile</span><span class="p">(</span><span class="n">io</span><span class="p">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>

<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">z</span><span class="p">.</span><span class="n">namelist</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">name</span><span class="p">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'API'</span><span class="p">):</span>
        <span class="n">dt</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="n">df_lifexp</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">skiprows</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

</code></pre></div></div>

<p>We can now combine the two dataframes. We will stick to the year 2021, as it is the most recent year for
most of the countries.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_le</span> <span class="o">=</span> <span class="n">df_lifexp</span><span class="p">[[</span><span class="s">'Country Code'</span><span class="p">,</span> <span class="s">'2021'</span><span class="p">]].</span><span class="n">set_index</span><span class="p">(</span><span class="s">'Country Code'</span><span class="p">).</span><span class="n">dropna</span><span class="p">().</span><span class="n">rename</span><span class="p">(</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s">'2021'</span><span class="p">:</span> <span class="s">'Life expectancy'</span><span class="p">})</span>

<span class="n">df_final</span> <span class="o">=</span> <span class="n">df_n</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">df_le</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s">'inner'</span><span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">df_final</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/regression/pairplot.webp" alt="The pairplot of our variables" /></p>

<p>There appears to be no linear relation between the two. However, by a suitable variable redefinition,
we can get linearity within a good approximation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_final</span><span class="p">[</span><span class="s">'log GDP'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">df_final</span><span class="p">[</span><span class="s">'gdp'</span><span class="p">])</span>

<span class="n">sns</span><span class="p">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">df_final</span><span class="p">[[</span><span class="s">'log GDP'</span><span class="p">,</span> <span class="s">'Life expectancy'</span><span class="p">]])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/regression/pairplot_log.webp" alt="The pairplot of our variables" /></p>

<p>The homoscedasticity only seems to hold approximately, as in the region with lower GDP the data shows
a larger variance with respect to countries with higher GDP.
For the sake of simplicity, we will stick to the constant variance assumption, and we will see how to deal
with heterogeneous variance in a future post.</p>

<p>Let us now set up our model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">beta</span><span class="o">*</span><span class="n">df_final</span><span class="p">[</span><span class="s">'log GDP'</span><span class="p">],</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_final</span><span class="p">[</span><span class="s">'Life expectancy'</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y_pred'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">beta</span><span class="o">*</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>  <span class="c1"># We want to get the error bands for all the values of x_pred
</span>
<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">,</span> <span class="s">'beta'</span><span class="p">,</span> <span class="s">'sigma'</span><span class="p">])</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/regression/trace.webp" alt="The trace plot" /></p>

<p>The trace looks fine. We will directly check the posterior predictive distribution.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>

<span class="n">y_mean</span> <span class="o">=</span> <span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_pred</span><span class="p">))).</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_low</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_pred</span><span class="p">))),</span> <span class="mf">0.025</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_high</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_pred</span><span class="p">))),</span> <span class="mf">0.975</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">y_low</span><span class="p">,</span> <span class="n">y_high</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_final</span><span class="p">[</span><span class="s">'log GDP'</span><span class="p">],</span> <span class="n">df_final</span><span class="p">[</span><span class="s">'Life expectancy'</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">x_pred</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x_pred</span><span class="p">)])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Log GDP per Capita'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Life Expectancy 2021'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/regression/ppc.webp" alt="The posterior predictive" /></p>

<p>While our model correctly reproduces the relation between the GDP and the average life expectancy,
it fails to reproduce the observed variance, confirming that the homoscedasticity assumption is violated.</p>

<p>Let us now inspect which nations show the biggest error</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">df_final</span><span class="p">[</span><span class="s">'Life expectancy'</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'beta'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="n">df_final</span><span class="p">[</span><span class="s">'log GDP'</span><span class="p">]).</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div class="code">
NGA   -13.328530
<br />
SWZ   -12.201977
<br />
GNQ   -11.809278
<br />
NRU   -11.197007
<br />
NAM   -10.621142
<br />
dtype: float64
</div>

<p>The above nations are Nigeria, eSwatini, Equatorial Guinea, Nuaru and Nambia,
so it looks like our model fails to reproduce some
African and Oceania countries, which have an average life expectancy
much lower than non-African and non-Oceania countries with similar GDP.</p>

<p>Let us also check if the assumption about the normality of the deviation
from the average trend is fulfilled within a good approximation</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span>
<span class="p">(</span><span class="n">df_final</span><span class="p">[</span><span class="s">'Life expectancy'</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'alpha'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> 
 <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'beta'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="n">df_final</span><span class="p">[</span><span class="s">'log GDP'</span><span class="p">]),</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span><span class="mf">1.5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Residuals histogram'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/regression/res_plot.webp" alt="The pairplot of our variables" /></p>

<p>It appears that the distribution of the residual is left skewed, so in order
to improve our model we could use a skewed distribution,
like the <a href="https://www.pymc.io/projects/docs/en/stable/api/distributions/generated/pymc.SkewNormal.html">Skewed Normal distribution</a> or the <a href="https://arxiv.org/pdf/2303.05615.pdf">Variance Gamma</a>
instead of a normal distribution.</p>

<p>This is however a somehow more advanced topic with respect to an introductory
post on linear regression, so we won’t implement these models here.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We introduced the linear model, and we saw how to implement it
with an example.
As we will see in the future posts, the linear model is the 
starting point for almost any regression model.
We discussed the interpretation of the parameters and some
of the most relevant assumptions we made about data.</p>

<h2 id="suggested-readings">Suggested readings</h2>

<ul>
  <li><cite> Kutner, M. H., Nachtsheim, C., Neter, J. (2004). Applied linear regression models.UK: McGraw-Hill/Irwin. </cite></li>
  <li><cite> Gelman, A., Hill, J., Vehtari, A. (2020). Regression and Other Stories. India: Cambridge University Press. </cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">pytensor</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>
<div class="code">
Last updated: Wed Nov 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.7
<br />
IPython version      : 8.24.0
<br />

<br />
xarray  : 2024.9.0
<br />
pytensor: 2.25.5
<br />
numpyro : 0.15.0
<br />
jax     : 0.4.28
<br />
jaxlib  : 0.4.28
<br />

<br />
requests  : 2.32.3
<br />
seaborn   : 0.13.2
<br />
matplotlib: 3.9.2
<br />
numpy     : 1.26.4
<br />
arviz     : 0.20.0
<br />
json      : 2.0.9
<br />
pymc      : 5.17.0
<br />
pandas    : 2.2.3
<br />

<br />
Watermark: 2.4.3
<br />
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/linear_regression/" /><summary type="html"><![CDATA[Including dependence on external variables]]></summary></entry><entry><title type="html">Model comparison, cont.</title><link href="http://localhost:4000/statistics/model_averaging_cont" rel="alternate" type="text/html" title="Model comparison, cont." /><published>2024-10-10T00:00:00+00:00</published><updated>2024-10-10T00:00:00+00:00</updated><id>http://localhost:4000/statistics/model_averaging_cont</id><content type="html" xml:base="http://localhost:4000/statistics/model_averaging_cont"><![CDATA[<p>We previously discussed the Bayes Factors as a tool to choose between different models.
This method has however many issues, and it is generally not recommended to use it.
We will now discuss a very powerful method, namely the Leave One Out cross validation</p>

<h2 id="leave-one-out-cross-validation">Leave One Out cross-validation</h2>
<p>This method is generally preferred to the above one, as it has been pointed out
that Bayes factors are appropriate only when one of the models is true,
while in real world problems we don’t have any certainty about which is the model that
generated the data, assuming that it makes sense to claim that it exists such a model.
Moreover, the sampler used to compute the Bayes factor, namely Sequential Monte Carlo,
is generally less stable than the standard one used by PyMC, which is the NUTS sampler.
There are other, more philosophical reasons, pointed out by Gelman in <a href="https://statmodeling.stat.columbia.edu/2017/07/21/bayes-factor-term-came-references-generally-hate/">this post</a>,
but for now we won’t dig into this kind of discussion.</p>

<p>The LOO method is much more in the spirit of the Machine Learning, where
one splits the sample into a training set and a test set.
The train set is used to find the parameters, while the second one is
used to assess the performances of the model for new data.
This method, namely the <strong>cross validation</strong>, is by far the most
reliable one, and we generally recommend to use it.</p>

<p>LOO and cross validation adhere to the principles of scientific method,
where we use the predictions of our models to compare and criticize them.</p>

<p>It is however very common that the dataset is too small to allow
a full cross-validation.
The LOO cross validation is equivalent to the computation of</p>

\[ELPD = \sum_i \log p(y_i \vert y_{-i})\]

<p>where \(p(y_i\vert y_{-i})\) is the posterior predictive probability
of the point \(y_i\) relative to the model fitted by removing \(y_i\,.\)</p>

<p>We already anticipated this method in the post on the
<a href="/statistics/negbin">negative binomial model</a>,
but we will discuss it here more in depth.</p>

<p>In this example, we are looking for the distribution of the log-return
of an indian company.
We will first try and use a normal distribution. We will then use a more general
t-Student distribution to fit the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>
<span class="kn">import</span> <span class="nn">yfinance</span> <span class="k">as</span> <span class="n">yf</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tk</span> <span class="o">=</span> <span class="n">yf</span><span class="p">.</span><span class="n">Ticker</span><span class="p">(</span><span class="s">"AJMERA.NS"</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">tk</span><span class="p">.</span><span class="n">get_shares_full</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="s">"2023-01-01"</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s">"2024-07-01"</span><span class="p">)</span>

<span class="n">logret</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">diff</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">values</span><span class="p">))</span>

<span class="n">sns</span><span class="p">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">logret</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s">'density'</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging_cont/logret.webp" alt="" /></p>

<p>The distribution shows heavy tails, it is therefore quite clear that a normal distribution
might not be appropriate.
We will however start from the simplest model, and use it as a benchmark for a more involved model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">norm</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">yobs</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'yobs'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">logret</span><span class="p">)</span>

<span class="k">with</span> <span class="n">norm</span><span class="p">:</span>
    <span class="n">idata_norm</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_norm</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging_cont/trace_norm.webp" alt="" /></p>

<p>The trace doesn’t show any issue. Let us try with a Student-T distribution</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">t</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Gamma</span><span class="p">(</span><span class="s">'nu'</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">yobs</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'yobs'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">logret</span><span class="p">)</span>

<span class="k">with</span> <span class="n">t</span><span class="p">:</span>
    <span class="n">idata_t</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_t</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>

</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging_cont/trace_t.webp" alt="" /></p>

<p>Also in this case the trace doesn’t show any relevant issue.
Let us now check if we are able to reproduce the observed data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">norm</span><span class="p">:</span>
    <span class="n">idata_norm</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata_norm</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">idata_norm</span><span class="p">,</span> <span class="n">num_pp_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/model_averaging_cont/ppc_norm.webp" alt="" /></p>

<p>Our model seems totally unable to fit the data due to the presence of heavy tails.
Let us now verify if the second model does a better job.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">t</span><span class="p">:</span>
    <span class="n">idata_t</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata_t</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">idata_t</span><span class="p">,</span> <span class="n">num_pp_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/model_averaging_cont/ppc_t.webp" alt="" /></p>

<p>As you can see, there is much more agreement with the data, so this model looks
more appropriate.
Let us now see if the LOO cross validation confirms our first impression.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">norm</span><span class="p">:</span>
    <span class="n">pm</span><span class="p">.</span><span class="n">compute_log_likelihood</span><span class="p">(</span><span class="n">idata_norm</span><span class="p">)</span>

<span class="k">with</span> <span class="n">t</span><span class="p">:</span>
    <span class="n">pm</span><span class="p">.</span><span class="n">compute_log_likelihood</span><span class="p">(</span><span class="n">idata_t</span><span class="p">)</span>

<span class="n">df_comp_loo</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">compare</span><span class="p">({</span><span class="s">"norm"</span><span class="p">:</span> <span class="n">idata_norm</span><span class="p">,</span> <span class="s">"t"</span><span class="p">:</span> <span class="n">idata_t</span><span class="p">})</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_compare</span><span class="p">(</span><span class="n">df_comp_loo</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/model_averaging_cont/loo.webp" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_comp_loo</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right">rank</th>
      <th style="text-align: right">elpd_loo</th>
      <th style="text-align: right">p_loo</th>
      <th style="text-align: right">elpd_diff</th>
      <th style="text-align: right">weight</th>
      <th style="text-align: right">se</th>
      <th style="text-align: right">dse</th>
      <th style="text-align: left">warning</th>
      <th style="text-align: left">scale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">t</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">929.203</td>
      <td style="text-align: right">2.6402</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0.870715</td>
      <td style="text-align: right">30.6267</td>
      <td style="text-align: right">0</td>
      <td style="text-align: left">False</td>
      <td style="text-align: left">log</td>
    </tr>
    <tr>
      <td style="text-align: left">norm</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">740.195</td>
      <td style="text-align: right">7.50734</td>
      <td style="text-align: right">189.008</td>
      <td style="text-align: right">0.129285</td>
      <td style="text-align: right">30.2578</td>
      <td style="text-align: right">25.8493</td>
      <td style="text-align: left">False</td>
      <td style="text-align: left">log</td>
    </tr>
  </tbody>
</table>

<p>We can also use the LOO Probability Integral Transform (LOO-PIT).
The main idea behind this method is that, if the $y_i$s are distributed
according to $p(\tilde{y} \vert y_{-i}),$ then the LOO PIT</p>

\[P(y_i \leq y^* \vert y_{-i}) = \int_{-\infty}^{y_i} d\tilde{y} p(\tilde{y} \vert y_{-i})\]

<p>should be a uniform distribution.
A very nice explanation of this method can be found in
<a href="https://oriolabril.github.io/gsoc2019_blog/2019/07/31/loo-pit.html">this blog</a>.
In the reference there are unfortunately some missing figure where one can
clearly understand how does the LOO-PIT relates to the posterior predictive
distribution.
We therefore decided to make a similar plot</p>

<p><img src="/docs/assets/images/statistics/model_averaging_cont/ecdf_comp.webp" alt="" /></p>

<p>Le left column corresponds to the posterior predictive distribution, the central one to the LOO-PIT and the right one to the LOO-PIT ECDF.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_loo_pit</span><span class="p">(</span><span class="n">idata_norm</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"yobs"</span><span class="p">,</span> <span class="n">ecdf</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging_cont/loo_pit_norm.webp" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_loo_pit</span><span class="p">(</span><span class="n">idata_t</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"yobs"</span><span class="p">,</span> <span class="n">ecdf</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging_cont/loo_pit_t.webp" alt="" /></p>

<p>It is clear that the normal model is over-dispersed with respect to the observed data,
while the t-Student model gives a LOO-PIT which is compatible with the uniform distribution.</p>

<p>Another related plot which may be useful is the difference between two models’ Expected Log Pointwise Density (ELPD),
defined as</p>

\[\int d\theta \log(p(y_i \vert \theta)) p(\theta \vert y) \approx \frac{1}{S} \sum_s \log(p(y_i \vert \theta^s))\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_elpd</span><span class="p">({</span><span class="s">'norm'</span><span class="p">:</span> <span class="n">idata_norm</span><span class="p">,</span> <span class="s">'t'</span><span class="p">:</span> <span class="n">idata_t</span><span class="p">})</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging_cont/plot_elpd.webp" alt="" /></p>

<p>Since there are many points below 0, we can see that we should favor the t-Student’s model.
There are also few points far below 0, and the t model gives much better results for them.
It is in fact likely that those points are far away from the mean value,
where the normal distribution has very small probability density,
while the t-Student model allows for heavier tails and therefore are more likely to be observed
according to this model.</p>

<p>Notice that Arviz plots the ELPD difference against the point index, it would
be instead better to plot the ELPD difference against one model’s ELPD,
since the ELPD difference only makes sense when compared to one model’s ELPD.
We can however easily overcome this issue as follows</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">elpd_norm</span> <span class="o">=</span> <span class="n">idata_norm</span><span class="p">.</span><span class="n">log_likelihood</span><span class="p">[</span><span class="s">'yobs'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">))</span>
<span class="n">elpd_t</span> <span class="o">=</span> <span class="n">idata_t</span><span class="p">.</span><span class="n">log_likelihood</span><span class="p">[</span><span class="s">'yobs'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">elpd_t</span><span class="p">,</span> <span class="n">elpd_norm</span><span class="o">-</span><span class="n">elpd_t</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'x'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'ELPD diff'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'t ELPD'</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging_cont/plot_elpd_mine.webp" alt="" /></p>

<p>This plot contains much more information with respect to the previous
one, and it tells us that the Student model 
does a better job in reproducing both the points
with very close to the center (those with very high ELPD)
and those far away to the center (those with very small ELPD),
while the normal model only focuses on the intermediate points.</p>

<p>The ELPD has an issue which is not present when using the LOO:
we are using our data twice. For this reason, the LOO method
is generally preferred over any method which relies on the simple
log density of the posterior.</p>

<h2 id="some-warning">Some warning</h2>

<p>While the LOO method gives reasonable results when the number of variables
is not too high, it is known that all the information criteria tend to overfit
when the number of variables grows too much (see Gronau <em>et al.</em>).</p>

<p>Moreover, you should always consider what you need to do with your model.
If you simply need to get the best possible prediction out of your model,
you should probably go for the most general model
and integrate over all the uncertainties.</p>

<p>You should also keep in mind that, if you have nested models, that is
models where one model is a special case of the other, it is generally
recommended by the Bayesian scientific community to stick to the more
general one, regardless on what your metrics tell you,
at least unless you have a reason to put some constraints to your model.</p>

<p>An in-depth discussion about this topic can be found in <a href="https://www.youtube.com/watch?v=D0kVMie93Yk&amp;list=PLBqnAso5Dy7O0IVoVn2b-WtetXQk5CDk6&amp;index=18">Aki Vehtari’s course
on YouTube</a>
(lectures 9.1-9.3).</p>

<h2 id="conclusions">Conclusions</h2>

<p>We discussed how to use the Leave One Out method to compare two models
and how should we read the most relevant plots related to this criterion.
We also discussed some limitation of this model.</p>

<h2 id="recommended-readings">Recommended readings</h2>

<ul>
  <li><cite> Gronau, Q.F., Wagenmakers, EJ. Limitations of Bayesian Leave-One-Out Cross-Validation for Model Selection. Comput Brain Behav 2, 1–11 (2019). https://doi.org/10.1007/s42113-018-0011-7</cite></li>
  <li><cite> Vehtari A., Gelman A., Gabry J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. https://arxiv.org/abs/1507.04544v5</cite></li>
  <li><cite> Navarro, D.J. Between the Devil and the Deep Blue Sea: Tensions Between Scientific Judgement and Statistical Model Selection. Comput Brain Behav 2, 28–34 (2019). https://doi.org/10.1007/s42113-018-0019-z </cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">pytensor</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>
<div class="code">
Last updated: Wed Nov 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.7
<br />
IPython version      : 8.24.0
<br />

<br />
xarray  : 2024.9.0
<br />
pytensor: 2.25.5
<br />
numpyro : 0.15.0
<br />
jax     : 0.4.28
<br />
jaxlib  : 0.4.28
<br />

<br />
numpy     : 1.26.4
<br />
arviz     : 0.20.0
<br />
pandas    : 2.2.3
<br />
seaborn   : 0.13.2
<br />
pymc      : 5.17.0
<br />
yfinance  : 0.2.40
<br />
matplotlib: 3.9.2
<br />

<br />
Watermark: 2.4.3
<br />
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/model_comparison/" /><summary type="html"><![CDATA[Cross validation in Bayesian statistics]]></summary></entry><entry><title type="html">Model comparison</title><link href="http://localhost:4000/statistics/model_averaging" rel="alternate" type="text/html" title="Model comparison" /><published>2024-10-09T00:00:00+00:00</published><updated>2024-10-09T00:00:00+00:00</updated><id>http://localhost:4000/statistics/model_averaging</id><content type="html" xml:base="http://localhost:4000/statistics/model_averaging"><![CDATA[<p>In the majority of cases, you won’t deal with a single model
for one dataset, but you will try many models
at the same time.</p>

<p>In this phase of the Bayesian workflow
we will discuss some methods to compare
models.</p>

<p>Comparing model sometimes may be understood as choosing the best model, but in most cases it means to 
assess which model is better to describe or predict some particular aspect of your data.
Model comparison can be done analytically in some case, but most of the time it will be done numerically or graphically, and here we will give an overview of the most important tools.</p>

<p><br /></p>

<blockquote>
  <p>Since all models are wrong the scientist must be alert
to what is importantly wrong. It is inappropriate to be
concerned about mice when there are tigers abroad.</p>

  <p>George Box</p>
</blockquote>

<p><br /></p>

<p>Here we will take a look at two of the most important
methods, the Bayes factor analysis and the
Leave One Out cross-validation.</p>

<h2 id="bayes-factors">Bayes factors</h2>

<p>Let us go back to the Beta-Binomial model
that we discussed in <a href="/_posts/statistics/betabin.md">this post</a>,
and let us assume that we have two candidate models to describe our data:
model 0 has Jeffreys prior, which mean that the prior
is a beta distribution with $\alpha=1/2$ and $\beta=1/2\,.$
The second model, named “model 2”, is instead centered in $0.5$ and has
\(\alpha = \beta = 10\,.\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="n">y</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">4320</span>

<span class="n">model_0</span> <span class="o">=</span> <span class="p">{</span><span class="s">'a'</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="s">'b'</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">}</span>
<span class="n">model_1</span> <span class="o">=</span> <span class="p">{</span><span class="s">'a'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s">'b'</span><span class="p">:</span> <span class="mi">10</span><span class="p">}</span>

<span class="n">x_pl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">model_0</span><span class="p">,</span> <span class="n">model_1</span><span class="p">]):</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">beta</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">model</span><span class="p">[</span><span class="s">'a'</span><span class="p">],</span> <span class="n">b</span><span class="o">=</span><span class="n">model</span><span class="p">[</span><span class="s">'b'</span><span class="p">]).</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_pl</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s">"model </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="n">legend</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">"$\theta$"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">"$p(\theta)$  "</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/model_averaging/priors.webp" alt="The priors used in this post" /></p>

<p>Given the two models $M_0$ and $M_1$ we may ask which one we prefer, given the data. The probability of the model given the data is given by</p>

\[p(M_k | y) = \frac{p(y | M_k)}{p(y)} p(M_k)\]

<p>where the quantity</p>

\[p(y | M_k)\]

<p>is the <strong>marginal likelihood</strong> of the model.</p>

<p>If we assign the same prior probability $p(M_k)\,,$
to each model,
since $p(y)$ is the same for both models,
then we can simply replace $p(M_k | y)$ with the
marginal likelihood.</p>

<p>As usual, an analytic calculation is only possible in a very limited number of models.</p>

<p>One may think to compute $p(M_k| y)$ by starting from $p(y | \theta, M_k)$ and integrating out $\theta$ but doing this naively is generally not a good idea, as
this method is unstable and prone to numerical errors.</p>

<p>We can however use the Sequential Monte Carlo to compare the two models,
since it allows to estimate the (log) marginal likelihood of the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">traces</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="p">[</span><span class="n">model_0</span><span class="p">,</span> <span class="n">model_1</span><span class="p">]:</span>
    <span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Beta</span><span class="p">(</span><span class="s">"theta"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">m</span><span class="p">[</span><span class="s">'a'</span><span class="p">],</span> <span class="n">beta</span><span class="o">=</span><span class="n">m</span><span class="p">[</span><span class="s">'b'</span><span class="p">])</span>
        <span class="n">yl</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">"yl"</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
        <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_smc</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">return_inferencedata</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>
        <span class="n">models</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">traces</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<p>Let us inspect as usual the traces.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging/trace_0.webp" alt="The trace for model 0" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging/trace_1.webp" alt="The trace for model 1" /></p>

<p>What one usually computes is the <strong>Bayes factor</strong> of the models, which is the ratio between the posterior probability of the model (which in this case is simply the
ratio between the marginal likelihoods).</p>

<table>
  <thead>
    <tr>
      <th>$BF = p(M_0)/p(M_1)$</th>
      <th>interpretation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$BF&lt;10^{0}$</td>
      <td>support to $M_1$ (see reciprocal)</td>
    </tr>
    <tr>
      <td>$10^{0}\leq BF&lt;10^{1/2}$</td>
      <td>Barely worth mentioning support to $M_0$</td>
    </tr>
    <tr>
      <td>$10^{1/2}\leq BF&lt;10^2$</td>
      <td>Substantial support to $M_0$</td>
    </tr>
    <tr>
      <td>$10^{2} \leq BF&lt;10^{3/2}$</td>
      <td>Strong support to $M_0$</td>
    </tr>
    <tr>
      <td>$10^{3/2} \leq BF&lt;10^2$</td>
      <td>Very strong support to $M_0$</td>
    </tr>
    <tr>
      <td>$\geq 10^2$</td>
      <td>Decisive support to $M_0$</td>
    </tr>
  </tbody>
</table>

<p>This can be easily done as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span>
    <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">sample_stats</span><span class="p">.</span><span class="n">log_marginal_likelihood</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
    <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">sample_stats</span><span class="p">.</span><span class="n">log_marginal_likelihood</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span><span class="p">)))</span>
</code></pre></div></div>

<div class="code">
18.175240388473817
</div>

<p>As we can see, there is a substantial preference
for model 0.
We can better understand this result if we compare our estimate with the
frequentist confidence interval,
which we recall being \([0.0004, 0.0028]\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">traces</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">rope</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0004</span><span class="p">,</span> <span class="mf">0.0028</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/model_averaging/forest.webp" alt="The forest plot of the two models" /></p>

<p>We can see that the preferred model HDI corresponds
with the frequentist CI, while the interval 
predicted by the second model only partially
overlaps with the frequentist CI.</p>

<p>We can also inspect the posterior predictive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ppc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">models</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">m</span><span class="p">:</span>
        <span class="n">ppc</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ppc</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yl'</span><span class="p">].</span><span class="n">mean</span><span class="p">([</span><span class="s">'chain'</span><span class="p">,</span> <span class="s">'draw'</span><span class="p">]).</span><span class="n">values</span>
</code></pre></div></div>
<div class="code">
array(7.445)
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ppc</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yl'</span><span class="p">].</span><span class="n">mean</span><span class="p">([</span><span class="s">'chain'</span><span class="p">,</span> <span class="s">'draw'</span><span class="p">]).</span><span class="n">values</span>
</code></pre></div></div>
<div class="code">
array(16.98925)
</div>

<p>We recall that the observed value for $y$ was 7,
which is much closer to the one provided by the preferred
model than to the one provided by Model 1.</p>

<h2 id="conclusions">Conclusions</h2>

<p>In this post we discussed the Bayes factor to choose between different models.
In the next post, we will discuss a more powerful method to compare models,
namely the Leave One Out cross validation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">pytensor</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>
<div class="code">
Last updated: Wed Nov 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.7
<br />
IPython version      : 8.24.0
<br />

<br />
xarray  : 2024.9.0
<br />
pytensor: 2.25.5
<br />
numpyro : 0.15.0
<br />
jax     : 0.4.28
<br />
jaxlib  : 0.4.28
<br />

<br />
pymc      : 5.17.0
<br />
numpy     : 1.26.4
<br />
matplotlib: 3.9.2
<br />
pandas    : 2.2.3
<br />
arviz     : 0.20.0
<br />

<br />
Watermark: 2.4.3
<br />

<br />
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/model_comparison/" /><summary type="html"><![CDATA[How to choose between models]]></summary></entry><entry><title type="html">Re-parametrizing your model</title><link href="http://localhost:4000/statistics/reparametrization" rel="alternate" type="text/html" title="Re-parametrizing your model" /><published>2024-10-02T00:00:00+00:00</published><updated>2024-10-02T00:00:00+00:00</updated><id>http://localhost:4000/statistics/reparametrization</id><content type="html" xml:base="http://localhost:4000/statistics/reparametrization"><![CDATA[<p>Model re-parametrization is a part of the Bayesian workflow that will likely face any advanced
user. On the other hand, if you are facing Bayesian inference for the first time, you might
safely skip this (rather technical) post, which is about advanced concepts
in MCMC.</p>

<h2 id="when-you-will-encounter-this-problem">When you will encounter this problem</h2>

<p>The NUTS sampler is an amazing tool, but it is of course not perfect,
and there are circumstances when even this tool has some issue.
This is especially true in high dimensional, multiscale and highly correlated
problems, where moving into the high density region of the posterior
probability requires moving in a non-trivial space.
A typical, well known class of models where this is likely to happen is
the family of <a href="/statistics/hierarchical_models">multilevel (or hierarchical)</a> models,
where even for a moderately high number of variables the sample
space might become troublesome.
Here we will re-phrase into the PyMC language <a href="https://mc-stan.org/users/documentation/case-studies/pool-binary-trials.html">this post
</a>.
which discusses the same issue when dealing with Stan.</p>

<h2 id="fitting-the-efron-morris-dataset">Fitting the Efron-Morris dataset</h2>

<p>Here we will analyze the well known “batting average” Efron-Morris
dataset, which can be found <a href="https://raw.githubusercontent.com/pymc-devs/pymc4/master/notebooks/data/efron-morris-75-data.tsv">here</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/pymc-devs/pymc4/master/notebooks/data/efron-morris-75-data.tsv'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<p>In order to fit this dataset we will use a hierarchical
model for the log-odds, similar to the one discussed <a href="statistics/hierarchical_models">here</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">centered_logit_model</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">log_sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'log_sigma'</span><span class="p">,</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigma</span><span class="p">))</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">invlogit</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'Hits'</span><span class="p">],</span> <span class="n">n</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'At-Bats'</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">)</span>

<span class="k">with</span> <span class="n">centered_logit_model</span><span class="p">:</span>
    <span class="n">idata_centered</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

</code></pre></div></div>

<p>By running this code I got 517 divergences, and these terrible traces</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_centered</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/reparametrization/trace_centered.webp" alt="
The trace plot of the centered parametrization" /></p>

<p>As discussed in <a href="https://arxiv.org/pdf/1312.0906">this paper</a>,
this is a common issue in hierarchical models.
We can however circumvent this issue by observing that,
if</p>

\[\alpha_i \sim \mathcal{N}(\mu, \sigma)\]

<p>then we can rewrite the above as</p>

\[\alpha_i = \mu + \sigma \phi_i\]

<p>where</p>

\[\phi_i \sim \mathcal{N}(0, 1)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">noncentered_logit_model</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">log_sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'log_sigma'</span><span class="p">,</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigma</span><span class="p">))</span>
    <span class="n">alpha_std</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha_std'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">+</span><span class="n">sigma</span><span class="o">*</span><span class="n">alpha_std</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">invlogit</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'Hits'</span><span class="p">],</span> <span class="n">n</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'At-Bats'</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">)</span>

<span class="k">with</span> <span class="n">noncentered_logit_model</span><span class="p">:</span>
    <span class="n">idata_noncentered</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_noncentered</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/reparametrization/trace_noncentered.webp" alt="
The trace plot of the non-centered parametrization" /></p>

<p>This is by far way better than the centered parametrization, and it worked
so well because now our variables are well separated.
Thanks to the re-parametrization, the sampling space
is isotropic within good approximation, and it is therefore easier
for the NUTS sampler to explore it.
We can also take a look at the different landscapes in the $\mu-\sigma$
plane</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_pair</span><span class="p">(</span><span class="n">idata_centered</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'mu'</span><span class="p">,</span> <span class="s">'log_sigma'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/reparametrization/kde_centered.webp" alt="
The kde plot of the centered parametrization" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_pair</span><span class="p">(</span><span class="n">idata_noncentered</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'mu'</span><span class="p">,</span> <span class="s">'log_sigma'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/reparametrization/kde_noncentered.webp" alt="
The kde plot of the non-centered parametrization" /></p>

<p>As we can see, the centered parametrization had problems
in exploring the region with a lower probability density,
while the non-centered one also explored this region.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We discussed how model re-parametrization greatly improved the MCMC
sampling procedure, by allowing you to sample clean traces even
for highly correlated models.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">pytensor</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>

<div class="code">
Last updated: Wed Nov 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.7
<br />
IPython version      : 8.24.0
<br />

<br />
xarray  : 2024.9.0
<br />
pytensor: 2.25.5
<br />
numpyro : 0.15.0
<br />
jax     : 0.4.28
<br />
jaxlib  : 0.4.28
<br />

<br />
numpy     : 1.26.4
<br />
pymc      : 5.17.0
<br />
pandas    : 2.2.3
<br />
seaborn   : 0.13.2
<br />
arviz     : 0.20.0
<br />
matplotlib: 3.9.2
<br />

<br />
Watermark: 2.4.3
<br />
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/model_comparison/" /><summary type="html"><![CDATA[Building equivalent models with less numerical issues]]></summary></entry><entry><title type="html">Predictive checks</title><link href="http://localhost:4000/statistics/predictive_checks" rel="alternate" type="text/html" title="Predictive checks" /><published>2024-09-25T00:00:00+00:00</published><updated>2024-09-25T00:00:00+00:00</updated><id>http://localhost:4000/statistics/predictive_checks</id><content type="html" xml:base="http://localhost:4000/statistics/predictive_checks"><![CDATA[<p>In this post we will collect two different
phases of the Bayesian workflow, namely the
<strong>prior predictive checks</strong> and the
<strong>posterior predictive checks</strong>.</p>

<p>The first one is aimed to ensure that your priors
are compatible with the current domain knowledge,
and it doesn’t require the comparison of the model
prediction with the true data.</p>

<p>The posterior predictive checks, instead, are focused
on clarifying if your fitted model can catch the relevant
aspects of the data.
You should always keep in mind that, even if it does a good job in
describing the known data, this does not imply that
the model will be successful in predicting future observations.</p>

<p><br /></p>

<blockquote>
  <p>I believe that it is possible to learn from experience. That is where
my faith comes in. And I think that all scientists who believe the same are consciously
or unconsciously exercising an act of faith.</p>

  <p>J. O. Irwin</p>
</blockquote>

<p><br /></p>

<p>While the two phases are in principle different,
they use similar methods, and for this reason
they are collected in a single post.</p>

<h2 id="prior-predictive-checks">Prior predictive checks</h2>

<p>When you are performing a prior predictive check,
you are verifying if your model is flexible
enough to include what you know about the problem and that you are
implementing the appropriate constraints.</p>

<p>There are many ways you can perform this,
and this can be done by generating fake data
according to what you know about the problem
and then fit them.</p>

<p>If you know nothing, you may decide to pick a 
dataset sub-sample and ensure that it is included
within the prior predictive. Remember,
however, that you do not want to <em>fit</em> the sub-sample,
otherwise you may end up overfitting your data.</p>

<h2 id="the-twitter-data-again">The twitter data again</h2>

<p>Let us consider again the dataset introduced
in the <a href="/statistics/negbin">post on the Negative Binomial</a>.
We already discussed some checks in that post,
and we carefully chose the value of the parameters
by making an educated guess on the order of magnitude
of the interactions.</p>

<p>Let us now assume that this time we made some
error in the procedure and we take</p>

\[\begin{align}
\theta &amp; \sim \mathcal{B}(1/2, 1/2)\\
\nu &amp; \sim \mathcal{Exp}(20)
\end{align}\]

<p>It is not rare to mess up with the parametrization,
so we may have confused $\lambda$ with its inverse
(it is more common than what you may imagine).
While the imported libraries and the data are the same,
this time the model reads as follows</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./data/tweets.csv'</span><span class="p">)</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">pp0</span><span class="p">:</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'nu'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Beta</span><span class="p">(</span><span class="s">'theta'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">NegativeBinomial</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">nu</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">yobs</span><span class="p">)</span>
</code></pre></div></div>

<p>Let us now sample the prior predictive</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pp0</span><span class="p">:</span>
    <span class="n">prior_pred_pp0</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_prior_predictive</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span>
<span class="n">prior_pred_pp0</span><span class="p">.</span><span class="n">prior_predictive</span><span class="p">[</span><span class="s">'y'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/predictive/prior_predictive.webp" alt="The histogram of the prior predictive distribution" /></p>

<p>We can see that the value $y=0$ has a probability
greater than the $90\%.$
Let us now compute this probability. This can be done by simply counting
the fraction of sampled points equal to 0, and this can be done as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">prior_pred_pp0</span><span class="p">.</span><span class="n">prior_predictive</span><span class="p">[</span><span class="s">'y'</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<div class="code">
xarray.DataArray
'y'
<br />
    array(0.97648148)

<br />
<br />
<ul>
    <li> Coordinates: (0)</li>
    <li> Indexes: (0) </li>
    <li> Attributes: (0) </li>
</ul>
</div>

<p>It doesn’t really make much sense to start
from a model which predicts that the $98\%$
of our tweets have zero interaction.</p>

<p>At this point, a wise Bayesian would go back and
check again the model. Let us see what happens
to the unwise Bayesian who fits the model
despite the unreasonable conclusion which
follows from the prior.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pp0</span><span class="p">:</span>
    <span class="n">trace_pp0</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_pp0</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/predictive/trace-wrong.webp" alt="The trace plot of the unwise Bayesian" /></p>

<p>With the old model, the value of $\nu$
was peaked at $\nu = 2\,,$
while this time we have a peak at around $0.8\,.$
This means that our inference is strongly
biased by our prior.
In fact, our posterior predictive
is much worse that the one in the old post.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pp0</span><span class="p">:</span>
    <span class="n">pp</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_pp0</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">pp</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/predictive/posterior_predictive.webp" alt="The posterior predictive plot" /></p>

<p>While our old model gave us,
on average, the correct probability for the tweet
with the lowest interaction number,
this time we overestimate the number
of tweets with few interactions.</p>

<p>Also in this case, the wise Bayesian would
stop and go back to the model construction.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We discussed how to implement some prior predictive
and posterior predictive check,
together with the risks that comes by
not doing them.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">pytensor</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>

<div class="code">
Last updated: Wed Nov 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.7
<br />
IPython version      : 8.24.0
<br />

<br />
xarray  : 2024.9.0
<br />
pytensor: 2.25.5
<br />
numpyro : 0.15.0
<br />
jax     : 0.4.28
<br />
jaxlib  : 0.4.28
<br />

<br />
pymc      : 5.17.0
<br />
numpy     : 1.26.4
<br />
matplotlib: 3.9.2
<br />
arviz     : 0.20.0
<br />
pandas    : 2.2.3
<br />

<br />
Watermark: 2.4.3
<br />
</div>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/predictive_checks/" /><summary type="html"><![CDATA[Verifying the predictions of your model]]></summary></entry></feed>