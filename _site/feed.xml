<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-02-16T13:10:50+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Data Perspectives</title><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><entry><title type="html">Introduction to Extreme Values theory</title><link href="http://localhost:4000/statistics/extreme_intro" rel="alternate" type="text/html" title="Introduction to Extreme Values theory" /><published>2024-02-02T00:00:00+01:00</published><updated>2024-02-02T00:00:00+01:00</updated><id>http://localhost:4000/statistics/extreme_intro</id><content type="html" xml:base="http://localhost:4000/statistics/extreme_intro"><![CDATA[<p>In some circumstances you may be not interested in modelling the distribution itself,
 but you may be interested in understanding its asymptotic behavior, and the extreme value theory is the discipline which studies this topic.</p>

<p>The EV theory is appropriate when you want to investigate the distribution
of the minimum or maximum value of some quantity,
as the maximum loss of a financial asset or the yearly maximum
volume of rain in a certain location.</p>

<p>The intuition behind the extreme value theory is that any probability distribution
function is positive and must integrate to one,
it must therefore fall to zero fast enough if $x \rightarrow \infty\,.$
This puts strong constraints to its asymptotic behavior,
and this leads to the <a href="https://en.wikipedia.org/wiki/Fisher%E2%80%93Tippett%E2%80%93Gnedenko_theorem">Fisher-Tippet-Gnedenko theorem</a>.</p>

<p>Formally if we have a continuous positive random variable $X$
with cumulative distribution function $F(x)\,,$
and we observe $X_1,…,X_n$ independent identically distributed
variables distributed according to $X\,,$
if we denote $M_n$ the maximum of $X_1,…,X_n\,,$ then</p>

<p>$P(M_n \leq x) = P(X_1 \leq x) P(X_2 \leq x) … P(X_n \leq x) = (F(x))^n$</p>

<p>However one may not know $F$ a priori, but the FTG theorem states that,
if there exist $a_n, b_n \in \mathbb{R}$ such that</p>

\[P\left( \frac{M_n - a_n}{b_n} \leq x \right) \rightarrow G(x)\]

<p>then \(G(x) \propto \exp{\left(-(1+ \xi x)^{-1/\xi}\right)}\,.\)</p>

<p>Once properly normalized and promoted to a location-scale family one arrives to the Generalized Extreme Value distribution:</p>

\[p(x) = \frac{1}{\sigma} t(x)^{\xi + 1}e^{- t(x)}\]

<p>where</p>

\[t(x) =
\begin{cases}
\left(1+ \xi \left(\frac{x-\mu}{\sigma}\right)\right)^{-1/\xi}\,&amp; if\,&amp;  \xi \neq 0 \\
e^{-\left(x-\mu\right)/\sigma}\,&amp; if\,&amp; \xi = 0\\
\end{cases}\]

<p>Notice that, if $X_1,…, X_n$ are distributed according to $G\,,$ then $\max(X_1,…,X_n)$ is distributed according to $G\,.$
This distribution is known as the <strong>Generalized Extreme Value</strong> (GEV) distribution.</p>

<h2 id="maximum-distribution-of-the-apple-stocks">Maximum distribution of the Apple stocks</h2>

<p>I have been working on financial risk assessment for a while, and
one of the central issues in this field is to determine the
risk due to extremely large fluctuations of the stock market.
EVT can be really helpful in this task, and we will show how in this post.
We will use <a href="https://pypi.org/project/yfinance/">Yahoo Finance</a> to download the values of the Apple stock
in the period from the January 1st 2020 to the December 31st 2023.</p>

<p>The Generalized Extreme Values distribution is not directly available
in PyMC, but can be found in the <a href="https://www.pymc.io/projects/experimental/en/latest/index.html">pymc_experimental</a> package.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="n">pymc_experimental.distributions</span> <span class="k">as</span> <span class="n">pmx</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">yfinance</span> <span class="k">as</span> <span class="n">yf</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">yf</span><span class="p">.</span><span class="nf">download</span><span class="p">(</span><span class="sh">'</span><span class="s">AAPL</span><span class="sh">'</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="sh">'</span><span class="s">2020-01-01</span><span class="sh">'</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="sh">'</span><span class="s">2023-12-01</span><span class="sh">'</span><span class="p">).</span><span class="nf">reset_index</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">Date</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">Date</span><span class="sh">'</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">LogRet</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">Close</span><span class="sh">'</span><span class="p">]).</span><span class="nf">diff</span><span class="p">()</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">dropna</span><span class="p">()</span>
</code></pre></div></div>

<p>First of all, we converted the close values (the value of the stock at the end of
the day) into logarithmic-returns (log-returns for short).
This is a common operation in finance, since for compound interest
assets the total value is</p>

\[\prod_i (1+r_i)\]

<p>If we take the logarithm of the above formula we transform the product into a sum,
and this makes log-returns so useful.</p>

<p>We are interested in finding the distribution of the weekly minima
of the daily close.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">groupby</span><span class="p">([</span><span class="n">pd</span><span class="p">.</span><span class="nc">Grouper</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="sh">'</span><span class="s">Date</span><span class="sh">'</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="sh">'</span><span class="s">W</span><span class="sh">'</span><span class="p">)])[</span><span class="sh">'</span><span class="s">LogRet</span><span class="sh">'</span><span class="p">].</span><span class="nf">min</span><span class="p">().</span><span class="nf">reset_index</span><span class="p">()</span>
<span class="n">dt</span> <span class="o">=</span> <span class="o">-</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">LogRet</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span>
</code></pre></div></div>

<p>Before fitting the model, let us take a look at the behavior of the data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">Date</span><span class="sh">'</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">LogRet</span><span class="sh">'</span><span class="p">])</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">LogRet</span><span class="sh">'</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/extreme_intro/logret.webp" alt="" /></p>

<p>There is some evident time dependence. As an example, we can observe quite
a high volatility during the Covid pandemic and another high volatility
period after the Ukraine invasion.
However, for the moment, we will neglect the time dependence, and assume that
the parameters are stationary.</p>

<p>Since we have quite a large amount of data, we can safely use uninformative priors.
We do expect that both $\mu$ and $\sigma$ are typically much smaller than
one, so we will take a standard deviation of 2 for the first one and
equal to 1 for the latter.</p>

<p>From the <a href="https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution">Wikipedia page</a> we observe that, if 
\(| \xi|&gt;1\,,\) the mean does not exists.
Since it is reasonable to assume that it exists, we expect that $\xi$
will be bounded into the $[-1, 1]$ region, therefore we have the following
model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_gev</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">mu</span><span class="sh">'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">'</span><span class="s">sigma</span><span class="sh">'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">xi</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">xi</span><span class="sh">'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">gev</span> <span class="o">=</span> <span class="n">pmx</span><span class="p">.</span><span class="nc">GenExtreme</span><span class="p">(</span><span class="sh">'</span><span class="s">gev</span><span class="sh">'</span><span class="p">,</span><span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="n">xi</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/extreme_intro/trace.webp" alt="The trace of our model" /></p>

<p>Let us take a look at the joint posterior distribution.</p>

<p><img src="/docs/assets/images/statistics/extreme_intro/kde.webp" alt="The KDE plot of the posterior distribution" /></p>

<p>We can now take a look at the PPC in order to verify if our model
is able to reproduce the data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model_gev</span><span class="p">:</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="nf">plot_ppc</span><span class="p">(</span><span class="n">ppc</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="sh">'</span><span class="s">kde</span><span class="sh">'</span><span class="p">,</span> <span class="n">num_pp_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/extreme_intro/ppc.webp" alt="The posterior predictive distribution" /></p>

<p>There’s a very good agreement between the observed and the predicted values,
so our estimate should be quite reliable.</p>

<h2 id="the-generalized-pareto-distribution">The Generalized Pareto distribution</h2>

<p>Keeping only the extreme values may be a waste of information. As an example, we only kept the
weekly maxima, so we trowed away four days out of five.
In some situation, instead of analyzing what is the distribution probability for the maxima,
it may be better to analyze what is the probability that your random variable exceeds some threshold.
More precisely, given $u,y&gt;0\,,$ we want to get information on</p>

\[P(X&gt;u+y | X&gt;u) = \frac{P((X&gt;u+y)\cap (X&gt;u))}{P(X&gt;u)} = \frac{P(X&gt;u+y)}{P(X&gt;u)} = \frac{1-F(u+y)}{1-F(u)}\]

<p>It can be proved (see Coles’ textbook for the outline) that, for large enough $u\,,$
the above distribution must have the form</p>

\[p(y | u, \sigma, \xi) = \left(1+\frac{\xi y}{\sigma}\right)^{-1/\xi}\]

<p>The distribution</p>

\[p(y | \mu, \sigma, \xi) = \left(1+\xi \frac{y-\mu}{\sigma}\right)^{-1/\xi}\]

<p>is named the <strong>Generalized Pareto Distribution</strong> (GPD).
For the mathematical details on the above distribution, see the
<a href="https://en.wikipedia.org/wiki/Generalized_Pareto_distribution">corresponding Wikipedia page</a>.</p>

<p>Now it comes one bad news and one good news. The bad one is that in PyMC it is only
implemented the Pareto type I distribution, which is a special case of the GPD.
The good one is that it is really easy to implement custom distributions in PyMC,
and this can be done following <a href="https://www.pymc.io/projects/examples/en/2022.12.0/howto/custom_distribution.html">this very nice tutorial</a>.
You can find my own implementation <a href="https://github.com/thestippe/thestippe.github.io/blob/main/scripts/generalized_pareto.py">on my github repo</a>.</p>

<p>Let us see how to model the tail of the Apple stocks by using it.
A reasonably high enough threshold for the log returns is $0.03\,,$
as this value is high enough to be far from the center and low enough to provide
a discrete amount of data.
We do expect $\sigma \ll 1\,,$ therefore assuming a variance of 1 for it may be enough.
$\xi$ must be lower than 1. If it is 1, then the mean
does not exists, and this doesn’t make much sense. 
If $\xi$ is negative, then the support of the GDP has an upper bound,
and it doesn’t make much sense too, so we can assume it is non-negative.
We can therefore take a half normal distribution for it, with variance 10.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">generalized_pareto</span> <span class="kn">import</span> <span class="n">GPD</span>

<span class="n">dt1</span> <span class="o">=</span> <span class="o">-</span><span class="n">df</span><span class="p">[</span><span class="o">-</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">LogRet</span><span class="sh">'</span><span class="p">]</span><span class="o">&gt;</span><span class="n">thr</span><span class="p">][</span><span class="sh">'</span><span class="s">LogRet</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">pareto_model</span><span class="p">:</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">'</span><span class="s">sigma</span><span class="sh">'</span><span class="p">,</span><span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">xi</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">'</span><span class="s">xi</span><span class="sh">'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="nc">GPD</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">thr</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="n">xi</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">dt1</span><span class="p">)</span>
    <span class="n">trace_pareto</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_pareto</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/extreme_intro/trace_pareto.webp" alt="The trace of the Pareto model" /></p>

<p>Notice that in our model we fixed $\mu$ to the threshold, which is fixed.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pareto_model</span><span class="p">:</span>
    <span class="n">ppc_pareto</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_pareto</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="nf">plot_ppc</span><span class="p">(</span><span class="n">ppc</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">([</span><span class="n">thr</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/extreme_intro/ppc_pareto.webp" alt="The posterior predictive of the Pareto model" /></p>

<p>In the last figure, the mean has been removed as Arviz has some issues in computing
the mean for this posterior predictive, probably because of the heavy tails or
due to the discontinuity at the threshold.
Regardless from this, the agreement between the posterior predictive and the
data looks perfect.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We introduced the Extreme Value theory, and we first applied it by
fitting the weekly minima of the Apple stocks by using the GEV distribution.
We then showed how to fit the data above a fixed threshold by using the generalized Pareto
distribution.</p>

<h2 id="suggested-readings">Suggested readings</h2>

<ul>
  <li><cite>Haan, L. d., Ferreira, A. (2006). Extreme Value Theory: An Introduction. UK: Springer New York.</cite></li>
  <li><cite>Coles, S. (2001). An Introduction to Statistical Modeling of Extreme Values. Germany: Springer London.</cite></li>
</ul>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/extreme_values_intro/" /><summary type="html"><![CDATA[Describing rare events]]></summary></entry><entry><title type="html">Application of survival analysis 1</title><link href="http://localhost:4000/statistics/survival_example" rel="alternate" type="text/html" title="Application of survival analysis 1" /><published>2024-02-01T00:00:00+01:00</published><updated>2024-02-01T00:00:00+01:00</updated><id>http://localhost:4000/statistics/survival_example</id><content type="html" xml:base="http://localhost:4000/statistics/survival_example"><![CDATA[<p>In the previous post we introduced survival
analysis and we discussed how to correctly treat
censorship.
In this post we will see an application of survival analysis.</p>

<h2 id="the-study">The study</h2>
<p>In this post we will use the “E1684” melanoma dataset available in the SurvSet python package.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">SurvSet.data</span> <span class="kn">import</span> <span class="n">SurvLoader</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">()</span>
<span class="n">loader</span> <span class="o">=</span> <span class="nc">SurvLoader</span><span class="p">()</span>

<span class="n">df_melanoma</span><span class="p">,</span> <span class="n">ref_melanoma</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="nf">load_dataset</span><span class="p">(</span><span class="n">ds_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">e1684</span><span class="sh">'</span><span class="p">).</span><span class="nf">values</span><span class="p">()</span>

<span class="c1"># Dataset reference
</span>
<span class="n">ref_melanoma</span>
</code></pre></div></div>

<div class="code">
<a href="https://www.rdocumentation.org/packages/smcure/versions/2.0/topics/e1684">https://www.rdocumentation.org/packages/smcure/versions/2.0/topics/e1684</a>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_melanoma</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">pid</th>
      <th style="text-align: right">event</th>
      <th style="text-align: right">time</th>
      <th style="text-align: right">num_age</th>
      <th style="text-align: left">fac_sex</th>
      <th style="text-align: left">fac_trt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1.15068</td>
      <td style="text-align: right">-11.0359</td>
      <td style="text-align: left">M</td>
      <td style="text-align: left">IFN</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">0.62466</td>
      <td style="text-align: right">-5.12904</td>
      <td style="text-align: left">M</td>
      <td style="text-align: left">IFN</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">1.89863</td>
      <td style="text-align: right">23.186</td>
      <td style="text-align: left">F</td>
      <td style="text-align: left">Control</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">3</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">0.45479</td>
      <td style="text-align: right">11.1449</td>
      <td style="text-align: left">F</td>
      <td style="text-align: left">Control</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">4</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2.09041</td>
      <td style="text-align: right">-13.3208</td>
      <td style="text-align: left">M</td>
      <td style="text-align: left">Control</td>
    </tr>
  </tbody>
</table>

<p>There variable “time” is our regression variable, the “event” column indicates if
the event happened, and its value is 1 if the event happened (the patient died)
while it is 0 if the event is censored.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">len</span><span class="p">(</span><span class="n">df_melanoma</span><span class="p">)</span>
</code></pre></div></div>

<div class="code">
284
</div>

<p>Let us count how many entries are censored</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">df_melanoma</span><span class="p">[</span><span class="sh">'</span><span class="s">event</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
</code></pre></div></div>

<div class="code">
88
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_melanoma</span><span class="p">[</span><span class="sh">'</span><span class="s">time</span><span class="sh">'</span><span class="p">].</span><span class="nf">max</span><span class="p">()</span>
</code></pre></div></div>
<div class="code">
9.64384
</div>

<h2 id="the-model">The model</h2>

<p>We will use a Weibull likelihood, which is a quite flexible distribution,
which allows for fat tails, and it should thereby be more robust than
a Gamma distribution.</p>

\[Y \sim \mathcal{Weibull}(\sigma, \lambda)\]

<p>The Weibull distribution has pdf</p>

\[p(x | \alpha, \beta) = \alpha \frac{x^{\alpha-1}}{\beta^\alpha} e^{-(x/\beta)^\alpha}\]

<p>Both the parameters must be positive, and the mean of the distribution is</p>

\[\mu = \beta \Gamma\left(1+\alpha^{-1}\right)\]

<p>We want to assess the effectiveness of the treatment. The first possibility
is to fit two different models, one per treatment. Another very common
possibility is to use the treatment as a regression variable, and we will
use this method.
We define the covariate</p>

\[x =
\begin{cases}
1 &amp; treatment=IFN\\
0 &amp; treatment=Control\\
\end{cases}\]

<p>and we assume</p>

\[\begin{align}
&amp;
\lambda = \exp\left(\beta_0 + \beta_1 x \right)
\\
&amp;
\sigma = \exp\left(\alpha \right)
\\
&amp;
\alpha, \beta_i \sim \mathcal{N}(0, 100)
\\
\end{align}\]

<p>In this way we ensure that both the parameters are positive,
and the priors are very uninformative.
Let us introduce the censoring variable</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_melanoma</span><span class="p">[</span><span class="sh">'</span><span class="s">censoring</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span> <span class="k">if</span> <span class="n">x</span><span class="o">==</span><span class="mi">1</span> <span class="k">else</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">df_melanoma</span><span class="p">[</span><span class="sh">'</span><span class="s">event</span><span class="sh">'</span><span class="p">],</span> <span class="n">df_melanoma</span><span class="p">[</span><span class="sh">'</span><span class="s">time</span><span class="sh">'</span><span class="p">])]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">lam</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">beta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">df_melanoma</span><span class="p">[</span><span class="sh">'</span><span class="s">trt</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Weibull</span><span class="p">.</span><span class="nf">dist</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">alpha</span><span class="p">),</span> <span class="n">beta</span><span class="o">=</span><span class="n">lam</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Censored</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">dist</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="n">df_melanoma</span><span class="p">[</span><span class="sh">'</span><span class="s">censoring</span><span class="sh">'</span><span class="p">],</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_melanoma</span><span class="p">[</span><span class="sh">'</span><span class="s">time</span><span class="sh">'</span><span class="p">])</span>
    
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/survival_melanoma/trace.webp" alt="The trace of our model" /></p>

<h2 id="treatment-comparison">Treatment comparison</h2>

<p>By the above figure we observe that $\beta_1&gt;0\,,$
and this indicates that the test treatment is more effective than the control one.
This becomes clearer by showing the distribution of the mean $\mu$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">mu0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="nf">gammaln</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))))</span>
<span class="n">mu1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="nf">gammaln</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">mu0</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Control</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">mu1</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">IFN</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">15</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">legend</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/survival_melanoma/mean.webp" alt="The posterior for the parameter mu" /></p>

<p>The mean for the test treatment is typically higher for the test group
than for the control group, and the peak of the mean for the IFN
treatment is roughly twice than the one for the control treatment.</p>

<p>Let us also take a look at the survival function, which is simply</p>

\[S(t, \alpha, \beta) = e^{-(t/\beta)^\alpha}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">S</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span><span class="o">/</span><span class="n">beta</span><span class="p">)</span><span class="o">**</span><span class="n">alpha</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="p">)</span>

<span class="n">t_pl</span> <span class="o">=</span>  <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="n">s0</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nc">S</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">alph</span><span class="p">,</span> <span class="n">b0</span><span class="p">))</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_pl</span><span class="p">]</span>

<span class="n">alph</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">b0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">s0</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nc">S</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">alph</span><span class="p">,</span> <span class="n">b0</span><span class="p">))</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_pl</span><span class="p">]</span>
<span class="n">s0_low</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">quantile</span><span class="p">(</span><span class="nc">S</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">alph</span><span class="p">,</span> <span class="n">b0</span><span class="p">),</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_pl</span><span class="p">]</span>
<span class="n">s0_high</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">quantile</span><span class="p">(</span><span class="nc">S</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">alph</span><span class="p">,</span> <span class="n">b0</span><span class="p">),</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_pl</span><span class="p">]</span>

<span class="n">s1</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nc">S</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">alph</span><span class="p">,</span> <span class="n">b1</span><span class="p">))</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_pl</span><span class="p">]</span>
<span class="n">s1_low</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">quantile</span><span class="p">(</span><span class="nc">S</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">alph</span><span class="p">,</span> <span class="n">b1</span><span class="p">),</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_pl</span><span class="p">]</span>
<span class="n">s1_high</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">quantile</span><span class="p">(</span><span class="nc">S</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">alph</span><span class="p">,</span> <span class="n">b1</span><span class="p">),</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_pl</span><span class="p">]</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">t_pl</span><span class="p">,</span> <span class="n">s0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Control</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">t_pl</span><span class="p">,</span> <span class="n">s0_low</span><span class="p">,</span> <span class="n">s0_high</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">lightgray</span><span class="sh">'</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">t_pl</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">IFN</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">t_pl</span><span class="p">,</span> <span class="n">s1_low</span><span class="p">,</span> <span class="n">s1_high</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">t_pl</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">S(t)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">legend</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/survival_melanoma/survival.webp" alt="The survival functions" /></p>

<p>We can safely conclude that, for the patients in this study, the IFN
treatment gives better results than the control one.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We discussed an application of survival analysis with continuous time, we explained how
to include the regressor dependence in bayesian survival analysis
and we also introduced the Weibull distribution.</p>

<h2 id="suggested-readings">Suggested readings</h2>

<ul>
  <li><cite>Ibrahim, J. G., Chen, M., Sinha, D. (2013). Bayesian Survival Analysis. Switzerland: Springer New York.</cite></li>
</ul>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/survival_continuous/" /><summary type="html"><![CDATA[Survival analysis with continouous time]]></summary></entry><entry><title type="html">Introduction to survival analysis</title><link href="http://localhost:4000/statistics/survival_analysis" rel="alternate" type="text/html" title="Introduction to survival analysis" /><published>2024-01-31T00:00:00+01:00</published><updated>2024-01-31T00:00:00+01:00</updated><id>http://localhost:4000/statistics/survival_analysis</id><content type="html" xml:base="http://localhost:4000/statistics/survival_analysis"><![CDATA[<p>There are situations where your task is to estimate the waiting time before
a certain event happens, and survival analysis is the branch of statistics
which deals with this kind of study.</p>

<p>In general, time can be either considered a continuous variable or a discrete
one. For the moment we will assume that it’s a continuous one.</p>

<p>Since we are dealing with a waiting time, our variable must be non-negative.</p>

<p>We will focus for now on parametric models, although non-parametric models
are very popular in survival analysis.</p>

<p>The analyzed event can be either the time before a component fails
or the occurrence of some biological change like the infection of one patient
or even the next eruption of a volcano.</p>

<h2 id="mathematical-background">Mathematical background</h2>

<p>Let us consider a random variable $T$ with pdf $p$ and cumulative $F\,,$ we define the <strong>survival function</strong> $S$ as:</p>

\[F(t) = P(t\leq T) = \int_0^t p(u) du = 1-S(t)\]

<p>We assume that at $t=0$ the event is not happened, so $S(0)=1$ while we
assume that we are certain that the event must occur, so $\lim_{t\rightarrow \infty} S(t)=0\,.$</p>

<p>We may alternatively assume that the event does not happen with probability $p_0\,,$
and in this case we may modify the above assumption with 
$\lim_{t\rightarrow \infty} S(t)=p_0\,.$</p>

<p>We define the <strong>hazard function</strong> as</p>

\[h(t) = \lim_{\Delta t \rightarrow 0} \frac{P(t&lt; T \leq t+\Delta t | T&gt;t)}{\Delta t} = \lim_{\Delta t \rightarrow 0} \frac{P((t&lt; T \leq t+\Delta t) \cap T&gt;t)}{P(T&gt;t)\Delta t} = \lim_{\Delta t \rightarrow 0} \frac{P(t &lt; T \leq t+\Delta t)  }{\Delta t} \frac{1}{P(T&gt;t)}  =
\frac{1}{S(t)}\lim_{\Delta t \rightarrow 0} \frac{F(t+\Delta t) - F(t)}{\Delta t }= \frac{F'(t)}{S(t)}  = \frac{f(t)}{S(t)}\]

<p>Since $h$ is the ratio of two positive quantities, it is positive itself.</p>

<p>We have that</p>

\[h(t) =  \frac{f(t)}{S(t)} = -\frac{S'(t)}{S(t)} = -\frac{d}{dt}\log S(t)\]

<p>which can be inverted by first integrating and then exponentiating:</p>

\[S(t) = \exp\left(-\int_0^t h(u) du\right)\]

<p>Notice that, if we assume that $\lim_{t\rightarrow \infty} S(t)=0\,,$ we must require that $\lim_{t\rightarrow \infty}\int_0^t h(u) du = \infty\,.$
If we otherwise assume that $\lim_{t\rightarrow \infty} S(t)=p_0\,,$ we must require that $\lim_{t\rightarrow \infty}\int_0^t h(u) du = -\log\left(p_0\right)\,.$</p>

<p>We define the <strong>cumulative hazard function</strong> as</p>

\[H(t) = \int_0^t h(u) du\]

<p>And it is related to the survival function by</p>

\[S(t) = \exp\left(-H(t)\right)\]

<h2 id="censoring">Censoring</h2>

<p>One of the main issues of survival analysis is that we are only able
to observe our system for a finite amount of time $c$, and in this
period the event may or may not occur.</p>

<p>Let us assume that we performed a study with duration $c\,,$ if we do not observe the event within the end of the study we cannot conclude that the event did not happened,
we can only conclude that it did not happened within time $c$. We assume that the event must happen at some time.
We introduce the outcome variable $y$ as \(y = \min(t, c)\) and define the
<strong>censoring status</strong> variable $\delta$ which indicates if the event was observed or not</p>

\[\delta =
\begin{cases}
1\,\,\,  if \,\,\,  t &lt; c \\
0 \,\,\,  if \,\,\,  t \geq c
\end{cases}\]

<p>and if it is not observed we say that it is <strong>censored</strong>.
If the event is not censored then its contribution to the likelihood is, as usual, $f(t)\,,$ but if we do not observe
the event within time $c$ then all we can conclude is that
the event happened after time $c\,,$ and the probability for
this is $S(c)\,.$
Thus the likelihood can be written as</p>

\[L = f(y)^\delta S(y)^{1-\delta} = (h(y) S(y))^\delta S(y)^{1-\delta}= h(y)^\delta S(y)\]

<p>and this quantity is sometimes defined as the <strong>generalized likelihood</strong>.</p>

<h2 id="wrong-methods-for-accounting-of-censoring">Wrong methods for accounting of censoring</h2>

<p>If you are new to survival analysis and you don’t know how to correctly
include censoring in your model, you may end up with a biased estimate of the 
waiting time.</p>

<p>Let us see why a naive handling of the unobserved data may end up with a wrong
estimate of the parameters.</p>

<p>Let us generate 100 fake observations, distributed according
to</p>

\[Y \sim \mathcal{Exp}(1)\]

<p>Let us also assume that our study started at $t=0$ and ended at $t=c=1.5\,.$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">123321</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">rng</span><span class="p">.</span><span class="nf">exponential</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">w_cp</span> <span class="o">=</span> <span class="n">w</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

<span class="n">c</span> <span class="o">=</span> <span class="mf">1.5</span>

<span class="n">censoring</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span><span class="o">&gt;</span><span class="n">c</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

</code></pre></div></div>

<h3 id="naive-method-1-putting-a-threshold">Naive method 1: putting a threshold</h3>

<p>A first attempt could be to replace the unobserved event with the
censoring time.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">w_cp</span><span class="p">[</span><span class="n">w_cp</span><span class="o">&gt;</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">uncensored_model</span><span class="p">:</span>
    <span class="n">lam</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Exponential</span><span class="p">(</span><span class="sh">'</span><span class="s">lam</span><span class="sh">'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Exponential</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="n">lam</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">w_cp</span><span class="p">)</span>
    <span class="n">trace_uncensored</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_uncensored</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/survival_intro/trace_uncensored.webp" alt="The trace of the truncated model" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">summary</span><span class="p">(</span><span class="n">trace_uncensored</span><span class="p">)</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right">mean</th>
      <th style="text-align: right">sd</th>
      <th style="text-align: right">hdi_3%</th>
      <th style="text-align: right">hdi_97%</th>
      <th style="text-align: right">mcse_mean</th>
      <th style="text-align: right">mcse_sd</th>
      <th style="text-align: right">ess_bulk</th>
      <th style="text-align: right">ess_tail</th>
      <th style="text-align: right">r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">lam</td>
      <td style="text-align: right">1.328</td>
      <td style="text-align: right">0.133</td>
      <td style="text-align: right">1.083</td>
      <td style="text-align: right">1.575</td>
      <td style="text-align: right">0.001</td>
      <td style="text-align: right">0.001</td>
      <td style="text-align: right">9164</td>
      <td style="text-align: right">12834</td>
      <td style="text-align: right">1</td>
    </tr>
  </tbody>
</table>

<p>From the above summary we can observe that the $94\%$
HDI for this model does not contain the true value for the parameter.</p>

<h3 id="naive-method-2-dropping-the-unobserved-units">Naive method 2: dropping the unobserved units</h3>

<p>Another wrong method to deal with censoring is to 
only include in our dataset units which has an observation,
while excluding the remaining.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w_1</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="n">w</span><span class="o">&lt;</span><span class="n">c</span><span class="p">]</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">dropped_model</span><span class="p">:</span>
    <span class="n">lam</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Exponential</span><span class="p">(</span><span class="sh">'</span><span class="s">lam</span><span class="sh">'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Exponential</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="n">lam</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">w_1</span><span class="p">)</span>
    <span class="n">trace_dropped</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_dropped</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/survival_intro/trace_dropped.webp" alt="The trace of the dropped model" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>az.summary(trace_dropped)
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right">mean</th>
      <th style="text-align: right">sd</th>
      <th style="text-align: right">hdi_3%</th>
      <th style="text-align: right">hdi_97%</th>
      <th style="text-align: right">mcse_mean</th>
      <th style="text-align: right">mcse_sd</th>
      <th style="text-align: right">ess_bulk</th>
      <th style="text-align: right">ess_tail</th>
      <th style="text-align: right">r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">lam</td>
      <td style="text-align: right">1.73</td>
      <td style="text-align: right">0.19</td>
      <td style="text-align: right">1.368</td>
      <td style="text-align: right">2.082</td>
      <td style="text-align: right">0.002</td>
      <td style="text-align: right">0.001</td>
      <td style="text-align: right">8492</td>
      <td style="text-align: right">14744</td>
      <td style="text-align: right">1</td>
    </tr>
  </tbody>
</table>

<p>This estimate is even worst than the above one.</p>

<h2 id="correct-method">Correct method</h2>

<p>Let us now show that a correct inclusion of the censoring
into the model gives a better estimate of the average lifetime.
In PyMC, censoring can be simply implemented by using the <a href="https://www.pymc.io/projects/docs/en/latest/api/distributions/censored.html">Censored class</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">censored_model</span><span class="p">:</span>
    <span class="n">lam</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Exponential</span><span class="p">(</span><span class="sh">'</span><span class="s">lam</span><span class="sh">'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">.</span><span class="nf">dist</span><span class="p">(</span><span class="n">lam</span><span class="o">=</span><span class="n">lam</span><span class="p">)</span>
    <span class="n">y_censored</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Censored</span><span class="p">(</span><span class="sh">'</span><span class="s">y_censored</span><span class="sh">'</span><span class="p">,</span> <span class="n">dist</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">w_cp</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="n">trace_censored</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_censored</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/survival_intro/trace_censored.webp" alt="The trace of the censored model" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">summary</span><span class="p">(</span><span class="n">trace_censored</span><span class="p">)</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right">mean</th>
      <th style="text-align: right">sd</th>
      <th style="text-align: right">hdi_3%</th>
      <th style="text-align: right">hdi_97%</th>
      <th style="text-align: right">mcse_mean</th>
      <th style="text-align: right">mcse_sd</th>
      <th style="text-align: right">ess_bulk</th>
      <th style="text-align: right">ess_tail</th>
      <th style="text-align: right">r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">lam</td>
      <td style="text-align: right">1.08</td>
      <td style="text-align: right">0.12</td>
      <td style="text-align: right">0.863</td>
      <td style="text-align: right">1.314</td>
      <td style="text-align: right">0.001</td>
      <td style="text-align: right">0.001</td>
      <td style="text-align: right">9889</td>
      <td style="text-align: right">14108</td>
      <td style="text-align: right">1</td>
    </tr>
  </tbody>
</table>

<p>We now have that our estimate is correct within one standard deviation, and
this is a huge improvement with respect to both the naive methods.</p>

<h2 id="comparison-of-the-results">Comparison of the results</h2>

<p>In order to better understand the difference in the estimate, let us now sample
and plot the posterior predictive distributions of the three models.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">censored_model</span><span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Exponential</span><span class="p">(</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="n">lam</span><span class="p">)</span>
    <span class="n">ppc_censored</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_censored</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">])</span>

<span class="k">with</span> <span class="n">uncensored_model</span><span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Exponential</span><span class="p">(</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="n">lam</span><span class="p">)</span>
    <span class="n">ppc_uncensored</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_uncensored</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">])</span>

<span class="k">with</span> <span class="n">dropped_model</span><span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Exponential</span><span class="p">(</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="n">lam</span><span class="p">)</span>
    <span class="n">ppc_dropped</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_dropped</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">])</span>

<span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">xlim</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]</span>

<span class="n">xticks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">yticks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">ppc_uncensored</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span>  <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Uncensored</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_xticks</span><span class="p">(</span><span class="n">xticks</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_yticks</span><span class="p">(</span><span class="n">yticks</span><span class="p">)</span>

<span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">ppc_dropped</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span>  <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Dropped</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">set_xticks</span><span class="p">(</span><span class="n">xticks</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="nf">set_yticks</span><span class="p">(</span><span class="n">yticks</span><span class="p">)</span>

<span class="n">ax3</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">ppc_censored</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span>  <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Censored</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="nf">set_xticks</span><span class="p">(</span><span class="n">xticks</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="nf">set_yticks</span><span class="p">(</span><span class="n">yticks</span><span class="p">)</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/survival_intro/ppc_compare.webp" alt="The PPC distribution for the three models" /></p>

<p>In the above figures, the red histogram corresponds to the true (uncensored) data, while
the blue one corresponds to the posterior predictive distribution of our model.
The effect of the bias for method 1 and 2 is quite evident, while the censored
model predicts a distribution which is quite close to the true data.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We introduced survival analysis, and we introduced some main concept as
the hazard function and the survival function.
We also discussed censorship, and we showed with an example why it is important
to correctly account of censoring.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/survival_intro/" /><summary type="html"><![CDATA[Estimating waiting times]]></summary></entry><entry><title type="html">Hierarchical models and meta-analysis</title><link href="http://localhost:4000/statistics/hierarchical_metaanalysis" rel="alternate" type="text/html" title="Hierarchical models and meta-analysis" /><published>2024-01-30T00:00:00+01:00</published><updated>2024-01-30T00:00:00+01:00</updated><id>http://localhost:4000/statistics/hierarchical_metaanalysis</id><content type="html" xml:base="http://localhost:4000/statistics/hierarchical_metaanalysis"><![CDATA[<p>In the last post we discussed how to build a hierarchical model.
These models are often used in meta-analysis and reviews,
<em>i.e.</em> in academic publications where the results of many studies are collected,
criticized and combined together.
In this kind of study using a full pooling would not be appropriate,
as each study is performed at its own conditions,
so a hierarchical model is much more appropriate to combine the results together.</p>

<p>We will use this method to re-analyze an old meta-analysis by Daryl Bem,
a well known researcher who published the famous 
<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4706048/">“Bem meta-analysis”</a>
where he suggested that he found some evidence of precognition.
This paper became very famous both because of the conclusions
and because researchers failed to replicate its findings.</p>

<p>By analyzing the methodological flaws used in that paper,
other scientist started proposing many methods to improve
the robustness of the research results.</p>

<p>We won’t analyze that article, since it combined many kinds of experiments,
and this makes the overall analysis more involved. We will instead use
another paper by the same author which can be found
on <a href="https://www.cia.gov/readingroom/docs/CIA-RDP96-00789R003200110001-4.pdf">this page of the CIA website</a> (yes, CIA has been really interested into paranormal
activity).
We choose this article as it simply involves a binomial likelihood.
The article, in fact, summarizes the results of 11 experiments
which has equi-probable binary outcome, so by random guessing one
would expect, on average, a success percentage of the $50\%\,.$</p>

<p>In the analysis we will use the <strong>Region Of Practical Equivalence</strong> (ROPE)
to assess if the effect is practically equivalent with absent.
We will conclude that there is no evidence of precognition if the
$94\%$ Highest Density Region for the average success ratio
is entirely included in the region $[0, 0.75]\,.$
We will instead conclude that there is evidence of precognition
if the $94\%$ HDI is entirely inside the $[0.75, 1]$ region.
If the $94\%$ HDI crosses 0.75 we will conclude that the analysis is inconclusive.</p>

<p>The limit 0.75 seems quite strong, but since a positive finding
would contradict the current scientific knowledge, we require a strong evidence
in order to get a positive result.
Notice that the ROPE choice must be done <em>before</em> the dataset is seen,
otherwise we could tune the choice on the data.</p>

<p>As before, we will take</p>

\[\begin{align}
\alpha \sim &amp; \mathcal{HN}(10)
\\
\beta \sim &amp; \mathcal{HN}(10)
\\
\theta_i \sim &amp; \mathcal{Beta}(\alpha, \beta)
\\
y_i \sim &amp; \mathcal{Binom}(\theta_i, n_i)
\end{align}\]

<p>where $\mathcal{HN}(\sigma)$ denotes the Half Normal distribution.</p>

<p>Our dataset will consist into the 4th and 5th columns of table 1, which we provide
here</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right">n</th>
      <th style="text-align: left">y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">22</td>
      <td style="text-align: left">8</td>
    </tr>
    <tr>
      <td style="text-align: right">9</td>
      <td style="text-align: left">3</td>
    </tr>
    <tr>
      <td style="text-align: right">35</td>
      <td style="text-align: left">10</td>
    </tr>
    <tr>
      <td style="text-align: right">50</td>
      <td style="text-align: left">12</td>
    </tr>
    <tr>
      <td style="text-align: right">50</td>
      <td style="text-align: left">18</td>
    </tr>
    <tr>
      <td style="text-align: right">50</td>
      <td style="text-align: left">15</td>
    </tr>
    <tr>
      <td style="text-align: right">36</td>
      <td style="text-align: left">12</td>
    </tr>
    <tr>
      <td style="text-align: right">20</td>
      <td style="text-align: left">10</td>
    </tr>
    <tr>
      <td style="text-align: right">7</td>
      <td style="text-align: left">3</td>
    </tr>
    <tr>
      <td style="text-align: right">50</td>
      <td style="text-align: left">15</td>
    </tr>
    <tr>
      <td style="text-align: right">25</td>
      <td style="text-align: left">16</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="n">scipy.stats</span> <span class="k">as</span> <span class="n">st</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">rope_reg</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">]</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">"</span><span class="s">n</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">22</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">25</span><span class="p">],</span> 
                   <span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">]})</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">binom_meta</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">"</span><span class="s">alpha</span><span class="sh">"</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">"</span><span class="s">beta</span><span class="sh">"</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Beta</span><span class="p">(</span><span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">n</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">n</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span>
                  <span class="n">observed</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>

<span class="k">with</span> <span class="n">binom_meta</span><span class="p">:</span>
    <span class="n">trace_meta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_meta</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/hierarchical_meta/trace.webp" alt="The trace of the hierarchical model" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">summary</span><span class="p">(</span><span class="n">trace_meta</span><span class="p">)</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right">mean</th>
      <th style="text-align: right">sd</th>
      <th style="text-align: right">hdi_3%</th>
      <th style="text-align: right">hdi_97%</th>
      <th style="text-align: right">mcse_mean</th>
      <th style="text-align: right">mcse_sd</th>
      <th style="text-align: right">ess_bulk</th>
      <th style="text-align: right">ess_tail</th>
      <th style="text-align: right">r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">alpha</td>
      <td style="text-align: right">8.198</td>
      <td style="text-align: right">3.069</td>
      <td style="text-align: right">2.821</td>
      <td style="text-align: right">13.908</td>
      <td style="text-align: right">0.03</td>
      <td style="text-align: right">0.021</td>
      <td style="text-align: right">10166</td>
      <td style="text-align: right">11478</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: left">beta</td>
      <td style="text-align: right">14.197</td>
      <td style="text-align: right">5.294</td>
      <td style="text-align: right">5.034</td>
      <td style="text-align: right">24.155</td>
      <td style="text-align: right">0.05</td>
      <td style="text-align: right">0.035</td>
      <td style="text-align: right">10506</td>
      <td style="text-align: right">11784</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: left">theta[0]</td>
      <td style="text-align: right">0.365</td>
      <td style="text-align: right">0.076</td>
      <td style="text-align: right">0.22</td>
      <td style="text-align: right">0.502</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">28685</td>
      <td style="text-align: right">14641</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: left">theta[1]</td>
      <td style="text-align: right">0.356</td>
      <td style="text-align: right">0.091</td>
      <td style="text-align: right">0.19</td>
      <td style="text-align: right">0.531</td>
      <td style="text-align: right">0.001</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">26656</td>
      <td style="text-align: right">13005</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: left">theta[2]</td>
      <td style="text-align: right">0.316</td>
      <td style="text-align: right">0.063</td>
      <td style="text-align: right">0.202</td>
      <td style="text-align: right">0.438</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">30418</td>
      <td style="text-align: right">13072</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: left">theta[3]</td>
      <td style="text-align: right">0.278</td>
      <td style="text-align: right">0.054</td>
      <td style="text-align: right">0.179</td>
      <td style="text-align: right">0.379</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">27290</td>
      <td style="text-align: right">15091</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: left">theta[4]</td>
      <td style="text-align: right">0.361</td>
      <td style="text-align: right">0.057</td>
      <td style="text-align: right">0.252</td>
      <td style="text-align: right">0.467</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">32101</td>
      <td style="text-align: right">14623</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: left">theta[5]</td>
      <td style="text-align: right">0.32</td>
      <td style="text-align: right">0.057</td>
      <td style="text-align: right">0.214</td>
      <td style="text-align: right">0.426</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">29704</td>
      <td style="text-align: right">14628</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: left">theta[6]</td>
      <td style="text-align: right">0.346</td>
      <td style="text-align: right">0.064</td>
      <td style="text-align: right">0.229</td>
      <td style="text-align: right">0.468</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">28892</td>
      <td style="text-align: right">14533</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: left">theta[7]</td>
      <td style="text-align: right">0.431</td>
      <td style="text-align: right">0.08</td>
      <td style="text-align: right">0.283</td>
      <td style="text-align: right">0.584</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">29458</td>
      <td style="text-align: right">14330</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: left">theta[8]</td>
      <td style="text-align: right">0.383</td>
      <td style="text-align: right">0.097</td>
      <td style="text-align: right">0.2</td>
      <td style="text-align: right">0.561</td>
      <td style="text-align: right">0.001</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">30103</td>
      <td style="text-align: right">13251</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: left">theta[9]</td>
      <td style="text-align: right">0.32</td>
      <td style="text-align: right">0.056</td>
      <td style="text-align: right">0.214</td>
      <td style="text-align: right">0.426</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">32310</td>
      <td style="text-align: right">13805</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: left">theta[10]</td>
      <td style="text-align: right">0.516</td>
      <td style="text-align: right">0.08</td>
      <td style="text-align: right">0.368</td>
      <td style="text-align: right">0.664</td>
      <td style="text-align: right">0.001</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">19487</td>
      <td style="text-align: right">13738</td>
      <td style="text-align: right">1</td>
    </tr>
  </tbody>
</table>

<p>There is no evident issue in the sampling procedure.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_forest</span><span class="p">(</span><span class="n">trace_meta</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">],</span> <span class="n">rope</span><span class="o">=</span><span class="n">rope_reg</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/hierarchical_meta/forest.webp" alt="The forest plot of the hierarchical model" /></p>

<p>None of the studies suggests that there is any evidence of precognition.
We can also estimate the overall average as well as the effective sample
size.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">binom_meta</span><span class="p">:</span>
    <span class="n">logit_mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Deterministic</span><span class="p">(</span><span class="sh">"</span><span class="s">logit_mu</span><span class="sh">"</span><span class="p">,</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">alpha</span><span class="o">/</span><span class="n">beta</span><span class="p">))</span>
    <span class="n">log_neff</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Deterministic</span><span class="p">(</span><span class="sh">"</span><span class="s">log_neff</span><span class="sh">"</span><span class="p">,</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">alpha</span><span class="o">+</span><span class="n">beta</span><span class="p">))</span>

<span class="k">with</span> <span class="n">binom_meta</span><span class="p">:</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_meta</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">logit_mu</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">log_neff</span><span class="sh">'</span><span class="p">])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="nf">plot_pair</span><span class="p">(</span><span class="n">ppc</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">logit_mu</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">log_neff</span><span class="sh">"</span><span class="p">],</span> <span class="n">kind</span><span class="o">=</span><span class="sh">"</span><span class="s">kde</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/hierarchical_meta/kde.webp" alt="The kernel density estimate for mu and for the effective sample size of the hierarchical model" /></p>

<h2 id="conclusions">Conclusions</h2>

<p>We applied the beta binomial hierarchical model to a meta-analysis on
precognition. We also introduced the Region Of Practical Equivalence (ROPE).</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/hierarcical_metaanalysis/" /><summary type="html"><![CDATA[How hierarchical models can be used to analyze scientific literature]]></summary></entry><entry><title type="html">Hierarchical models</title><link href="http://localhost:4000/statistics/hierarchical_models" rel="alternate" type="text/html" title="Hierarchical models" /><published>2024-01-29T00:00:00+01:00</published><updated>2024-01-29T00:00:00+01:00</updated><id>http://localhost:4000/statistics/hierarchical_models</id><content type="html" xml:base="http://localhost:4000/statistics/hierarchical_models"><![CDATA[<p>There are many circumstances where your data are somehow connected,
but cannot be treated as iid.
As an example, when you compare clinical studies on the effectiveness
of a medicine, data originated from
different studies will likely involve different inclusion policies as
well as different hospital setup.
However, it is reasonable to assume that parameters describing
different studies are related.</p>

<div class="emphbox">
In Bayesian statistics, a common approach
in these situations is to assume that the parameters are sampled from
a common prior distribution.
</div>

<p>Mathematically speaking, you assume that</p>

\[Y_i \sim P(\theta_i)\]

<p>so each observation will be described by its own parameter. However,
the parameters are considered as iid</p>

\[\theta_i \sim P'(\phi)\]

<p>This kind of model strictly belongs to the Bayesian framework, as in the frequentist
one the parameters $\theta_i$ are numbers, so you can either treat them
as different (unpooled) or they are sampled from the same distribution (pooled).</p>

<p>Let us take a look at this kind of model with an example.</p>

<h2 id="spacex-analysis">SpaceX analysis</h2>

<p>In the following we will consider the launches from the four
main launch vehicles: Falcon 1, Falcon 9, Falcon Heavy and Starship.
For the sake of simplicity, we will treat as identical
rockets of different variants within the same family.
Below we provide the relevant statistics for the different launchers.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: left">Mission</th>
      <th style="text-align: right">Number</th>
      <th style="text-align: right">successes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: left">Falcon 1</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: left">Falcon 9</td>
      <td style="text-align: right">304</td>
      <td style="text-align: right">301</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: left">Falcon Heavy</td>
      <td style="text-align: right">9</td>
      <td style="text-align: right">9</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: left">Starship</td>
      <td style="text-align: right">9</td>
      <td style="text-align: right">5</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="n">scipy.stats</span> <span class="k">as</span> <span class="n">st</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">df_spacex</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">'</span><span class="s">Mission</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">Falcon 1</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Falcon 9</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Falcon Heavy</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Starship</span><span class="sh">'</span><span class="p">],</span> <span class="sh">'</span><span class="s">N</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">304</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">301</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">5</span><span class="p">]})</span>

<span class="n">df_spacex</span><span class="p">[</span><span class="sh">'</span><span class="s">mu</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_spacex</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">]</span><span class="o">/</span><span class="n">df_spacex</span><span class="p">[</span><span class="sh">'</span><span class="s">N</span><span class="sh">'</span><span class="p">]</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="no-pooling">No pooling</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">spacex_model_no_pooling</span><span class="p">:</span>  
  <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Beta</span><span class="p">(</span><span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">df_spacex</span><span class="p">[</span><span class="sh">'</span><span class="s">N</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">))</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">df_spacex</span><span class="p">[</span><span class="sh">'</span><span class="s">N</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span>
                  <span class="n">observed</span><span class="o">=</span><span class="n">df_spacex</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>

<span class="n">pm</span><span class="p">.</span><span class="nf">model_to_graphviz</span><span class="p">(</span><span class="n">spacex_model_no_pooling</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/hierarchical/model_unpooled.webp" alt="The structure of the unpooled model" /></p>

<p>In the above diagram we see that each of the four vehicle has its own
parameter.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">spacex_model_no_pooling</span><span class="p">:</span>
  <span class="n">trace_spacex_no_pooling</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_spacex_no_pooling</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/hierarchical/trace_unpooled.webp" alt="The trace of the unpooled model" /></p>

<p>In the above block, we increased the “target_accept” parameter in order to avoid
numerical issues.
The trace looks fine, but let us now take a better look at the estimated parameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_forest</span><span class="p">(</span><span class="n">trace_spacex_no_pooling</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/hierarchical/forest_unpooled.webp" alt="The forest plot of the unpooled model" /></p>

<p>The parameters make sense, and the Falcon 1 as well as the Starship are very
unconstrained, due to the small number of launches.
Let us now take a look at the pooled model.</p>

<h3 id="full-pooling">Full pooling</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">spacex_model_full_pooling</span><span class="p">:</span>  
  <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Beta</span><span class="p">(</span><span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">df_spacex</span><span class="p">[</span><span class="sh">'</span><span class="s">N</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span>
                  <span class="n">observed</span><span class="o">=</span><span class="n">df_spacex</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>

<span class="n">pm</span><span class="p">.</span><span class="nf">model_to_graphviz</span><span class="p">(</span><span class="n">spacex_model_full_pooling</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/hierarchical/model_pooled.webp" alt="The structure of the pooled model" /></p>

<p>In this model, all the launches are treated as sampled from a common
iid, and we therefore have only one parameter for all the launch vehicle.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">spacex_model_full_pooling</span><span class="p">:</span>
  <span class="n">trace_spacex_full_pooling</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_spacex_full_pooling</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/hierarchical/trace_pooled.webp" alt="The trace of the pooled model" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_forest</span><span class="p">(</span><span class="n">trace_spacex_full_pooling</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/hierarchical/forest_pooled.webp" alt="The forest plot of the pooled model" /></p>

<p>In this case the single parameter is almost entirely above 0.94,
so any launch should succeed with a probability higher than the 94%.
Would you to bet that a Falcon 1 would succeed? I honestly wouldn’t,
but this is what you should do according to this model.</p>

<h3 id="hierarchical-model">Hierarchical model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">spacex_model_hierarchical</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">"</span><span class="s">alpha</span><span class="sh">"</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">"</span><span class="s">beta</span><span class="sh">"</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Deterministic</span><span class="p">(</span><span class="sh">"</span><span class="s">mu</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">/</span><span class="p">(</span><span class="n">alpha</span><span class="o">+</span><span class="n">beta</span><span class="p">))</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Beta</span><span class="p">(</span><span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">df_spacex</span><span class="p">[</span><span class="sh">'</span><span class="s">N</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">df_spacex</span><span class="p">[</span><span class="sh">'</span><span class="s">N</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span>
                  <span class="n">observed</span><span class="o">=</span><span class="n">df_spacex</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>

<span class="n">pm</span><span class="p">.</span><span class="nf">model_to_graphviz</span><span class="p">(</span><span class="n">spacex_model_hierarchical</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/hierarchical/model_hierarchical.webp" alt="The structure of the hierarchical model" /></p>

<p>In this case, each vehicle has its own parameter. The parameters are
however sampled according to a Beta distribution, with priors
$\alpha$ and $\beta\,.$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">spacex_model_hierarchical</span><span class="p">:</span>
    <span class="n">trace_spacex_hierarchical</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_spacex_hierarchical</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/hierarchical/trace_hierarchical.webp" alt="The trace of the hierarchical model" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_forest</span><span class="p">(</span><span class="n">trace_spacex_hierarchical</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/hierarchical/forest_hierarchical.webp" alt="The forest plot of the hierarchical model" /></p>

<p>These estimates are similar to the unpooled model, they are however
closer one to the other ones.
This happens because the hierarchical model allow us to share information
across the variables.</p>

<p>One of the most relevant features of hierarchical models, is that they allow us to make predictions for unobserved variables
with unknown parameters.
Let us assume that SpaceX produces a new launcher, this model allows us
to estimate its success probability.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">spacex_model_hierarchical</span><span class="p">:</span>
    <span class="n">theta_new</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Beta</span><span class="p">(</span><span class="sh">'</span><span class="s">th_new</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
    <span class="n">ppc_new</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_spacex_hierarchical</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">th_new</span><span class="sh">'</span><span class="p">])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">ppc_new</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">th_new</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">),</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/hierarchical/hierarchical_new_theta.webp" alt="The probability distribution for a new theta" /></p>

<p>We can moreover estimate the average success rate for any SpaceX
vehicle.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">map_mu</span> <span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="nf">mode</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">digitize</span><span class="p">(</span><span class="n">trace_spacex_hierarchical</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">"</span><span class="s">mu</span><span class="sh">"</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">))).</span><span class="n">mode</span><span class="o">/</span><span class="mi">100</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="nf">plot_posterior</span><span class="p">(</span><span class="n">trace_spacex_hierarchical</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">mu</span><span class="sh">"</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="s">MAP: </span><span class="si">{</span><span class="n">map_mu</span><span class="si">}</span><span class="sh">'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/hierarchical/hierarchical_mu.webp" alt="The probability distribution of a success" /></p>

<p>This estimate is much more generous than the pooled one,
as it properly takes into account the failure rates of less successful
models.</p>

<p>We also reported the Maximum A Posteriori (MAP) estimate for the above parameter,
as it is a better point estimate than the mean for non-symmetric distributions
as the one above.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We discussed how hierarchical models allow us to share information
across the variables and to make predictions for new, unobserved, variables.
In the next post we will discuss a very important application
of hierarchical models to meta-analysis.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/hierarchical_models/" /><summary type="html"><![CDATA[How to implement hierarchies]]></summary></entry><entry><title type="html">Poisson regression</title><link href="http://localhost:4000/statistics/poisson_regression" rel="alternate" type="text/html" title="Poisson regression" /><published>2024-01-28T00:00:00+01:00</published><updated>2024-01-28T00:00:00+01:00</updated><id>http://localhost:4000/statistics/poisson_regression</id><content type="html" xml:base="http://localhost:4000/statistics/poisson_regression"><![CDATA[<p>In the last post we introduced the Generalized Linear Models,
and we explained how to perform regression on data types which are
not appropriate for a Gaussian likelihood.
We also saw a concrete example of logistic regression, and here we will
discuss another type of GLM, the Poisson regression.</p>

<h2 id="poisson-regression">Poisson regression</h2>

<p>In the Poisson regression one assumes that</p>

\[Y_i \sim \mathcal{Poisson}(\theta_i)\]

<p>where $\theta_i$ must be a non-negative variable. One can 
use the exponential function to map any real number on the
positive axis, we therefore assume that</p>

\[\theta_i = \exp\left(\alpha + \beta X_i\right)\]

<p>We will use this model to estimate the average number of
bear attacks in North America.
The original data can be found on this <a href="https://data.world/ajsanne/north-america-bear-killings/workspace/file?filename=north_america_bear_killings.csv">data.world
</a>
page, where there are listed all human killing by a black, brown, or polar bear from 1900-2018 in North America.
We will limit ourself to black and brown bears, as attacks by polar bears are very rare.
We will also limit our dataset to the years after 1999, as we want to assume that the attack probability
is constant within the entire time range, and we will neglect attacks by captive animals.
We want to assess the attack probability by bear type, and to do this we will use the bear type
as a regressor.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">./data/north_america_bear_killings.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">df_red</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">groupby</span><span class="p">([</span><span class="sh">'</span><span class="s">Year</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Type</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Type of bear</span><span class="sh">'</span><span class="p">]).</span><span class="nf">count</span><span class="p">().</span><span class="nf">reset_index</span><span class="p">()[[</span><span class="sh">'</span><span class="s">Year</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Type</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Type of bear</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Hikers</span><span class="sh">'</span><span class="p">]]</span>

<span class="n">df_clean</span> <span class="o">=</span> <span class="n">df_red</span><span class="p">[(</span><span class="n">df_red</span><span class="p">[</span><span class="sh">'</span><span class="s">Year</span><span class="sh">'</span><span class="p">]</span><span class="o">&gt;=</span><span class="mi">2000</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df_red</span><span class="p">[</span><span class="sh">'</span><span class="s">Type of bear</span><span class="sh">'</span><span class="p">]</span> <span class="o">!=</span> <span class="sh">'</span><span class="s">Polar Bear</span><span class="sh">'</span><span class="p">)</span><span class="o">&amp;</span> <span class="p">(</span><span class="n">df_red</span><span class="p">[</span><span class="sh">'</span><span class="s">Type</span><span class="sh">'</span><span class="p">]</span> <span class="o">!=</span> <span class="sh">'</span><span class="s">Captive</span><span class="sh">'</span><span class="p">)]</span>

<span class="n">df_fit</span> <span class="o">=</span> <span class="n">df_clean</span><span class="p">.</span><span class="nf">set_index</span><span class="p">([</span><span class="sh">'</span><span class="s">Year</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Type of bear</span><span class="sh">'</span><span class="p">]).</span><span class="nf">unstack</span><span class="p">(</span><span class="n">fill_value</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">stack</span><span class="p">().</span><span class="nf">reset_index</span><span class="p">()[[</span><span class="sh">'</span><span class="s">Year</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Type of bear</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Hikers</span><span class="sh">'</span><span class="p">]]</span>

<span class="n">df_fit</span><span class="p">.</span><span class="nf">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">Hikers</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Count</span><span class="sh">'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df_fit</span><span class="p">[</span><span class="sh">'</span><span class="s">is_black</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_fit</span><span class="p">[</span><span class="sh">'</span><span class="s">Type of bear</span><span class="sh">'</span><span class="p">].</span><span class="nb">str</span><span class="p">.</span><span class="nf">contains</span><span class="p">(</span><span class="sh">'</span><span class="s">Black</span><span class="sh">'</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">poisson_regr</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">df_fit</span><span class="p">[</span><span class="sh">'</span><span class="s">is_black</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Poisson</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">z</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_fit</span><span class="p">[</span><span class="sh">'</span><span class="s">Count</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">y_brown</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Poisson</span><span class="p">(</span><span class="sh">'</span><span class="s">y_brown</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">y_black</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Poisson</span><span class="p">(</span><span class="sh">'</span><span class="s">y_black</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">alpha</span><span class="o">+</span><span class="n">beta</span><span class="p">))</span>

<span class="k">with</span> <span class="n">poisson_regr</span><span class="p">:</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/poisson_glm/trace.webp" alt="The trace of the Poisson model" /></p>

<p>The trace seems fine, we can now verify if the model is compatible with the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">poisson_regr</span><span class="p">:</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y_brown</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y_black</span><span class="sh">'</span><span class="p">],</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="c1"># Let us estimate the probability that, in one year, there are k brown/black bear attacks 
</span>
<span class="n">n_brown</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nf">count_nonzero</span><span class="p">(</span><span class="n">ppc</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_brown</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>
<span class="n">n_black</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nf">count_nonzero</span><span class="p">(</span><span class="n">ppc</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_black</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">df_fit</span><span class="p">[</span><span class="n">df_fit</span><span class="p">[</span><span class="sh">'</span><span class="s">is_black</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">Count</span><span class="sh">'</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">crimson</span><span class="sh">'</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">+</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">n_brown</span><span class="o">/</span><span class="n">n_brown</span><span class="p">.</span><span class="nf">sum</span><span class="p">(),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">df_fit</span><span class="p">[</span><span class="n">df_fit</span><span class="p">[</span><span class="sh">'</span><span class="s">is_black</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">][</span><span class="sh">'</span><span class="s">Count</span><span class="sh">'</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">crimson</span><span class="sh">'</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">+</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">n_black</span><span class="o">/</span><span class="n">n_black</span><span class="p">.</span><span class="nf">sum</span><span class="p">(),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/poisson_glm/posterior_predictive.webp" alt="The posterior predictive of the Poisson model" /></p>

<p>The data seems compatible with the average estimate of our model.
We can now verify if the average number of attacks by black bears is statistically
compatible with the average number of attacks by brown bears.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_forest</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/poisson_glm/posterior.webp" alt="The forest plot of our parameters" /></p>

<p>As we can see, $\beta$ is compatible with 0, so we can consider the average attack number
by black bears is compatible with the average attack number by brown bears.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We discussed a second kind of GLM, namely the Poisson regression,
and we applied this model to estimate the average number of lethal
attacks by wild bears in North America.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/logistic_regression/" /><summary type="html"><![CDATA[Regression on count data]]></summary></entry><entry><title type="html">Logistic regression</title><link href="http://localhost:4000/statistics/logistic_regression" rel="alternate" type="text/html" title="Logistic regression" /><published>2024-01-27T00:00:00+01:00</published><updated>2024-01-27T00:00:00+01:00</updated><id>http://localhost:4000/statistics/logistic_regression</id><content type="html" xml:base="http://localhost:4000/statistics/logistic_regression"><![CDATA[<p>In the last posts we discussed how to build
the simplest regression model for a real variable
with the linear model.
This model can be used as a starting block
to perform regression on many other types of data,
and this can be done by building
a <strong>Generalized Linear Model</strong> (GLM).</p>

<p>GLMs can be constructed by starting
from any likelihood for the
data \(P(y | \theta)\,.\)</p>

<p>The parameter $\theta$ usually is bounded to
some specific range \(D\): we have
\(\theta \in [0, 1]\) for the Binomial likelihood,
while we have $\theta &gt; 0$ for the Poisson model.
On the other hand, the variable</p>

\[Z \sim \alpha + \beta X\]

<p>can generally take any real value.
However, by choosing a suitable function</p>

\[f : \mathbb{R} \rightarrow D\]

<p>we can map our random variable \(Z\) to
the desired domain \(D\,.\)</p>

<p>The general GLM can therefore reads</p>

\[\begin{align}
Y &amp; \sim P(\theta) \\
\theta &amp; = f\left(\alpha + \beta X\right)
\end{align}\]

<p>Of course $\alpha$ and $\beta$ and any other parameter
$\phi$ will be described by a suitable prior distribution.</p>

<p>Let us now see how to do this in practice.</p>

<h2 id="the-logistic-model">The logistic model</h2>

<p>The logistic model can be applied when there is a single binary dependent variable
which depends on one or more independent variables, which can be binary, integer or continuous.
In the logistic model the likelihood is taken as the binomial one,
while the mapping function $f$ is taken as the logistic function, plotted below:</p>

\[f(x) = \frac{1}{1+e^{-x}}\]

<p><img src="/docs/assets/images/statistics/logistic/logistic.webp" alt="The logistic function" /></p>

<p>We will apply the logistic regression to the Challenger O-ring dataset. 
On January 28th 1986 the shuttle broke during the launch, killing several people,
and the USA president formed a commission to investigate on the causes of the incident.
One of the member of the commission was the physicist Richard Feynman,
who proved that the incident was caused by a loss of flexibility of the shuttle
O-rings caused by the low temperature 
(see the <a href="https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disaster">Wikipedia page</a>)
Here we will take the data on the number of O-rings damaged in each mission of the Challenger
and we will provide an estimate on the probability that one o-ring becomes damaged as a function of the temperature.
The original data can be found <a href="https://archive.ics.uci.edu/dataset/92/challenger+usa+space+shuttle+o+ring">here</a>,
and we provide here the dataset grouped by temperature for completeness (the temperature is expressed in °F).</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">temperature</th>
      <th style="text-align: right">damaged</th>
      <th style="text-align: right">undamaged</th>
      <th style="text-align: right">count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">53</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: right">57</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">58</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">63</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">66</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">5</td>
      <td style="text-align: right">67</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">18</td>
      <td style="text-align: right">18</td>
    </tr>
    <tr>
      <td style="text-align: right">6</td>
      <td style="text-align: right">68</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">7</td>
      <td style="text-align: right">69</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">8</td>
      <td style="text-align: right">70</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">22</td>
      <td style="text-align: right">24</td>
    </tr>
    <tr>
      <td style="text-align: right">9</td>
      <td style="text-align: right">72</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">10</td>
      <td style="text-align: right">73</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">11</td>
      <td style="text-align: right">75</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">11</td>
      <td style="text-align: right">12</td>
    </tr>
    <tr>
      <td style="text-align: right">12</td>
      <td style="text-align: right">76</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">12</td>
      <td style="text-align: right">12</td>
    </tr>
    <tr>
      <td style="text-align: right">13</td>
      <td style="text-align: right">78</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">14</td>
      <td style="text-align: right">79</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">15</td>
      <td style="text-align: right">81</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
  </tbody>
</table>

<p>The dataset contains all the information collected before the Challenger disaster.
The logistic model is already implemented into PyMC, but to see how it works we will implement it from scratch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">pymc.sampling_jax</span> <span class="k">as</span> <span class="n">pmjax</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>


<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">df_oring</span><span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">./data/orings.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Convert it to Celsius
</span>
<span class="n">df_oring</span><span class="p">[</span><span class="sh">'</span><span class="s">deg</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_oring</span><span class="p">[</span><span class="sh">'</span><span class="s">temperature</span><span class="sh">'</span><span class="p">]</span><span class="o">-</span><span class="mi">32</span><span class="p">)</span><span class="o">*</span><span class="mi">5</span><span class="o">/</span><span class="mi">9</span>
</code></pre></div></div>

<p>I converted the temperature to Celsius degree because it is easier for me to 
reason in terms of Celsius degree.
Let us write down our model</p>

\[\begin{align}
Y_i \sim &amp; \mathcal{Binom}(p_i, n_i)\\
p_i = &amp; 
\frac{1}{1+e^{-\alpha - \beta X_i}} 
\\
\end{align}\]

<p>The <strong>odds ratio</strong> is defined as</p>

\[\begin{align}
\frac{p}{1-p} 
&amp;
=
\frac{1}{1+e^{-\alpha - \beta X}}\frac{1}{1-\frac{1}{1+e^{-\alpha - \beta X}}}
\\
&amp;
=
\frac{1}{1+e^{-\alpha - \beta X}}\frac{ 1+e^{-\alpha - \beta X} }{e^{-\alpha - \beta X}}
\\
&amp;
= e^{\alpha + \beta X}
\end{align}\]

<p>therefore</p>

\[\log\left(\frac{p}{1-p}\right) = \alpha + \beta X\]

<p>We can therefore identify $\alpha$ with the log odds at $T=0°C$
It doesn’t really makes sense to assume either a too big number or a too small one,
so we will take</p>

\[\alpha \sim \mathcal{N}(0, 15)\]

<p>On the other hand, $\beta$ represents the variation of the log odds with an increase of $1°C\,.$
We do expect a meaningful variation on a scale of $10°C\,,$ 
so we can generously take</p>

\[\beta \sim \mathcal{N}(0, 2)\]

<p>We are now ready to implement our model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">log_theta</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">df_oring</span><span class="p">[</span><span class="sh">'</span><span class="s">deg</span><span class="sh">'</span><span class="p">]</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">log_theta</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">df_oring</span><span class="p">[</span><span class="sh">'</span><span class="s">count</span><span class="sh">'</span><span class="p">],</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_oring</span><span class="p">[</span><span class="sh">'</span><span class="s">undamaged</span><span class="sh">'</span><span class="p">])</span>

<span class="k">with</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">trace_logistic</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_logistic</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/logistic/trace.webp" alt="The trace of the logistic model" /></p>

<p>The trace looks fine, we can now take a look at the posterior predictive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_pl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

<span class="k">with</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Deterministic</span><span class="p">(</span><span class="sh">'</span><span class="s">mu</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">x_pl</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Deterministic</span><span class="p">(</span><span class="sh">'</span><span class="s">p</span><span class="sh">'</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">mu</span><span class="p">)))</span>

<span class="k">with</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_logistic</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mu</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">p</span><span class="sh">'</span><span class="p">])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span>
<span class="n">posterior_predictive</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">.</span><span class="n">p</span><span class="p">.</span><span class="nf">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]),</span>
                <span class="mi">1</span><span class="o">-</span>
<span class="n">posterior_predictive</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">.</span><span class="n">p</span><span class="p">.</span><span class="nf">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]),</span>
        <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">lightgray</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span>
<span class="n">posterior_predictive</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">.</span><span class="n">p</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]),</span>
        <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">df_oring</span><span class="p">[</span><span class="sh">'</span><span class="s">deg</span><span class="sh">'</span><span class="p">],</span> <span class="n">df_oring</span><span class="p">[</span><span class="sh">'</span><span class="s">damaged</span><span class="sh">'</span><span class="p">]</span><span class="o">/</span><span class="n">df_oring</span><span class="p">[</span><span class="sh">'</span><span class="s">count</span><span class="sh">'</span><span class="p">],</span>
           <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">raw data estimate</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">1.01</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">T $\degree$C</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">Fraction of damaged O-rings</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/logistic/posterior_predictive.webp" alt="The trace of the logistic model" /></p>

<p>As we can see, the more we approach $0°\,,$ the more it is likely that an O-ring gets damaged.
The forecasted temperature for the launch day was $26-29 °F\,,$ corresponding to a range between
$-1.6$ °C and $-3.3$ °C.</p>

<p>We must however consider that one broken O-ring is not enough to create serious issues.
We can therefore estimate the probability as a function of the number of undamaged rings.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tm</span> <span class="o">=</span> <span class="p">(</span><span class="mi">26</span><span class="o">-</span><span class="mi">32</span><span class="p">)</span><span class="o">*</span><span class="mi">5</span><span class="o">/</span><span class="mi">9</span>
<span class="n">tM</span> <span class="o">=</span> <span class="p">(</span><span class="mi">29</span><span class="o">-</span><span class="mi">32</span><span class="p">)</span><span class="o">*</span><span class="mi">5</span><span class="o">/</span><span class="mi">9</span>

<span class="k">with</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">theta_m</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">tm</span><span class="o">*</span><span class="n">beta</span><span class="p">)))</span>
    <span class="n">ym</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">(</span><span class="sh">'</span><span class="s">ym</span><span class="sh">'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta_m</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
    <span class="n">theta_M</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">tm</span><span class="o">*</span><span class="n">beta</span><span class="p">)))</span>
    <span class="n">yM</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">(</span><span class="sh">'</span><span class="s">yM</span><span class="sh">'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta_M</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

<span class="k">with</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">ppc_t</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_logistic</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">ym</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">yM</span><span class="sh">'</span><span class="p">])</span>

<span class="c1"># We count how many O-rings are undamaged for each draw
</span>
<span class="n">hm</span> <span class="o">=</span> <span class="p">[(</span><span class="n">ppc_t</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">ym</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="n">k</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">7</span><span class="p">)]</span>
<span class="n">hM</span> <span class="o">=</span> <span class="p">[(</span><span class="n">ppc_t</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">yM</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="n">k</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">7</span><span class="p">)]</span>
<span class="n">h_0</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">7</span><span class="p">)]</span>

<span class="c1"># And we now estimate the corresponding probability
</span>
<span class="n">df_h</span><span class="p">[</span><span class="sh">'</span><span class="s">prob_m</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_h</span><span class="p">[</span><span class="sh">'</span><span class="s">count_m</span><span class="sh">'</span><span class="p">]</span><span class="o">/</span><span class="n">df_h</span><span class="p">[</span><span class="sh">'</span><span class="s">count_m</span><span class="sh">'</span><span class="p">].</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">df_h</span><span class="p">[</span><span class="sh">'</span><span class="s">prob_M</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_h</span><span class="p">[</span><span class="sh">'</span><span class="s">count_M</span><span class="sh">'</span><span class="p">]</span><span class="o">/</span><span class="n">df_h</span><span class="p">[</span><span class="sh">'</span><span class="s">count_M</span><span class="sh">'</span><span class="p">].</span><span class="nf">sum</span><span class="p">()</span>

<span class="n">df_h</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">'</span><span class="s">n</span><span class="sh">'</span><span class="p">:</span> <span class="n">h_0</span><span class="p">,</span> <span class="sh">'</span><span class="s">count_m</span><span class="sh">'</span><span class="p">:</span> <span class="n">hm</span><span class="p">,</span> <span class="sh">'</span><span class="s">count_M</span><span class="sh">'</span><span class="p">:</span> <span class="n">hM</span><span class="p">})</span>

<span class="c1"># Let us take a look at the best-case scenario
</span><span class="n">sns</span><span class="p">.</span><span class="nf">barplot</span><span class="p">(</span><span class="n">df_h</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="sh">'</span><span class="s">n</span><span class="sh">'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">'</span><span class="s">prob_M</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/logistic/best_case.webp" alt="The probaility as a function of the undamaged rings" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_h</span><span class="p">[</span><span class="n">df_h</span><span class="p">[</span><span class="sh">'</span><span class="s">n</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">prob_M</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div>

<div class="code">
0.949
</div>
<p>The most probable scenario is that all O-rings get damaged, and this 
is scenario has, according to our model, the $95\%$ or probability to happen.</p>

<p>We can conclude that, with the available information,
it was not safe to perform the launch.
This is however a <em>post-hoc</em> analysis (an analysis performed on some
data once the outcome is known), and one should be really careful
to draw conclusions based on this kind of analysis, as this easily results
into false positive errors (see <a href="https://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data">the Wikipedia page on this topic</a>).</p>

<h2 id="conclusions">Conclusions</h2>

<p>We introduced the Generalized Linear Model and we analyzed the Challenger dataset
by means of a logistic regression.
In the next post we will discuss about Poisson regression.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/logistic_regression/" /><summary type="html"><![CDATA[How to perform regression on binary data]]></summary></entry><entry><title type="html">Robust linear regression</title><link href="http://localhost:4000/statistics/robust_regression" rel="alternate" type="text/html" title="Robust linear regression" /><published>2024-01-26T00:00:00+01:00</published><updated>2024-01-26T00:00:00+01:00</updated><id>http://localhost:4000/statistics/robust_regression</id><content type="html" xml:base="http://localhost:4000/statistics/robust_regression"><![CDATA[<p>In some case your data may be not good enough to provide you reliable estimates with normal linear regression,
and this is the case of the conclusions drawn from
<a href="https://www.cambridge.org/core/journals/american-political-science-review/article/abs/political-institutions-and-voter-turnout-in-the-industrial-democracies/D6725BBF93F2F90F03A69B0794728BF7">this</a>
article, where the author concludes that there is a significant correlation between
the voter turnout in a country and its average income inequality.
This example is a classical example of misleading result of a regression,
where the author does not provide a plot of the data, taken from
<a href="https://www.google.it/books/edition/Data_Visualization/3XOYDwAAQBAJ?hl=it&amp;gbpv=1&amp;dq=Data+visualization,+a+practical+introduction&amp;printsec=frontcover">Healy, “Data visualization, a practical introduction”</a>.
The data below is extracted the data from the figure of Healy’s book.
South Africa corresponds to the last point.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">turnout</th>
      <th style="text-align: right">inequality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0.85822</td>
      <td style="text-align: right">1.95745</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: right">0.837104</td>
      <td style="text-align: right">1.95745</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">0.822021</td>
      <td style="text-align: right">2.41135</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">0.87632</td>
      <td style="text-align: right">2.76596</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">0.901961</td>
      <td style="text-align: right">2.95035</td>
    </tr>
    <tr>
      <td style="text-align: right">5</td>
      <td style="text-align: right">0.776772</td>
      <td style="text-align: right">3.21986</td>
    </tr>
    <tr>
      <td style="text-align: right">6</td>
      <td style="text-align: right">0.72549</td>
      <td style="text-align: right">3.14894</td>
    </tr>
    <tr>
      <td style="text-align: right">7</td>
      <td style="text-align: right">0.72549</td>
      <td style="text-align: right">2.92199</td>
    </tr>
    <tr>
      <td style="text-align: right">8</td>
      <td style="text-align: right">0.61991</td>
      <td style="text-align: right">2.93617</td>
    </tr>
    <tr>
      <td style="text-align: right">9</td>
      <td style="text-align: right">0.574661</td>
      <td style="text-align: right">2.31206</td>
    </tr>
    <tr>
      <td style="text-align: right">10</td>
      <td style="text-align: right">0.880845</td>
      <td style="text-align: right">3.60284</td>
    </tr>
    <tr>
      <td style="text-align: right">11</td>
      <td style="text-align: right">0.803922</td>
      <td style="text-align: right">3.5461</td>
    </tr>
    <tr>
      <td style="text-align: right">12</td>
      <td style="text-align: right">0.778281</td>
      <td style="text-align: right">3.47518</td>
    </tr>
    <tr>
      <td style="text-align: right">13</td>
      <td style="text-align: right">0.739065</td>
      <td style="text-align: right">3.68794</td>
    </tr>
    <tr>
      <td style="text-align: right">14</td>
      <td style="text-align: right">0.819005</td>
      <td style="text-align: right">4.41135</td>
    </tr>
    <tr>
      <td style="text-align: right">15</td>
      <td style="text-align: right">0.645551</td>
      <td style="text-align: right">3.91489</td>
    </tr>
    <tr>
      <td style="text-align: right">16</td>
      <td style="text-align: right">0.669683</td>
      <td style="text-align: right">5.64539</td>
    </tr>
    <tr>
      <td style="text-align: right">17</td>
      <td style="text-align: right">0.14178</td>
      <td style="text-align: right">9.30496</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">pymc.sampling_jax</span> <span class="k">as</span> <span class="n">pmjax</span>

<span class="n">df_turnout</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">data/inequality.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">pairplot</span><span class="p">(</span><span class="n">df_turnout</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/inequality_pairplot.webp" alt="The dataset pairplot" /></p>

<p>By simply plotting the data we can clearly see that there is one point,
the South Africa, which is far away from the other,
and this may have a huge impact on the fit.
Let us see this, and how one may avoid this kind of error.</p>

<h2 id="the-normal-linear-regression">The normal linear regression</h2>

<p>Let us start by assuming that the inequality is distributed
according to a normal linear model,
analogous to the one already discussed in the <a href="/linear_regression">regression post</a>.</p>

\[Y \sim \mathcal{N}(\mu, \sigma)\]

<p>where</p>

\[\mu = \alpha + \beta X\]

<p>We will assume that the precision $\tau = 1/\sigma$ is distributed according to a Half Normal
distribution. Since the inequality goes from 0 to 10, assuming a
standard deviation of $5$ for $\tau$ should be sufficient.
On the other hand, we will make the quite generous assumption that</p>

\[\alpha \sim \mathcal{N}(0, 20)\]

\[\beta \sim \mathcal{N}(0, 20)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_norm</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">'</span><span class="s">tau</span><span class="sh">'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">turnout</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">inequality</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_norm</span><span class="p">:</span>
    <span class="n">trace_norm</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> 
                           <span class="n">idata_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">log_likelihood</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_norm</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/trace_norm.webp" alt="The trace of the normal model" /></p>

<p>The traces doesn’t show any relevant issue, and for our purposes it is
sufficient this check.
Let us check our fit</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_plt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_norm</span><span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">x_plt</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>
    
<span class="k">with</span> <span class="n">model_norm</span><span class="p">:</span>
    <span class="n">ppc_norm</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_norm</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">],</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">ppc_norm</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">ppc_norm</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]),</span>
                <span class="n">ppc_norm</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">grey</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">turnout</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">inequality</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/ppc_norm.webp" alt="The posterior preditcive distribution of our model" /></p>

<p>The error bands correctly reproduce almost all the data. However,
since the South Africa is far away from the other countries,
it may happen that its behavior strongly influences the fit.</p>

<p>Let us now use a more robust model.
In order to make it more robust, which in this context means
less sensitive to isolated data, let us take a t-Student likelihood
instead of a normal one.</p>

<p>We will leave the parameters $\alpha\,, \beta$ and $\tau = \frac{1}{\sigma}$
unchanged, but we must choose a prior for the number of degrees of
freedom $\nu\,.$</p>

<p>We wand a robust estimate, so we want a prior with a small
number of degrees of freedom. However, $\nu \approx 0$
can be hard to handle from a numeric perspective,
since the resulting distribution decreases very slowly 
as one steps away from the peak.
For the above reason, we choose a Gamma prior with $\alpha=4$
and $\beta=2\,.$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_robust</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">'</span><span class="s">tau</span><span class="sh">'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Gamma</span><span class="p">(</span><span class="sh">'</span><span class="s">nu</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">StudentT</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">turnout</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">inequality</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">tau</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_robust</span><span class="p">:</span>
    <span class="n">trace_robust</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> 
                                             <span class="n">idata_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">log_likelihood</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span> 
                                             <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace_robust</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/trace_robust.webp" alt="The trace of the robust model" /></p>

<p>The trace doesn’t show relevant issues, so we can compute the posterior predictive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model_robust</span><span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">StudentT</span><span class="p">(</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">x_plt</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">tau</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_robust</span><span class="p">:</span>
    <span class="n">ppc_robust</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_robust</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">],</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>


<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">ppc_robust</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">median</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">ppc_robust</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]),</span>
                <span class="n">ppc_robust</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">grey</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">turnout</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">inequality</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">/home/stippe/thestippe.github.io/docs/assets/images/statistics/robust_regression/ppc_robust.webp</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/ppc_robust.webp" alt="The PPC of the robust model" /></p>

<p>This distribution does a better job in reproducing the data, but
it tells a very different story from the normal model.</p>

<p>While in fact in the above model an increase of the turnout
translated into a reduction of the average inequality,
with this robust model this conclusion does not appear so clearly.</p>

<p>Let us try and see what does the LOO can tell us.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loo_normal</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="nf">loo</span><span class="p">(</span><span class="n">trace_norm</span><span class="p">,</span> <span class="n">model_norm</span><span class="p">)</span>
<span class="n">loo_robust</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="nf">loo</span><span class="p">(</span><span class="n">trace_robust</span><span class="p">,</span> <span class="n">model_robust</span><span class="p">)</span>

<span class="n">df_loo</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="nf">compare</span><span class="p">({</span><span class="sh">'</span><span class="s">Normal model</span><span class="sh">'</span><span class="p">:</span> <span class="n">trace_norm</span><span class="p">,</span> <span class="sh">'</span><span class="s">Robust model</span><span class="sh">'</span><span class="p">:</span> <span class="n">trace_robust</span><span class="p">})</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_compare</span><span class="p">(</span><span class="n">df_compare</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/loo.webp" alt="The plot of the LOO cross-validation" /></p>

<p>The LOO is slightly better for the normal model,
they are however very similar. Let us try and understand why.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_compare</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right">rank</th>
      <th style="text-align: right">elpd_loo</th>
      <th style="text-align: right">p_loo</th>
      <th style="text-align: right">elpd_diff</th>
      <th style="text-align: right">weight</th>
      <th style="text-align: right">se</th>
      <th style="text-align: right">dse</th>
      <th style="text-align: left">warning</th>
      <th style="text-align: left">scale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Normal model</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">-30.8017</td>
      <td style="text-align: right">4.74402</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0.879221</td>
      <td style="text-align: right">4.33551</td>
      <td style="text-align: right">0</td>
      <td style="text-align: left">True</td>
      <td style="text-align: left">log</td>
    </tr>
    <tr>
      <td style="text-align: left">Robust model</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">-32.452</td>
      <td style="text-align: right">6.97777</td>
      <td style="text-align: right">1.65029</td>
      <td style="text-align: right">0.120779</td>
      <td style="text-align: right">4.68686</td>
      <td style="text-align: right">2.10687</td>
      <td style="text-align: left">False</td>
      <td style="text-align: left">log</td>
    </tr>
  </tbody>
</table>

<p>The difference is $1.65\,,$ and the difference due to the number
of the degrees of freedom is the difference of the $p_loo\,,$
which is approximately 2.2, so the entire preference is due
to the lower number of degrees of freedom of the normal distribution.</p>

<p>We can see, however, that the LOO estimate for the normal
model has a warning. This generally happens because the ELPD
estimate is not exact, and it’s only reliable when 
removing one point does not affect too much log predictive density.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loo_normal</span>
</code></pre></div></div>

<div class="code">
Computed from 16000 posterior samples and 18 observations log-likelihood matrix.
<br />
&nbsp; &nbsp; &nbsp; &nbsp;         Estimate       SE
<br />
elpd_loo   -30.80     4.34
<br />
p_loo        4.74        -
<br />

There has been a warning during the calculation. Please check the results.
<br />
- - - -
<br />

Pareto k diagnostic values:
<br />
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Count   Pct.
<br />
(-Inf, 0.5]   (good)       16   88.9%
<br />
 (0.5, 0.7]   &nbsp; (ok)  &nbsp; 0    0.0%
<br />
 (0.7, 1]   &nbsp; (bad)  &nbsp;  2   11.1%
<br />
   (1, Inf)   (very bad)    0    0.0%
</div>

<p>There are two points which strongly affect our parameters,
and one reasonable assumption is that one of those is the South Africa.</p>

<p>Let us try and see what does it happens once we remove it.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>with pm.Model() as model_norm_red:
    alpha = pm.Normal('alpha', mu=0, sigma=20)
    beta = pm.Normal('beta', mu=0, sigma=20)
    tau = pm.HalfNormal('tau', sigma=5)
    y = pm.Normal('y', mu=alpha+df_turnout['turnout'].values[:17]*beta, observed=df_turnout['inequality'].values[:17], tau=tau)

with model_norm_red:
    trace_norm_red = pm.sample(draws=2000, chains=4, tune=2000,
                               idata_kwargs = {'log_likelihood': True},
                               random_seed=rng)

az.plot_trace(trace_norm_red)

</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/trace_norm_red.webp" alt="The trace for the new normal model" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model_norm_red</span><span class="p">:</span>
    <span class="n">y_pred_red</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">x_plt</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_norm_red</span><span class="p">:</span>
    <span class="n">ppc_norm_red</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_norm_red</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">],</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">ppc_norm_red</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">ppc_norm_red</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]),</span>
                <span class="n">ppc_norm_red</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">]),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">grey</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">turnout</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">df_turnout</span><span class="p">[</span><span class="sh">'</span><span class="s">inequality</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">/home/stippe/thestippe.github.io/docs/assets/images/statistics/robust_regression/ppc_norm_red.webp</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/ppc_norm_red.webp" alt="The PPC for the new model" /></p>

<p>This result looks much more to the robust estimate than to 
the full normal estimate.
While in the full normal model the parameter beta was
not compatible with 0, both for the robust and for the reduced
normal model it is.
This implies that those models contradict the full normal model,
which shows a negative association between the turnover
and the average income inequality.
Since the conclusion of the full normal model are heavily 
affected by the South Africa, before drawing
any conclusion one should carefully assess whether does this
makes sense. Is the South Africa really representative or
is it a special case?</p>

<h2 id="conclusions">Conclusions</h2>

<p>We have discussed how to perform a robust linear regression,
and we have shown with an example that using it instead of a normal
linear regression makes our model more stable to the presence
of non-representative items.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/robust_regression/" /><summary type="html"><![CDATA[Reducing sensitivity to large deviations]]></summary></entry><entry><title type="html">Introduction to the linear regression</title><link href="http://localhost:4000/statistics/regression" rel="alternate" type="text/html" title="Introduction to the linear regression" /><published>2024-01-25T00:00:00+01:00</published><updated>2024-01-25T00:00:00+01:00</updated><id>http://localhost:4000/statistics/regression</id><content type="html" xml:base="http://localhost:4000/statistics/regression"><![CDATA[<p>So far we discussed about how to model one variable. With this
post we will start a discussion on how to model the dependence of one
variable on other variables, named <strong>covariates</strong>, <strong>confounders</strong>, <strong>regressors</strong>,
<strong>predictors</strong>
or <strong>risk factors</strong> depending on the research area we are dealing with.</p>

<h2 id="regression">Regression</h2>

<p>In regression we want to model the dependence of one variable
\(Y\) on one or more external variables \(X\).
In other words, we are trying to determine an $f$ such that</p>

\[Y_i = f(X_i, \theta) + \varepsilon_i\]

<p>where $\varepsilon_i$ is some random noise and $\theta$ represents a set of parameters.
In the case of regression, we are not interested in modelling $X_i$.
What we want to model is the <strong>statistical dependence</strong>, which is not an exact one, since
we assume that there is some noise which makes our dependence inaccurate.
This fact makes statistical dependence different from the mathematical dependence,
where the relation is exactly fulfilled.
We must also draw a distinction between statistical dependence and causal dependence,
since a common misconception is that finding a statistical dependence 
implies a causal relation between $X$ and $Y$.</p>

<div class="emphbox">
Causal inference requires much stronger
assumptions than statistical inference.
</div>

<p>We are only allowed to draw conclusions about causality
when these assumptions are satisfied, as we will discuss later in this blog.
Notice that referring to $X$ as the risk factor is usually done in the context
of causal inference, and we will therefore avoid this term for now.</p>

<p>As pointed out by John Carlin in <a href="https://arxiv.org/pdf/2309.06668.pdf">this paper</a>, there are two main purposes for regression
other than causal inference: we may either want to <strong>describe</strong> a relation between $X$ and $Y\,,$
or we may desire to use our model to <strong>predict</strong> the value of $Y$ one $X$ has been measured.</p>

<p>By Taylor expanding $f$ around $X=0$ we have that the simplest
dependence we can assume is</p>

\[Y_i = \theta_0 + \theta_1 X_i + \varepsilon_i\,,\]

<p>we are therefore assuming the <strong>additivity</strong> of $Y_i$ with respect to $X_i\,.$</p>

<p>The assumption which is by far the most common for $\varepsilon_i$ is</p>

\[\varepsilon_i \sim \mathcal{N}(0, \sigma)\]

<p>We are therefore assuming that the errors are normally distributed, and that the
variance is independent on $X_i\,.$
The constant variance assumption is named <strong>homoscedasticity</strong>,
while the condition of variable variance is named <strong>heteroskedasticity</strong>.</p>

<p>Let us now take a look at our parameters:</p>
<ul>
  <li>$\theta_1$ is the average $Y$ difference of two groups with $\Delta Y = 1\,.$</li>
  <li>$\theta_0$ is the intercept of the model. If our data includes $X=0$ we can interpret $\theta_0$ as the value of $Y$ when $X=0\,.$</li>
  <li>$\sigma$ is the average variance.</li>
</ul>

<h2 id="gdp-life-expectancy-relation">GDP-Life expectancy relation</h2>

<p>In order to understand our model, we will apply it to investigate the relation between the gross domestic product of
a country and its life expectancy.</p>

<p>First of all, let us import the relevant libraries.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">requests</span>
<span class="kn">import</span> <span class="n">json</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">zipfile</span> <span class="kn">import</span> <span class="n">ZipFile</span>
<span class="kn">import</span> <span class="n">io</span>
</code></pre></div></div>

<p>We can now download the GDP data from the IMF rest API as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">https://www.imf.org/external/datamapper/api/v1/NGDPDPC</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">gdp</span><span class="p">:</span>
    <span class="n">data_gdp</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">loads</span><span class="p">(</span><span class="n">gdp</span><span class="p">.</span><span class="n">content</span><span class="p">)</span>

<span class="n">dt</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="p">[</span><span class="n">data_gdp</span><span class="p">[</span><span class="sh">'</span><span class="s">values</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">NGDPDPC</span><span class="sh">'</span><span class="p">][</span><span class="n">key</span><span class="p">][</span><span class="sh">'</span><span class="s">2021</span><span class="sh">'</span><span class="p">]]</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">data_gdp</span><span class="p">[</span><span class="sh">'</span><span class="s">values</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">NGDPDPC</span><span class="sh">'</span><span class="p">]</span> <span class="k">if</span> <span class="sh">'</span><span class="s">2021</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">data_gdp</span><span class="p">[</span><span class="sh">'</span><span class="s">values</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">NGDPDPC</span><span class="sh">'</span><span class="p">][</span><span class="n">key</span><span class="p">]}</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="nf">from_dict</span><span class="p">(</span><span class="n">dt</span><span class="p">).</span><span class="nf">transpose</span><span class="p">().</span><span class="nf">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="sh">'</span><span class="s">gdp</span><span class="sh">'</span><span class="p">})</span>

</code></pre></div></div>

<p>We also download the country names from the same API and we combine the two tables</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">https://www.imf.org/external/datamapper/api/v1/countries</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">countries</span><span class="p">:</span>
    <span class="n">data_countries</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">loads</span><span class="p">(</span><span class="n">countries</span><span class="p">.</span><span class="n">content</span><span class="p">)</span>

<span class="n">df_countries</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="nf">from_dict</span><span class="p">({</span><span class="n">key</span><span class="p">:</span> <span class="p">[</span><span class="n">data_countries</span><span class="p">[</span><span class="sh">'</span><span class="s">countries</span><span class="sh">'</span><span class="p">][</span><span class="n">key</span><span class="p">][</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]]</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">data_countries</span><span class="p">[</span><span class="sh">'</span><span class="s">countries</span><span class="sh">'</span><span class="p">]}).</span><span class="nf">transpose</span><span class="p">().</span><span class="nf">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">})</span>

<span class="n">df_n</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">df_countries</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="sh">'</span><span class="s">inner</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p>The table containing the life expectancy can be downloaded from <a href="https://data.worldbank.org/indicator/SP.DYN.LE00.IN">this page of the World Bank website</a>.
Rather than clicking on the website, we will download it automatically as follows</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">https://api.worldbank.org/v2/en/indicator/SP.DYN.LE00.IN?downloadformat=csv</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">content</span>
    <span class="n">z</span> <span class="o">=</span> <span class="nc">ZipFile</span><span class="p">(</span><span class="n">io</span><span class="p">.</span><span class="nc">BytesIO</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>

<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">z</span><span class="p">.</span><span class="nf">namelist</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">name</span><span class="p">.</span><span class="nf">startswith</span><span class="p">(</span><span class="sh">'</span><span class="s">API</span><span class="sh">'</span><span class="p">):</span>
        <span class="n">dt</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="nf">extract</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="n">df_lifexp</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">skiprows</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

</code></pre></div></div>

<p>We can now combine the two dataframes. We will stick to the year 2021, as it is the most recent year for
most of the countries.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_le</span> <span class="o">=</span> <span class="n">df_lifexp</span><span class="p">[[</span><span class="sh">'</span><span class="s">Country Code</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">2021</span><span class="sh">'</span><span class="p">]].</span><span class="nf">set_index</span><span class="p">(</span><span class="sh">'</span><span class="s">Country Code</span><span class="sh">'</span><span class="p">).</span><span class="nf">dropna</span><span class="p">()</span>

<span class="n">df_final</span> <span class="o">=</span> <span class="n">df_n</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">df_le</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="sh">'</span><span class="s">inner</span><span class="sh">'</span><span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">pairplot</span><span class="p">(</span><span class="n">df_final</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/regression/pairplot.webp" alt="The pairplot of our variables" /></p>

<p>There appears to be no linear relation between the two. However, by a suitable variable redefinition,
we can get linearity within a good approximation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_final</span><span class="p">[</span><span class="sh">'</span><span class="s">log GDP</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">df_final</span><span class="p">[</span><span class="sh">'</span><span class="s">gdp</span><span class="sh">'</span><span class="p">])</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">pairplot</span><span class="p">(</span><span class="n">df_final</span><span class="p">[[</span><span class="sh">'</span><span class="s">log GDP</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Life expectancy</span><span class="sh">'</span><span class="p">]])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/regression/pairplot_log.webp" alt="The pairplot of our variables" /></p>

<p>The homoscedasticity only seems to hold approximately, as in the region with lower GDP the data shows
a larger variance with respect to countries with higher GDP.
For the sake of simplicity, we will stick to the constant variance assumption, and we will see how to deal
with heterogenic variance in a future post.</p>

<p>Let us now setup our model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">'</span><span class="s">sigma</span><span class="sh">'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">beta</span><span class="o">*</span><span class="n">df_final</span><span class="p">[</span><span class="sh">'</span><span class="s">log GDP</span><span class="sh">'</span><span class="p">],</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_final</span><span class="p">[</span><span class="sh">'</span><span class="s">Life expectancy</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">beta</span><span class="o">*</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>  <span class="c1"># We want to get the error bands for all the values of x_pred
</span>
<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">sigma</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/regression/trace.webp" alt="The trace plot" /></p>

<p>The trace looks fine. We will directly check the posterior predictive distribution.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>

<span class="n">y_mean</span> <span class="o">=</span> <span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">x_pred</span><span class="p">))).</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_low</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">quantile</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">x_pred</span><span class="p">))),</span> <span class="mf">0.025</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_high</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">quantile</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">y_pred</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">x_pred</span><span class="p">))),</span> <span class="mf">0.975</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">y_low</span><span class="p">,</span> <span class="n">y_high</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">lightgray</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">grey</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">df_final</span><span class="p">[</span><span class="sh">'</span><span class="s">log GDP</span><span class="sh">'</span><span class="p">],</span> <span class="n">df_final</span><span class="p">[</span><span class="sh">'</span><span class="s">Life expectancy</span><span class="sh">'</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">x_pred</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">x_pred</span><span class="p">)])</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Log GDP per Capita</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Life Expectancy 2021</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/regression/ppc.webp" alt="The posterior predictive" /></p>

<p>While our model correctly reproduces the relation between the GDP and the average life expectancy,
it fails to reproduce the observed variance, confirming that the homoscedasticity assumption is violated.</p>

<p>Let us now inspect which nations show the biggest error</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">df_final</span><span class="p">[</span><span class="sh">'</span><span class="s">Life expectancy</span><span class="sh">'</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="n">df_final</span><span class="p">[</span><span class="sh">'</span><span class="s">log GDP</span><span class="sh">'</span><span class="p">]).</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="code">
NGA   -13.359600
<br />
SWZ   -12.138907
<br />
GNQ   -11.857505
<br />
NAM   -10.662630
<br />
BWA   -10.601424
</div>

<p>The above nations are Nigeria, eSwatini, Guyana, Namibia and Botswana,
so it looks like our model fails to reproduce some
African countries, which have an average life expectancy
much lower than non-African countries with similar GDP.</p>

<p>Let us also check if the assumption about the normality of the deviation
from the average trend is fulfilled within a good approximation</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span>
<span class="p">(</span><span class="n">df_final</span><span class="p">[</span><span class="sh">'</span><span class="s">Life expectancy</span><span class="sh">'</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> 
 <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="n">df_final</span><span class="p">[</span><span class="sh">'</span><span class="s">log GDP</span><span class="sh">'</span><span class="p">]),</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span><span class="mf">1.5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Residuals histogram</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/regression/res_plot.webp" alt="The pairplot of our variables" /></p>

<p>It appears that the distribution of the residual is left skewed, so in order
to improve our model we could use a skewed distribution,
like the <a href="https://www.pymc.io/projects/docs/en/stable/api/distributions/generated/pymc.SkewNormal.html">Skewed Normal distribution</a> or the <a href="https://arxiv.org/pdf/2303.05615.pdf">Variance Gamma</a>
instead of a normal distribution.</p>

<p>This is however a somehow more advanced topic with respect to an introductory
post on linear regression, so we won’t implement these models here.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We introduced the linear model, and we saw how to implement it
with an example.
We discussed the interpretation of the parameters and some
of the most relevant assumptions we made about data.</p>

<h2 id="suggested-readings">Suggested readings</h2>

<ul>
  <li><cite> Kutner, M. H., Nachtsheim, C., Neter, J. (2004). Applied linear regression models. UK: McGraw-Hill/Irwin. </cite></li>
  <li><cite> Gelman, A., Hill, J., Vehtari, A. (2020). Regression and Other Stories. India: Cambridge University Press. </cite></li>
</ul>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/linear_regression/" /><summary type="html"><![CDATA[Including dependence on external variables]]></summary></entry><entry><title type="html">Model comparison</title><link href="http://localhost:4000/statistics/model_averaging" rel="alternate" type="text/html" title="Model comparison" /><published>2024-01-23T00:00:00+01:00</published><updated>2024-01-23T00:00:00+01:00</updated><id>http://localhost:4000/statistics/model_averaging</id><content type="html" xml:base="http://localhost:4000/statistics/model_averaging"><![CDATA[<p>Most of the times, you won’t deal with a single model
for one dataset, but you will try many models
at the same time.</p>

<p>In this phase of the Bayesian workflow
we will discuss some methods to compare
models.</p>

<p>Comparing model sometimes may be understood as choosing the best model, but in most cases it means to asses which model is better to describe or predict some particular aspect of your data.
Model comparison can be done analytically in some case, but most of the time it will be done numerically or graphically, and here we will give an overview of the most important tools.</p>

<p>Here we will take a look at two of the most important
methods, the Bayes factor analysis and the
Leave One Out cross-validation.</p>

<h2 id="bayes-factors">Bayes factors</h2>

<p>Let us go back to the Beta-Binomial model
that we discussed in <a href="/betabin">this post</a>,
and let us assume that we have two candidate models to describe our data:
model 0 has Jeffreys prior, which mean that the prior
is a beta distribution with $\alpha=1/2$ and $\beta=1/2\,.$
The second model, named “model 2”, is instead centered in $0.5$ and has
\(\alpha = \beta = 10\,.\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="n">y</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">4320</span>

<span class="n">model_0</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">}</span>
<span class="n">model_1</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">:</span> <span class="mi">10</span><span class="p">}</span>

<span class="n">x_pl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">([</span><span class="n">model_0</span><span class="p">,</span> <span class="n">model_1</span><span class="p">]):</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="nf">beta</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">model</span><span class="p">[</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">],</span> <span class="n">b</span><span class="o">=</span><span class="n">model</span><span class="p">[</span><span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">]).</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x_pl</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">model </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="n">legend</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">$\theta$</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">$p(\theta)$  </span><span class="sh">"</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/model_averaging/priors.webp" alt="The priors used in this post" /></p>

<p>Given the two models $M_0$ and $M_1$ we may ask which one we prefer, given the data. The probability of the model given the data is given by</p>

\[p(M_k | y) = \frac{p(y | M_k)}{p(y)} p(M_k)\]

<p>where the quantity</p>

\[p(y | M_k)\]

<p>is the <strong>marginal likelihood</strong> of the model.</p>

<p>If we assign the same prior probability $p(M_k)\,,$
to each model,
since $p(y)$ is the same for both models,
then we can simply replace $p(M_k | y)$ with the
marginal likelihood.</p>

<p>As usual, an analytic calculation is only possible in a very limited number of models.</p>

<p>One may think to compute $p(M_k| y)$ by starting from $p(y | \theta, M_k)$ and integrating out $\theta$ but doing this naively is generally not a good idea, as
this method is unstable and prone to numerical errors.</p>

<p>However can use the Sequential Monte Carlo to compare the two models, since it allows to estimate the (log) marginal likelihood of the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">traces</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="p">[</span><span class="n">model_0</span><span class="p">,</span> <span class="n">model_1</span><span class="p">]:</span>
    <span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Beta</span><span class="p">(</span><span class="sh">"</span><span class="s">theta</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">m</span><span class="p">[</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">],</span> <span class="n">beta</span><span class="o">=</span><span class="n">m</span><span class="p">[</span><span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">])</span>
        <span class="n">yl</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">(</span><span class="sh">"</span><span class="s">yl</span><span class="sh">"</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
        <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_smc</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">return_inferencedata</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>
        <span class="n">models</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">traces</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<p>Let us inspect as usual the traces.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging/trace_0.webp" alt="The trace for model 0" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging/trace_1.webp" alt="The trace for model 1" /></p>

<p>What one usually computes is the <strong>Bayes factor</strong> of the models, which is the ratio between the posterior probability of the model (which in this case is simply the
ratio between the marginal likelihoods).</p>

<table>
  <thead>
    <tr>
      <th>$BF = p(M_0)/p(M_1)$</th>
      <th>interpretation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$BF&lt;10^{0}$</td>
      <td>support to $M_1$ (see reciprocal)</td>
    </tr>
    <tr>
      <td>$10^{0}\leq BF&lt;10^{1/2}$</td>
      <td>Barely worth mentioning support to $M_0$</td>
    </tr>
    <tr>
      <td>$10^{1/2}\leq BF&lt;10^2$</td>
      <td>Substantial support to $M_0$</td>
    </tr>
    <tr>
      <td>$10^{2} \leq BF&lt;10^{3/2}$</td>
      <td>Strong support to $M_0$</td>
    </tr>
    <tr>
      <td>$10^{3/2} \leq BF&lt;10^2$</td>
      <td>Very strong support to $M_0$</td>
    </tr>
    <tr>
      <td>$\geq 10^2$</td>
      <td>Decisive support to $M_0$</td>
    </tr>
  </tbody>
</table>

<p>This can be easily done as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="nf">log10</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span>
    <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">sample_stats</span><span class="p">.</span><span class="n">log_marginal_likelihood</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
    <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">sample_stats</span><span class="p">.</span><span class="n">log_marginal_likelihood</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span><span class="p">)))</span>
</code></pre></div></div>

<div class="code">
17.29
</div>

<p>As we can see, there is a substantial preference
for model 0.
We can better understand this result if we compare our estimate with the
frequentist confidence interval,
which we recall being \([0.0004, 0.0028]\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="nf">plot_forest</span><span class="p">(</span><span class="n">traces</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">rope</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0004</span><span class="p">,</span> <span class="mf">0.0028</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/model_averaging/forest.webp" alt="The forest plot of the two models" /></p>

<p>We can see that the preferred model HDI corresponds
with the frequentist CI, while the interval 
predicted by the second model only partially
overlaps with the frequentist CI.</p>

<p>We can also inspect the posterior predictive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ppc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">models</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">m</span><span class="p">:</span>
        <span class="n">ppc</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ppc</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">yl</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">([</span><span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">]).</span><span class="n">values</span>
</code></pre></div></div>
<div class="code">
array(7.5875)
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ppc</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">'</span><span class="s">yl</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">([</span><span class="sh">'</span><span class="s">chain</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">draw</span><span class="sh">'</span><span class="p">]).</span><span class="n">values</span>
</code></pre></div></div>
<div class="code">
array(17.02425)
</div>

<p>We recall that the observed value for $y$ was 7,
which is much closer to the one provided by the preferred
model than to the one provided by Model 1.</p>

<h2 id="leave-one-out">Leave One Out</h2>

<p>The second method that we will see is the Leave One Out (LOO) cross validation.
This method is generally preferred to the above one, as it has been pointed out
that Bayes factors are appropriate only when one of the models is true,
while in real world problems we don’t have any certainty about which is the model that
generated the data, assuming that it makes sense to claim that it exists such a model.
Moreover, the sampler used to compute the Bayes factor, namely Sequential Monte Carlo,
is generally less stable than the standard one used by PyMC, which is the NUTS sampler.
There are other, more philosophical reasons, pointed out by Gelman in <a href="https://statmodeling.stat.columbia.edu/2017/07/21/bayes-factor-term-came-references-generally-hate/">this post</a>,
but for now we won’t dig into this kind of discussion.</p>

<p>The LOO method is much more in the spirit of the Machine Learning, where
one splits the sample into a training set and a test set.
The train set is used to find the parameters, while the second one is
used to assess the performances of the model for new data.
This method, namely the <strong>cross validation</strong>, is by far the most
reliable one, and we generally recommend to use it.
It is however very common that the dataset is too small to allow
a full cross-validation.
The LOO cross validation is equivalent to the computation of</p>

\[ELPD = \sum_i \log p_{-i}(y_i)\]

<p>where \(p_{-i}(y_i)\) is the posterior predictive probability
of the point \(y_i\) relative to the model fitted by removing \(y_i\,.\)</p>

<p>Since we already discussed how to implement this method in the post on the 
<a href="/statistics/negbin">negative binomial model</a>, and since it doesn’t make much sense
to check what happens by removing one point out of four thousands,
we won’t repeat it again.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We verified two of the main methods to make model comparison.
We will see other more advanced methods in the future, but for now 
you should keep in mind that generally the LOO is the preferred one.</p>]]></content><author><name>Data-perspectives</name><email>dataperspectivesblog@gmail.com</email></author><category term="/statistics/" /><category term="/model_comparison/" /><summary type="html"><![CDATA[How to choose between models]]></summary></entry></feed>