<!DOCTYPE html>
<html lang="en"><head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-W9G73E5P44"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-W9G73E5P44');
</script>
          <link rel="icon" 
                type="image/png" 
                href="/docs/assets/images/dp_icon.png">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>The Beta-Binomial model | Data Perspectives</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="The Beta-Binomial model" />
<meta name="author" content="Data-perspectives" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Dealing with binary outcomes" />
<meta property="og:description" content="Dealing with binary outcomes" />
<link rel="canonical" href="http://localhost:4000/statistics/betabin" />
<meta property="og:url" content="http://localhost:4000/statistics/betabin" />
<meta property="og:site_name" content="Data Perspectives" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-07-25T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="The Beta-Binomial model" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Data-perspectives"},"dateModified":"2024-07-25T00:00:00+00:00","datePublished":"2024-07-25T00:00:00+00:00","description":"Dealing with binary outcomes","headline":"The Beta-Binomial model","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/statistics/betabin"},"url":"http://localhost:4000/statistics/betabin"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Perspectives" />


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
<a rel="me" href="https://vis.social/@thestippe" style="display: none;">Mastodon</a>
<link rel="manifest" href="manifest.json">
</head>
<body>

        <div id='upperBar'><header class="site-header">

        <div id='upperBarr'>
        <script src="https://d3js.org/d3.v7.js"></script>
                <div class="wrapper" style="display:flex;">

                
                
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                    
                    
                    
                    
                    

        <ul hidden='hidden' id="postList"><li>
                        Random models and mixed models;/statistics/random_models;1
                </li><li>
                        Hierarchical models and meta-analysis;/statistics/hierarchical_metaanalysis;2
                </li><li>
                        Hierarchical models;/statistics/hierarchical_models;3
                </li><li>
                        Poisson regression;/statistics/poisson_regression;4
                </li><li>
                        Logistic regression;/statistics/logistic_regression;5
                </li><li>
                        Robust linear regression;/statistics/robust_regression;6
                </li><li>
                        Multi-linear regression;/statistics/multivariate_regression;7
                </li><li>
                        Linear regression with binary input;/statistics/regression_binary_input;8
                </li><li>
                        Horseshoe priors;/statistics/horseshoe;9
                </li><li>
                        Introduction to the linear regression;/statistics/regression;10
                </li><li>
                        MRP;/statistics/mrp;11
                </li><li>
                        Model comparison, cont.;/statistics/model_averaging_cont;12
                </li><li>
                        Model comparison;/statistics/model_averaging;13
                </li><li>
                        Application of the Lotka-Volterra model;/statistics/lotka_volterra;14
                </li><li>
                        Differential equations;/statistics/ode;15
                </li><li>
                        Introduction to GIS;/gis/gis_intro;16
                </li><li>
                        Re-parametrizing your model;/statistics/reparametrization;17
                </li><li>
                        Predictive checks;/statistics/predictive_checks;18
                </li><li>
                        Trace inspection;/statistics/trace_inspection;19
                </li><li>
                        Introduction to the Bayesian workflow;/statistics/bayesian_workflow;20
                </li><li>
                        Mixture models;/statistics/mixture;21
                </li><li>
                        Multidimensional distributions;/statistics/categories;22
                </li><li>
                        Dirichlet Process Mixture Models;/statistics/dp;23
                </li><li>
                        The Gaussian model;/statistics/reals;24
                </li><li>
                        Bayesian Additive Regression Trees;/statistics/bart;25
                </li><li>
                        Bonus: counting animals in a park;/statistics/hypergeom;26
                </li><li>
                        Splines;/statistics/spline;27
                </li><li>
                        The Negative Binomial model;/statistics/negbin;28
                </li><li>
                        Gaussian processes regression;/statistics/gp_example;29
                </li><li>
                        Gaussian processes;/statistics/gp;30
                </li><li>
                        The Poisson model;/statistics/poisson;31
                </li><li>
                        Nonparametric models;/statistics/nonparametric_intro;32
                </li><li>
                        Time series;/statistics/time_series;33
                </li><li>
                        The Beta-Binomial model;/statistics/betabin;34
                </li><li>
                        Section introduction;/statistics/simple_models_intro;35
                </li><li>
                        Structural time series;/statistics/structural_time_series;36
                </li><li>
                        Some notation about probability;/statistics/probability_reminder;37
                </li><li>
                        Synthetic control;/statistics/synthetic_control;38
                </li><li>
                        How does MCMC works;/statistics/mcmc_intro;39
                </li><li>
                        Regression discontinuity design;/statistics/rdd;40
                </li><li>
                        Introduction to Bayesian inference;/statistics/bayes_intro;41
                </li><li>
                        An overview to statistics;/statistics/preface;42
                </li><li>
                        Difference in difference;/statistics/difference_in_differences;43
                </li><li>
                        Instrumental variable regression;/statistics/instrumental_variable;44
                </li><li>
                        Randomized controlled trials;/statistics/randomized;45
                </li><li>
                        Causal inference and Bayesian networks;/statistics/causal_intro_2;46
                </li><li>
                        Causal inference;/statistics/causal_intro;47
                </li><li>
                        Experiment analysis with many blocking variables;/statistics/experiment_design_cont;48
                </li><li>
                        Experiment analysis;/statistics/experiment_design;49
                </li><li>
                        Introduction to Extreme Values theory;/statistics/extreme_intro;50
                </li><li>
                        Application of survival analysis with discrete times;/statistics/survival_example_2;51
                </li><li>
                        Application of survival analysis 1;/statistics/survival_example;52
                </li><li>
                        Introduction to survival analysis;/statistics/survival_analysis;53
                </li><li>
                        Drawing geographic maps;/dataviz/geography;54
                </li><li>
                        The Gestalt principles;/dataviz/gestalt;55
                </li><li>
                        Design tricks;/dataviz/design-introduction;56
                </li><li>
                        How to choose a color map;/dataviz/palettes-introduction;57
                </li><li>
                        Introduction to color perception;/dataviz/color-introduction;58
                </li><li>
                        Drawing is redrawing;/dataviz/gender-economist;59
                </li><li>
                        Visual queries;/dataviz/visual-queries;60
                </li><li>
                        Channel effectiveness;/dataviz/effectiveness;61
                </li><li>
                        Evolutions of the line chart;/dataviz/linechart-evolution;62
                </li><li>
                        Beyond the 1D scatterplot;/dataviz/scatterplot-evolution;63
                </li><li>
                        Perception;/dataviz/perception;64
                </li><li>
                        Fundamental charts;/dataviz/fundamental-charts;65
                </li><li>
                        Marks and channels;/dataviz/marks-channels;66
                </li><li>
                        Data abstraction;/dataviz/data-types;67
                </li><li>
                        Data visualization;/dataviz/dataviz;68
                </li></ul>
                        <div style="display:flex">
                  <a href="/statistics/simple_models_intro" class="prev">&#8249;</a>
                  
                                <a href="/"><img class="site-masthead" src="/docs/assets/images/logo_dp.png" alt="Data Perspectives" id="logo" /></a><div id='searchNav' style="flex;">
                                        <input type="search" id="search_0" class="searchBar" onkeydown="searchText()" placeholder="Search">
                                </div>

                                <div hidden='hidden' id="search_focus">0</div>



                        </div><nav class="site-nav" style="display:flex;">
                                <input type="checkbox" id="nav-trigger" class="nav-trigger" />
                                <label for="nav-trigger">
                                        <span class="menu-icon">
                                                <svg viewBox="0 0 18 15" width="18px" height="15px">
                                                        <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
                                                </svg>
                                        </span>
                                </label>

                                <div class="trigger"><a  class="page-link" href="/about">About me</a><a  class="page-link" href="/links">Resources</a>
                                <a href="/statistics/" class="page-link">Up</a>
                                
                                </div>
                        </nav>
                  <a href="/statistics/time_series" class="next">&#8250;</a>
                  
        </div>


        <script src="/docs/assets/javascript/search.js">
        </script>
                </div>

</header>
<div class="progress-container">
    <div class="progress-bar" id="myBar"></div>
    </div>
        </div>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
          <div id='topPage'></div>
        <a href='/index'>
<img src="/docs/assets/images/background_resized.webp" alt="backround" style="margin:auto;display:block;width:1200px">

</a>
    <h1 class="post-title p-name" itemprop="name headline">The Beta-Binomial model</h1>
    <p class="post-meta"><time class="dt-published" datetime="2024-07-25T00:00:00+00:00" itemprop="datePublished">
        Jul 25, 2024
      </time></p>
    <p class="post-meta"> Reading time: <span class="reading-time" title="Estimated read time">
  
  11&prime;
</span>
</p>
  </header>
  <br>

  <div class="post-content e-content" itemprop="articleBody">
    <p>One of the hot topics of these days is the record of retracted scientific
papers in 2023, as reported by Nature in <a href="https://www.nature.com/articles/d41586-023-03974-8">this article</a>. Even Nature Journal itself had 7 retractions
out of 4320 published articles.
I wanted to have an estimate about what’s the probability that
after submitting an article to that Journal, the article gets retracted.
Let us assume that the retraction probability will be roughly constant
in the near future.
This is of course quite a strong assumptions, as it is likely that the journal
will take some measure to reduce the retraction ratio in the future, but
this is just a working assumption, since otherwise we should model
how the measures will affect the retraction probability, and modelling an
unknown trend would introduce additional arbitrariness in our model.
We decide that it’s better to stick to the simpler assumption,
so that it will be easier to control the sources of error.</p>

<h2 id="the-frequentist-way">The frequentist way</h2>

<p>Let us start by doing the calculation in the frequentist way.
First of all, we assume that each article is retracted with
the same probability $\theta \in [0, 1]\,.$
This implies that the total number of retracted articles $Y$ out of $n$
published articles is distributed
as</p>

\[Y \sim \mathcal{Binom}(\theta, n)\]

<details class="math-details">
<summary> The binomial distribution</summary>
<div class="math-details-detail">

The binomial distribution $$\mathcal{Binom}(p, n)$$ describes the probability
of the random variable $$X= \sum_{i=1}^n X_i\,, X \in \{0,1,...,n\}$$
where the $X_i \in \{0, 1\}$ are independent identically distributed random variables
following the Bernoulli distribution with probability $p:$

$$
X_i = 
\begin{cases}
1 &amp; \text{with probability } p\\
0 &amp; \text{with probability } 1-p
\end{cases}\,,
p \in [0, 1]
$$

The Bernoulli distribution has

$$
\mathbb{E}[X_i] = \sum_{i=0}^1 P(x=i)i = 0(1-p)+1p=p 
$$

and

$$
\mathbb{E}[(X_i-\mathbb{E}[X_i])^2] = \sum_{i=0}^1 P(x=i)(i-p)^2 = (1-p)(0-p)^2+p(1-p)^2 = p(1-p)
$$

Since a binomial random variable is the sum of independent Bernoulli
random variables, we immediately have

$$
\begin{align}
\mathbb{E}[X] &amp; = n p\\
Var[X] &amp; = n p (1-p) \\
\end{align}
$$

The binomial probability mass function must fulfill

$$
p(k | p, n) \propto p^k (1-p)^{n-k}\,. 
$$

By normalizing it to 1 we get

$$
p(k | p, n) = \binom{n}{k} p^k (1-p)^{n-k}
$$
</div>
</details>

<p>In other terms, we assume that we are looking for the best distribution
within the family</p>

\[\{p(k \vert \theta, n) = \binom{n}{k} \theta^k (1-\theta)^{n-k}\}_\theta\]

<p>There is more than one possible criteria to decide which is the best parameter $\theta\,.$</p>

<h3 id="the-maximum-likelihood-method">The Maximum likelihood method</h3>

<p>This method looks for the parameter $\theta$ such that the likelihood for
the observed data is maximum for any allowed value for $\theta$:</p>

\[\bar\theta : p(x \vert \bar\theta) &gt; p(x \vert \theta)\,  \forall\,  \theta \in \mathcal{D}\]

<p>where $\mathcal{D}$ is the domain we are looking for.
For the binomial model, since the parameter space is continuous, we can simply require</p>

\[\frac{d}{d\theta} \binom{n}{y} \theta^k (1-\theta)^{n-y} = 0\,.\]

<p>Since the probability mass function is positive for any $\theta$ within our domain,
we can safely take the logarithm of the likelihood and work with the so-called log-likelihood.</p>

\[\frac{d}{d\theta}\log\left( \binom{n}{y} \theta^k (1-\theta)^{n-y}\right) _\bar{\theta}= 0\,.\]

<p>By computing the derivative, we get</p>

\[\frac{y}{\bar\theta} - \frac{n-y}{1-\bar\theta} = 0\]

<p>and this can be easily solved obtaining</p>

\[\bar\theta = \frac{y}{n}\,.\]

<p>You can easily verify that it is a maximum, since the second derivative reads</p>

\[\frac{d^2}{d\theta^2}\log\left( \binom{n}{y} \theta^k (1-\theta)^{n-y}\right) _{\theta=\bar{\theta}=y/n} = -\frac{1}{\bar{\theta}(1-\bar{\theta})}&lt;0 \, if\, 0&lt;y&lt;n\]

<h3 id="the-method-of-moments">The method of moments</h3>

<p>This method matches the moments of the distribution with the observed corresponding statistics.
In our case we can simply match \(\mathbb{E}[y]\) with the observed number of successes:</p>

\[y = n \bar\theta\]

<p>and, in our case, this is equivalent to the MLE.</p>

<h3 id="estimating-the-confidence-interval">Estimating the confidence interval</h3>

<p>We can also obtain a confidence interval for $\theta\,,$
but in order to do so we must rely on the central limit theorem,
which tells us that the</p>

\[\frac{Y -n \bar{\theta}}{\sqrt{n \bar\theta (1-\bar\theta)}} \sim \mathcal{N}(0, 1)\]

<p>In order to get the $1-\alpha$ CI we only have to evaluate the CI
with a significance $\alpha$ for the
normal distribution with zero mean and unit variance.
Since the distribution is symmetric around 0, the CI is</p>

\[[z_{\alpha/2}, z_{1-\alpha/2}] = [-z_{1-\alpha/2}, z_{1-\alpha/2}]\,,\]

<p>as we are leaving outside from the CI a region with probability $\alpha\,,$
so we must leave out $\alpha/2$ on the lower side and $\alpha/2$ on the upper side.
We will stick for now to the usual $\alpha=0.05\,,$
so $z_{0.975}=1.96\,,$ and our confidence interval reads</p>

\[[
\bar{\theta} -z_{1-\alpha/2} \sqrt{\frac{\bar{\theta}(1-\bar{\theta})}{n}},
\bar{\theta} +z_{1-\alpha/2} \sqrt{\frac{\bar{\theta}(1-\bar{\theta})}{n}}
]d
=[0.4 \, 10^{-3}, 2.8\,  10^{-3}]\]

<p>We stress again that this does not represent the range where $\theta$
is within $1-\alpha$ probability, as $\theta$ is not a random variable.
What we know is that, if we repeat the experiment many times
and every time we construct a CI with significance $\alpha\,,$ then
a fraction $1-\alpha$ of the CI will contain the true value $\theta\,.$</p>

<h2 id="the-bayesian-way">The Bayesian way</h2>

<p>Also in this case, we assume that</p>

\[Y \sim \mathcal{Binom}(\theta, n)\]

<p>where $n$ is fixed.
We must now specify a prior distribution for $\theta\,,$
with the requirement that it must have $[0, 1]$ as support.
A flexible enough family of distributions is given by the Beta distribution
\(\mathcal{Beta}(\alpha, \beta)\,.\)</p>

<details class="math-details">
<summary> The beta distribution
</summary>
<br />
The beta distribution is defined via

$$
p(x | \alpha, \beta) \propto x^{\alpha-1} (1-x)^{\beta-1}\,,x\in[0, 1]
$$

The probability density function $p(x|\alpha, \beta)$ must be integrable,
we therefore have $\alpha,\beta &gt; 0\,.$
This distribution takes its name from the normalization constant,
which is the inverse of the Euler beta function

$$
B(\alpha, \beta) = \int_0^1 dx x^{\alpha-1}(1-x)^{\beta-1}
$$

$$
p(x | \alpha, \beta) = \frac{1}{B(\alpha, \beta)} x^{\alpha-1} (1-x)^{\beta-1}
$$

The expected value for a random variable distributed according to the beta distribution is

$$
\begin{align}
\mathbb{E}[X] = &amp; \frac{1}{B(\alpha, \beta)} \int_0^1 dx x x^{\alpha-1} (1-x)^{\beta-1}
= \frac{B(\alpha+1, \beta)}{B(\alpha, \beta)} =
\frac{\Gamma(\alpha+1)\Gamma(\beta)}{\Gamma(\alpha+\beta+1)} \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}
= \frac{\alpha}{\alpha+\beta}
\end{align}
$$

In a similar way one obtains

$$
Var[X] = \frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}
$$

</details>

<p>Now the question is: which values for $\alpha$ and $\beta$ should we choose should we choose?
This is one of the central issues in Bayesian statistics, and there are
many ongoing debates on this.
A possible choice would be to choose $\alpha=\beta=1\,,$
and in this way we would reduce our distribution to the uniform distribution
\(\mathcal{U}(0, 1)\,.\)
This makes sense, as we don’t want to put too much information in our prior.
If this is our aim, an even better choice is the <a href="https://en.wikipedia.org/wiki/Jeffreys_prior">Jeffreys prior</a> for the Binomial distribution, which corresponds to
\(\alpha = \beta = 1/2\,.\)
Roughly speaking, this is equivalent to the requirement that <strong>locally</strong>
there is the least information as possible.
We can now build our model as</p>

\[\begin{align}
\theta &amp; \sim \mathcal{Beta}(1/2, 1/2)
\\
Y &amp; \sim \mathcal{Binom}(\theta, n)
\end{align}\]

<p>An analytic treatment would be possible, but we prefer to show how to use python
in order to solve this problem.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Let us first import the libraries
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># The data
</span><span class="n">k</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">4320</span>

<span class="c1"># For reproducibility
</span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># The model
</span>
<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">betabin</span><span class="p">:</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Beta</span><span class="p">(</span><span class="s">'theta'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
</code></pre></div></div>

<p>In this way, the model is specified.
We can now perform the sampling (or, in jargon, <strong>compute the traces</strong>)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">betabin</span><span class="p">:</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></div>

<p>All the sampled data is available inside the trace object.
We can visually inspect the traces as</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/betabin/trace.webp" alt="The sampled trace" /></p>

<p>The trace looks fine.
For the moment trust me, we will discuss later in this blog how to verify if the sampling had problems.</p>

<p>We can also get some useful information like the mean or the standard variance,
together with some estimate of the error.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right">mean</th>
      <th style="text-align: right">sd</th>
      <th style="text-align: right">hdi_3%</th>
      <th style="text-align: right">hdi_97%</th>
      <th style="text-align: right">mcse_mean</th>
      <th style="text-align: right">mcse_sd</th>
      <th style="text-align: right">ess_bulk</th>
      <th style="text-align: right">ess_tail</th>
      <th style="text-align: right">r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">theta</td>
      <td style="text-align: right">0.002</td>
      <td style="text-align: right">0.001</td>
      <td style="text-align: right">0.001</td>
      <td style="text-align: right">0.003</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">1843</td>
      <td style="text-align: right">2118</td>
      <td style="text-align: right">1</td>
    </tr>
  </tbody>
</table>

<p>In the above table “hdi” corresponds to the Highest Density Interval (HDI), and it is the central interval
which contains the specified probability, so we may say that with probability $0.94$ the parameter
$\theta$ is inside $[0.001, 0.003]\,.$
The remaining statistics (MCSE, ESS and $\hat{R}$) will be discussed in a future post.</p>

<p>We can also plot the posterior distribution for $\theta$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_posterior</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/betabin/posterior.webp" alt="The sampled posterior" /></p>

<h2 id="comparing-the-results">Comparing the results</h2>

<p>As we have seen, the frequentist confidence interval for our estimate is
\([0.4 \, 10^{-3}, 2.8\,  10^{-3}]\)
which is very close to the Bayesian credible interval, namely \([0.001, 0.003]\,.\)
The reason for this is that, for a large class of well definite models
with a finite number of parameters,
when the sample size grows, the Bayesian
credible interval approaches the frequentist confidence interval,
and this result is known as the <a href="https://encyclopediaofmath.org/wiki/Bernstein-von_Mises_theorem">Bernstein-von Mises theorem</a>.</p>

<p>There have been proposed many methods for computing the confidence interval in the 
small sample limit, such as the Wilson confidence interval or the Clopper-Pearson
confidence interval, but they are often hard to implement and to explain than the method
used above.
Due to these difficulties, it is generally recommended to use the central limit
theorem to estimate confidence intervals.</p>

<p>We therefore have that we can only use the central limit theorem
to compute the confidence interval when the sample size is large,
while we can always compute the credible interval.
We also have that, when the sample size is large enough, we can approximate
the confidence interval with the credible interval.</p>

<p>For this reason, we see no reason not to stick to the Bayesian framework rather
limiting ourselves to large samples and getting similar results.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We estimated the retraction probability on Nature Journal both in the frequentist
way and in the Bayesian one by using PyMC in the latter case.
We showed that the Bayesian approach allows for a simpler interpretation of the results.
Moreover, reporting the full posterior provides much more information about how
the data constrain the parameter.
We also introduced two key issues in the Bayesian approach, the prior specification
and the assessment of the sample convergence.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span>
</code></pre></div></div>

<div class="code">
Last updated: Wed Nov 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.7
<br />
IPython version      : 8.24.0
<br />

<br />
numpy     : 1.26.4
<br />
matplotlib: 3.9.2
<br />
pymc      : 5.17.0
<br />
arviz     : 0.20.0
<br />

<br />
Watermark: 2.4.3
<br />
</div>

<h2 id="suggested-readings">Suggested readings</h2>

<ul>
  <li><cite> Gelman, A., Carlin, J. B., Stern, H. S., Rubin, D. B. (2003). Bayesian Data Analysis, Second Edition. US: Taylor &amp; Francis. </cite></li>
</ul>


  </div><a class="u-url" href="/statistics/betabin" hidden></a>

  <br>
  <div id='autograph'>
          Stippe Jul 25, 2024

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>


  <div class="wrapper">
          <div class='footer-text'>
                  Opinions are mine, mistakes too, and if you find any feel free to report it via mail or via Twitter.
                  <br>
                  Most of the material in the statistics section is an adaptation to Python of some pre-existing model.
                  <br>
                  I have tried to provide the necessary credits, but if you think that a relevant contribution is missing, please let me know.
                  <br>
                  No AI were harmed in creating this blog.
                  <br>
                  This is a blog, not a bakery: there are no cookies here!
                  <br>
                  The top image has been generated with a modified version Dan Gries' code, available at <a href="http://rectangleworld.com/blog/archives/538">http://rectangleworld.com</a>.
          </div>
          <br>

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Data-perspectives</li>
          <li><a class="u-email" href="mailto:dataperspectivesblog@gmail.com">dataperspectivesblog@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://twitter.com/SteffPy" target="_blank" title="twitter">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://stackoverflow.com/users/11065831/stefano" target="_blank" title="stackoverflow">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#stackoverflow"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://github.com/thestippe/" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://vis.social/@thestippe" target="_blank" title="mastodon">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#mastodon"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://bsky.app/profile/stippe87.bsky.social" target="_blank" title="bluesky">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#bluesky"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

  <script src="/docs/assets/javascript/scroll.js">
  </script>

</html>

