<!DOCTYPE html>
<html lang="en"><head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-W9G73E5P44"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-W9G73E5P44');
</script>
          <link rel="icon" 
                type="image/png" 
                href="/docs/assets/images/dp_icon.png">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Model comparison | Data Perspectives</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Model comparison" />
<meta name="author" content="Data-perspectives" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How to choose between models" />
<meta property="og:description" content="How to choose between models" />
<link rel="canonical" href="http://localhost:4000/statistics/model_averaging" />
<meta property="og:url" content="http://localhost:4000/statistics/model_averaging" />
<meta property="og:site_name" content="Data Perspectives" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-01-23T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Model comparison" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Data-perspectives"},"dateModified":"2024-01-23T00:00:00+00:00","datePublished":"2024-01-23T00:00:00+00:00","description":"How to choose between models","headline":"Model comparison","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/statistics/model_averaging"},"url":"http://localhost:4000/statistics/model_averaging"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Perspectives" />


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
<a rel="me" href="https://vis.social/@thestippe" style="display: none;">Mastodon</a>
<link rel="manifest" href="manifest.json">
</head>
<body>

        <div id='upperBar'><header class="site-header">

        <div id='upperBarr'>
        <script src="https://d3js.org/d3.v7.js"></script>
                <div class="wrapper" style="display:flex;"><ul hidden='hidden' id="postList"><li>
                        How does MCMC works;/statistics/mcmc_intro;1
                </li><li>
                        Introduction to bayesian statistics;/statistics/bayes_intro;2
                </li><li>
                        The autoregressive model;/statistics/autoregressive;3
                </li><li>
                        Introduction to time series modelling;/statistics/time_series;4
                </li><li>
                        Synthetic control;/statistics/synthetic_control;5
                </li><li>
                        Regression discontinuity ;/statistics/discontinuity_regression;6
                </li><li>
                        Difference in difference;/statistics/difference_in_differences;7
                </li><li>
                        Instrumental variable regression;/statistics/instrumental_variable;8
                </li><li>
                        Randomized controlled trials;/statistics/randomized;9
                </li><li>
                        Causal inference;/statistics/causal_intro;10
                </li><li>
                        Random models and mixed models;/statistics/random_models;11
                </li><li>
                        Introduction to Extreme Values theory;/statistics/extreme_intro;12
                </li><li>
                        Application of survival analysis 1;/statistics/survival_example;13
                </li><li>
                        Introduction to survival analysis;/statistics/survival_analysis;14
                </li><li>
                        Hierarchical models and meta-analysis;/statistics/hierarchical_metaanalysis;15
                </li><li>
                        Hierarchical models;/statistics/hierarchical_models;16
                </li><li>
                        Poisson regression;/statistics/poisson_regression;17
                </li><li>
                        Logistic regression;/statistics/logistic_regression;18
                </li><li>
                        Robust linear regression;/statistics/robust_regression;19
                </li><li>
                        Introduction to the linear regression;/statistics/regression;20
                </li><li>
                        Model comparison;/statistics/model_averaging;21
                </li><li>
                        Predictive checks;/statistics/predictive_checks;22
                </li><li>
                        Trace inspection;/statistics/trace_inspection;23
                </li><li>
                        Introduction to the Bayesian workflow;/statistics/bayesian_workflow;24
                </li><li>
                        Mixture models;/statistics/mixture;25
                </li><li>
                        Multidimensional distributions;/statistics/categories;26
                </li><li>
                        Exponential model, gaussian model and their evolutions;/statistics/reals;27
                </li><li>
                        The Negative Binomial model;/statistics/negbin;28
                </li><li>
                        The Poisson model;/statistics/poisson;29
                </li><li>
                        The Beta-Binomial model;/statistics/betabin;30
                </li><li>
                        Common continuous probabilities;/statistics/common_continuous_probabilities;31
                </li><li>
                        Common discrete probabilities;/statistics/common_discrete_probabilities;32
                </li><li>
                        Design tricks;/dataviz/design-introduction;33
                </li><li>
                        How to choose a color map;/dataviz/palettes-introduction;34
                </li><li>
                        Introduction to color perception;/dataviz/color-introduction;35
                </li><li>
                        Drawing is redrawing;/dataviz/gender-economist;36
                </li><li>
                        Visual queries;/dataviz/visual-queries;37
                </li><li>
                        The Gestalt principles;/dataviz/gestalt;38
                </li><li>
                        Channel effectiveness;/dataviz/effectiveness;39
                </li><li>
                        Evolutions of the line chart;/dataviz/linechart-evolution;40
                </li><li>
                        Beyond the 1D scatterplot;/dataviz/scatterplot-evolution;41
                </li><li>
                        Perception;/dataviz/perception;42
                </li><li>
                        Fundamental charts;/dataviz/fundamental-charts;43
                </li><li>
                        Marks and channels;/dataviz/marks-channels;44
                </li><li>
                        Data abstraction;/dataviz/data-types;45
                </li><li>
                        Data visualization;/dataviz/dataviz;46
                </li></ul>
                        <div style="display:flex">
                  <a href="/statistics/predictive_checks" class="prev">&#8249;</a>
                  
                                <a href="/"><img class="site-masthead" src="/docs/assets/images/logo_dp.png" alt="Data Perspectives" id="logo" /></a><div id='searchNav' style="flex;">
                                        <input type="search" id="search_0" class="searchBar" onkeydown="searchText()" placeholder="Search">
                                </div>

                                <div hidden='hidden' id="search_focus">0</div>



                        </div><nav class="site-nav" style="display:flex;">
                                <input type="checkbox" id="nav-trigger" class="nav-trigger" />
                                <label for="nav-trigger">
                                        <span class="menu-icon">
                                                <svg viewBox="0 0 18 15" width="18px" height="15px">
                                                        <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
                                                </svg>
                                        </span>
                                </label>

                                <div class="trigger"><a  class="page-link" href="/about">About me</a><a  class="page-link" href="/links">Resources</a>
                                <a href="/statistics/" class="page-link">Up</a>
                                
                                </div>
                        </nav>
                  <a href="/statistics/regression" class="next">&#8250;</a>
                  
        </div>


        <script src="/docs/assets/javascript/search.js">
        </script>
                </div>

</header>
<div class="progress-container">
    <div class="progress-bar" id="myBar"></div>
    </div>
        </div>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
          <div id='topPage'></div>
        <a href='/index'>
<img src="/docs/assets/images/background_resized.webp" alt="backround" style="margin:auto;display:block;width:1200px">

</a>
    <h1 class="post-title p-name" itemprop="name headline">Model comparison</h1>
    <p class="post-meta"><time class="dt-published" datetime="2024-01-23T00:00:00+00:00" itemprop="datePublished">
        Jan 23, 2024
      </time></p>
    <p class="post-meta"> Reading time: <span class="reading-time" title="Estimated read time">
  
  8&prime;
</span>
</p>
  </header>
  <br>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Most of the times, you won’t deal with a single model
for one dataset, but you will try many models
at the same time.</p>

<p>In this phase of the Bayesian workflow
we will discuss some methods to compare
models.</p>

<p>Comparing model sometimes may be understood as choosing the best model, but in most cases it means to asses which model is better to describe or predict some particular aspect of your data.
Model comparison can be done analytically in some case, but most of the time it will be done numerically or graphically, and here we will give an overview of the most important tools.</p>

<p>Here we will take a look at two of the most important
methods, the Bayes factor analysis and the
Leave One Out cross-validation.</p>

<h2 id="bayes-factors">Bayes factors</h2>

<p>Let us go back to the Beta-Binomial model
that we discussed in <a href="/betabin">this post</a>,
and let us assume that we have two candidate models to describe our data:
model 0 has Jeffreys prior, which mean that the prior
is a beta distribution with $\alpha=1/2$ and $\beta=1/2\,.$
The second model, named “model 2”, is instead centered in $0.5$ and has
\(\alpha = \beta = 10\,.\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="n">y</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">4320</span>

<span class="n">model_0</span> <span class="o">=</span> <span class="p">{</span><span class="s">'a'</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="s">'b'</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">}</span>
<span class="n">model_1</span> <span class="o">=</span> <span class="p">{</span><span class="s">'a'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s">'b'</span><span class="p">:</span> <span class="mi">10</span><span class="p">}</span>

<span class="n">x_pl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">model_0</span><span class="p">,</span> <span class="n">model_1</span><span class="p">]):</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">beta</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">model</span><span class="p">[</span><span class="s">'a'</span><span class="p">],</span> <span class="n">b</span><span class="o">=</span><span class="n">model</span><span class="p">[</span><span class="s">'b'</span><span class="p">]).</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_pl</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s">"model </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="n">legend</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">"$\theta$"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">"$p(\theta)$  "</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/model_averaging/priors.webp" alt="The priors used in this post" /></p>

<p>Given the two models $M_0$ and $M_1$ we may ask which one we prefer, given the data. The probability of the model given the data is given by</p>

\[p(M_k | y) = \frac{p(y | M_k)}{p(y)} p(M_k)\]

<p>where the quantity</p>

\[p(y | M_k)\]

<p>is the <strong>marginal likelihood</strong> of the model.</p>

<p>If we assign the same prior probability $p(M_k)\,,$
to each model,
since $p(y)$ is the same for both models,
then we can simply replace $p(M_k | y)$ with the
marginal likelihood.</p>

<p>As usual, an analytic calculation is only possible in a very limited number of models.</p>

<p>One may think to compute $p(M_k| y)$ by starting from $p(y | \theta, M_k)$ and integrating out $\theta$ but doing this naively is generally not a good idea, as
this method is unstable and prone to numerical errors.</p>

<p>However can use the Sequential Monte Carlo to compare the two models, since it allows to estimate the (log) marginal likelihood of the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">traces</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="p">[</span><span class="n">model_0</span><span class="p">,</span> <span class="n">model_1</span><span class="p">]:</span>
    <span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Beta</span><span class="p">(</span><span class="s">"theta"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">m</span><span class="p">[</span><span class="s">'a'</span><span class="p">],</span> <span class="n">beta</span><span class="o">=</span><span class="n">m</span><span class="p">[</span><span class="s">'b'</span><span class="p">])</span>
        <span class="n">yl</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">"yl"</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
        <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_smc</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">return_inferencedata</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>
        <span class="n">models</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">traces</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<p>Let us inspect as usual the traces.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging/trace_0.webp" alt="The trace for model 0" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging/trace_1.webp" alt="The trace for model 1" /></p>

<p>What one usually computes is the <strong>Bayes factor</strong> of the models, which is the ratio between the posterior probability of the model (which in this case is simply the
ratio between the marginal likelihoods).</p>

<table>
  <thead>
    <tr>
      <th>$BF = p(M_0)/p(M_1)$</th>
      <th>interpretation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$BF&lt;10^{0}$</td>
      <td>support to $M_1$ (see reciprocal)</td>
    </tr>
    <tr>
      <td>$10^{0}\leq BF&lt;10^{1/2}$</td>
      <td>Barely worth mentioning support to $M_0$</td>
    </tr>
    <tr>
      <td>$10^{1/2}\leq BF&lt;10^2$</td>
      <td>Substantial support to $M_0$</td>
    </tr>
    <tr>
      <td>$10^{2} \leq BF&lt;10^{3/2}$</td>
      <td>Strong support to $M_0$</td>
    </tr>
    <tr>
      <td>$10^{3/2} \leq BF&lt;10^2$</td>
      <td>Very strong support to $M_0$</td>
    </tr>
    <tr>
      <td>$\geq 10^2$</td>
      <td>Decisive support to $M_0$</td>
    </tr>
  </tbody>
</table>

<p>This can be easily done as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span>
    <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">sample_stats</span><span class="p">.</span><span class="n">log_marginal_likelihood</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
    <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">sample_stats</span><span class="p">.</span><span class="n">log_marginal_likelihood</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span><span class="p">)))</span>
</code></pre></div></div>

<div class="code">
17.29
</div>

<p>As we can see, there is a substantial preference
for model 0.
We can better understand this result if we compare our estimate with the
frequentist confidence interval,
which we recall being \([0.0004, 0.0028]\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">traces</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">rope</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0004</span><span class="p">,</span> <span class="mf">0.0028</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/model_averaging/forest.webp" alt="The forest plot of the two models" /></p>

<p>We can see that the preferred model HDI corresponds
with the frequentist CI, while the interval 
predicted by the second model only partially
overlaps with the frequentist CI.</p>

<p>We can also inspect the posterior predictive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ppc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">models</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">m</span><span class="p">:</span>
        <span class="n">ppc</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ppc</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yl'</span><span class="p">].</span><span class="n">mean</span><span class="p">([</span><span class="s">'chain'</span><span class="p">,</span> <span class="s">'draw'</span><span class="p">]).</span><span class="n">values</span>
</code></pre></div></div>
<div class="code">
array(7.5875)
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ppc</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yl'</span><span class="p">].</span><span class="n">mean</span><span class="p">([</span><span class="s">'chain'</span><span class="p">,</span> <span class="s">'draw'</span><span class="p">]).</span><span class="n">values</span>
</code></pre></div></div>
<div class="code">
array(17.02425)
</div>

<p>We recall that the observed value for $y$ was 7,
which is much closer to the one provided by the preferred
model than to the one provided by Model 1.</p>

<h2 id="leave-one-out">Leave One Out</h2>

<p>The second method that we will see is the Leave One Out (LOO) cross validation.
This method is generally preferred to the above one, as it has been pointed out
that Bayes factors are appropriate only when one of the models is true,
while in real world problems we don’t have any certainty about which is the model that
generated the data, assuming that it makes sense to claim that it exists such a model.
Moreover, the sampler used to compute the Bayes factor, namely Sequential Monte Carlo,
is generally less stable than the standard one used by PyMC, which is the NUTS sampler.
There are other, more philosophical reasons, pointed out by Gelman in <a href="https://statmodeling.stat.columbia.edu/2017/07/21/bayes-factor-term-came-references-generally-hate/">this post</a>,
but for now we won’t dig into this kind of discussion.</p>

<p>The LOO method is much more in the spirit of the Machine Learning, where
one splits the sample into a training set and a test set.
The train set is used to find the parameters, while the second one is
used to assess the performances of the model for new data.
This method, namely the <strong>cross validation</strong>, is by far the most
reliable one, and we generally recommend to use it.
It is however very common that the dataset is too small to allow
a full cross-validation.
The LOO cross validation is equivalent to the computation of</p>

\[ELPD = \sum_i \log p_{-i}(y_i)\]

<p>where \(p_{-i}(y_i)\) is the posterior predictive probability
of the point \(y_i\) relative to the model fitted by removing \(y_i\,.\)</p>

<p>Since we already discussed how to implement this method in the post on the 
<a href="/statistics/negbin">negative binomial model</a>, and since it doesn’t make much sense
to check what happens by removing one point out of four thousands,
we won’t repeat it again.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We verified two of the main methods to make model comparison.
We will see other more advanced methods in the future, but for now 
you should keep in mind that generally the LOO is the preferred one.</p>

  </div><a class="u-url" href="/statistics/model_averaging" hidden></a>

  <div>
          
          
  </div>

  <br>
  <div id='autograph'>
          Stippe Jan 23, 2024

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>


  <div class="wrapper">
          <div class='footer-text'>

                  No AI were harmed in creating this blog.
                  <br>
                  This is a blog, not a bakery: there are no cookies here!
                  <br>
                  The top image has been generated with a modified version Dan Gries' code, available at <a href="http://rectangleworld.com/blog/archives/538">http://rectangleworld.com</a>.
          </div>
          <br>

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Data-perspectives</li>
          <li><a class="u-email" href="mailto:dataperspectivesblog@gmail.com">dataperspectivesblog@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://twitter.com/SteffPy" target="_blank" title="twitter">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://stackoverflow.com/users/11065831/stefano" target="_blank" title="stackoverflow">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#stackoverflow"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://github.com/thestippe/" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://vis.social/@thestippe" target="_blank" title="mastodon">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#mastodon"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

  <script src="/docs/assets/javascript/scroll.js">
  </script>

</html>

