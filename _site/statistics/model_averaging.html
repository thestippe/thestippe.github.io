<!DOCTYPE html>
<html lang="en"><head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-W9G73E5P44"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-W9G73E5P44');
</script>
          <link rel="icon" 
                type="image/png" 
                href="/docs/assets/images/dp_icon.png">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Model comparison | Data Perspectives</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Model comparison" />
<meta name="author" content="Data-perspectives" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How to choose between models" />
<meta property="og:description" content="How to choose between models" />
<link rel="canonical" href="http://localhost:4000/statistics/model_averaging" />
<meta property="og:url" content="http://localhost:4000/statistics/model_averaging" />
<meta property="og:site_name" content="Data Perspectives" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-09T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Model comparison" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Data-perspectives"},"dateModified":"2024-10-09T00:00:00+00:00","datePublished":"2024-10-09T00:00:00+00:00","description":"How to choose between models","headline":"Model comparison","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/statistics/model_averaging"},"url":"http://localhost:4000/statistics/model_averaging"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Perspectives" />


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
<a rel="me" href="https://vis.social/@thestippe" style="display: none;">Mastodon</a>
<link rel="manifest" href="manifest.json">
</head>
<body>

        <div id='upperBar'><header class="site-header">

        <div id='upperBarr'>
        <script src="https://d3js.org/d3.v7.js"></script>
                <div class="wrapper" style="display:flex;">

                
                
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                    
                    
                    
                    
                    

        <ul hidden='hidden' id="postList"><li>
                        Fitting complex models;/statistics/complex_models;1
                </li><li>
                        Directional statistics;/statistics/directional;2
                </li><li>
                        Horseshoe priors;/statistics/horseshoe;3
                </li><li>
                        MRP;/statistics/mrp;4
                </li><li>
                        Application of the Lotka-Volterra model;/statistics/lotka_volterra;5
                </li><li>
                        Differential equations;/statistics/ode;6
                </li><li>
                        Dirichlet Process Mixture Models;/statistics/dp;7
                </li><li>
                        Bayesian Additive Regression Trees;/statistics/bart;8
                </li><li>
                        Splines;/statistics/spline;9
                </li><li>
                        Gaussian processes regression;/statistics/gp_example;10
                </li><li>
                        Gaussian processes;/statistics/gp;11
                </li><li>
                        Nonparametric models;/statistics/nonparametric_intro;12
                </li><li>
                        Stochastic volatility models;/statistics/stochastic_volatility;13
                </li><li>
                        Time series;/statistics/time_series;14
                </li><li>
                        Structural time series;/statistics/structural_time_series;15
                </li><li>
                        Quantile regression;/statistics/extreme_quantile;16
                </li><li>
                        Introduction to Extreme Values theory;/statistics/extreme_intro;17
                </li><li>
                        Accelerated Failure Time models;/statistics/survival_example_aft;18
                </li><li>
                        Application of survival analysis with discrete times;/statistics/survival_example_2;19
                </li><li>
                        Application of survival analysis 1;/statistics/survival_example;20
                </li><li>
                        Introduction to survival analysis;/statistics/survival_analysis;21
                </li><li>
                        Regression discontinuity design;/statistics/rdd;22
                </li><li>
                        Difference in difference;/statistics/difference_in_differences;23
                </li><li>
                        Instrumental variable regression;/statistics/instrumental_variable;24
                </li><li>
                        Randomized controlled trials;/statistics/randomized;25
                </li><li>
                        Causal inference and Bayesian networks;/statistics/causal_intro_2;26
                </li><li>
                        Causal inference;/statistics/causal_intro;27
                </li><li>
                        Nested factor;/statistics/nested_factors;28
                </li><li>
                        Repeated measures;/statistics/repeated_measures;29
                </li><li>
                        Split plot design;/statistics/split_plot;30
                </li><li>
                        Crossover design;/statistics/crossover;31
                </li><li>
                        Latin square design;/statistics/latin_square;32
                </li><li>
                        Full factorial design;/statistics/full_factorial;33
                </li><li>
                        Completely randomized design;/statistics/crd;34
                </li><li>
                        Design of experiments;/statistics/doe;35
                </li><li>
                        Validity;/statistics/validity;36
                </li><li>
                        Stratification;/statistics/stratification;37
                </li><li>
                        Random sampling;/statistics/random_sampling;38
                </li><li>
                        Data collection;/statistics/data_collection;39
                </li><li>
                        Things that could go wrong;/statistics/problem_solving_issues;40
                </li><li>
                        The problem solving workflow;/statistics/problem_solving;41
                </li><li>
                        Time_series;/time_series;42
                </li><li>
                        Survival_example_2;/survival_example_2;43
                </li><li>
                        Survival_example;/survival_example;44
                </li><li>
                        Survival_analysis;/survival_analysis;45
                </li><li>
                        Structural_time_series;/structural_time_series;46
                </li><li>
                        Spline;/spline;47
                </li><li>
                        Ode;/ode;48
                </li><li>
                        Nonparametric_intro;/nonparametric_intro;49
                </li><li>
                        Mrp;/mrp;50
                </li><li>
                        Lotka_volterra;/lotka_volterra;51
                </li><li>
                        Horseshoe;/horseshoe;52
                </li><li>
                        Gp_example;/gp_example;53
                </li><li>
                        Gp;/gp;54
                </li><li>
                        Extreme_intro;/extreme_intro;55
                </li><li>
                        Dp;/dp;56
                </li><li>
                        Bart;/bart;57
                </li><li>
                        Mixed effects models with more than two levels;/statistics/three_levels;58
                </li><li>
                        Leveraging mixed-effect models;/statistics/bambi_multilevel;59
                </li><li>
                        Rdd;/rdd;60
                </li><li>
                        Difference_in_differences;/difference_in_differences;61
                </li><li>
                        Instrumental_variable;/instrumental_variable;62
                </li><li>
                        Randomized;/randomized;63
                </li><li>
                        Causal_intro_2;/causal_intro_2;64
                </li><li>
                        Causal_intro;/causal_intro;65
                </li><li>
                        Nested_factors;/nested_factors;66
                </li><li>
                        Repeated_measures;/repeated_measures;67
                </li><li>
                        Split_plot;/split_plot;68
                </li><li>
                        Crossover Design;/crossover-design;69
                </li><li>
                        Latin_square;/latin_square;70
                </li><li>
                        Full_factorial;/full_factorial;71
                </li><li>
                        Crd;/crd;72
                </li><li>
                        Doe;/doe;73
                </li><li>
                        Validity;/validity;74
                </li><li>
                        Random_sampling;/random_sampling;75
                </li><li>
                        Data_collection;/data_collection;76
                </li><li>
                        Problem_solving_issues;/problem_solving_issues;77
                </li><li>
                        Problem_solving;/problem_solving;78
                </li><li>
                        Three_levels;/three_levels;79
                </li><li>
                        Bambi_multilevel;/bambi_multilevel;80
                </li><li>
                        Random models and mixed models;/statistics/random_models;81
                </li><li>
                        Radar interferometry with Pygmtsar;/gis/pygmtsar;82
                </li><li>
                        Hierarchical models and meta-analysis;/statistics/hierarchical_metaanalysis;83
                </li><li>
                        OpenEO for SAR images;/gis/openeo_sar;84
                </li><li>
                        OpenEO 2: time series;/gis/openeo_ts;85
                </li><li>
                        Hierarchical models;/statistics/hierarchical_models;86
                </li><li>
                        OpenEO;/gis/openeo;87
                </li><li>
                        Poisson regression;/statistics/poisson_regression;88
                </li><li>
                        Open Street Map services;/gis/openstreetmap;89
                </li><li>
                        Logistic regression;/statistics/logistic_regression;90
                </li><li>
                        Open Web Consortium standards;/gis/owc_standards;91
                </li><li>
                        Robust linear regression;/statistics/robust_regression;92
                </li><li>
                        Map design;/gis/map_design;93
                </li><li>
                        Multi-linear regression;/statistics/multivariate_regression;94
                </li><li>
                        Operations on raster data;/gis/raster_ops;95
                </li><li>
                        Linear regression with binary input;/statistics/regression_binary_input;96
                </li><li>
                        Operations on vector data;/gis/vector_ops;97
                </li><li>
                        Introduction to the linear regression;/statistics/regression;98
                </li><li>
                        101 ways to reproject your data;/gis/pyproj;99
                </li><li>
                        Model comparison, cont.;/statistics/model_averaging_cont;100
                </li><li>
                        Model comparison;/statistics/model_averaging;101
                </li><li>
                        Raster data;/gis/raster_data;102
                </li><li>
                        Vector data;/gis/vector_data;103
                </li><li>
                        Choosing the right projection;/gis/projections;104
                </li><li>
                        Introduction to geographic data analysis;/gis/gis_intro;105
                </li><li>
                        Re-parametrizing your model;/statistics/reparametrization;106
                </li><li>
                        Predictive checks;/statistics/predictive_checks;107
                </li><li>
                        Trace inspection;/statistics/trace_inspection;108
                </li><li>
                        Introduction to the Bayesian workflow;/statistics/bayesian_workflow;109
                </li><li>
                        Mixture models;/statistics/mixture;110
                </li><li>
                        Multidimensional distributions;/statistics/categories;111
                </li><li>
                        The Gaussian model;/statistics/reals;112
                </li><li>
                        Bonus: counting animals in a park;/statistics/hypergeom;113
                </li><li>
                        The Negative Binomial model;/statistics/negbin;114
                </li><li>
                        The Poisson model;/statistics/poisson;115
                </li><li>
                        The Beta-Binomial model;/statistics/betabin;116
                </li><li>
                        Section introduction;/statistics/simple_models_intro;117
                </li><li>
                        Some notation about probability;/statistics/probability_reminder;118
                </li><li>
                        How does MCMC works;/statistics/mcmc_intro;119
                </li><li>
                        Introduction to Bayesian inference;/statistics/bayes_intro;120
                </li><li>
                        An overview to statistics;/statistics/preface;121
                </li><li>
                        The Gestalt principles;/dataviz/gestalt;122
                </li><li>
                        Design tricks;/dataviz/design-introduction;123
                </li><li>
                        How to choose a color map;/dataviz/palettes-introduction;124
                </li><li>
                        Introduction to color perception;/dataviz/color-introduction;125
                </li><li>
                        Drawing is redrawing;/dataviz/gender-economist;126
                </li><li>
                        Visual queries;/dataviz/visual-queries;127
                </li><li>
                        Channel effectiveness;/dataviz/effectiveness;128
                </li><li>
                        Evolutions of the line chart;/dataviz/linechart-evolution;129
                </li><li>
                        Beyond the 1D scatterplot;/dataviz/scatterplot-evolution;130
                </li><li>
                        Perception;/dataviz/perception;131
                </li><li>
                        Fundamental charts;/dataviz/fundamental-charts;132
                </li><li>
                        Marks and channels;/dataviz/marks-channels;133
                </li><li>
                        Data abstraction;/dataviz/data-types;134
                </li><li>
                        Data visualization;/dataviz/dataviz;135
                </li></ul>
                        <div style="display:flex">
                  <a href="/statistics/reparametrization" class="prev">&#8249;</a>
                  
                                <a href="/"><img class="site-masthead" src="/docs/assets/images/logo_dp.png" alt="Data Perspectives" id="logo" /></a><div id='searchNav' style="flex;">
                                        <input type="search" id="search_0" class="searchBar" onkeydown="searchText()" placeholder="Search">
                                </div>

                                <div hidden='hidden' id="search_focus">0</div>



                        </div><nav class="site-nav" style="display:flex;">
                                <input type="checkbox" id="nav-trigger" class="nav-trigger" />
                                <label for="nav-trigger">
                                        <span class="menu-icon">
                                                <svg viewBox="0 0 18 15" width="18px" height="15px">
                                                        <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
                                                </svg>
                                        </span>
                                </label>

                                <div class="trigger"><a  class="page-link" href="/about">About me</a><a  class="page-link" href="/links">Resources</a>
                                <a href="/statistics/" class="page-link">Up</a>
                                
                                </div>
                        </nav>
                  <a href="/statistics/model_averaging_cont" class="next">&#8250;</a>
                  
        </div>


        <script src="/docs/assets/javascript/search.js">
        </script>
                </div>

</header>
<div class="progress-container">
    <div class="progress-bar" id="myBar"></div>
    </div>
        </div>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
          <div id='topPage'></div>
        <a href='/index'>
<img src="/docs/assets/images/background_resized.webp" alt="backround" style="margin:auto;display:block;width:1200px">

</a>
    <h1 class="post-title p-name" itemprop="name headline">Model comparison</h1>
    <p class="post-meta"><time class="dt-published" datetime="2024-10-09T00:00:00+00:00" itemprop="datePublished">
        Oct 9, 2024
      </time></p>
    <p class="post-meta"> Reading time: <span class="reading-time" title="Estimated read time">
  
  7&prime;
</span>
</p>
  </header>
  <br>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In the majority of cases, you won’t deal with a single model
for one dataset, but you will try many models
at the same time.</p>

<p>In this phase of the Bayesian workflow
we will discuss some methods to compare
models.</p>

<p>Comparing model sometimes may be understood as choosing the best model, but in most cases it means to 
assess which model is better to describe or predict some particular aspect of your data.
Model comparison can be done analytically in some case, but most of the time it will be done numerically or graphically, and here we will give an overview of the most important tools.</p>

<p><br /></p>

<blockquote>
  <p>Since all models are wrong the scientist must be alert
to what is importantly wrong. It is inappropriate to be
concerned about mice when there are tigers abroad.</p>

  <p>George Box</p>
</blockquote>

<p><br /></p>

<p>Here we will take a look at two of the most important
methods, the Bayes factor analysis and the
Leave One Out cross-validation.</p>

<h2 id="bayes-factors">Bayes factors</h2>

<p>Let us go back to the Beta-Binomial model
that we discussed in <a href="/_posts/statistics/betabin.md">this post</a>,
and let us assume that we have two candidate models to describe our data:
model 0 has Jeffreys prior, which mean that the prior
is a beta distribution with $\alpha=1/2$ and $\beta=1/2\,.$
The second model, named “model 2”, is instead centered in $0.5$ and has
\(\alpha = \beta = 10\,.\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="n">y</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">4320</span>

<span class="n">model_0</span> <span class="o">=</span> <span class="p">{</span><span class="s">'a'</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="s">'b'</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">}</span>
<span class="n">model_1</span> <span class="o">=</span> <span class="p">{</span><span class="s">'a'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s">'b'</span><span class="p">:</span> <span class="mi">10</span><span class="p">}</span>

<span class="n">x_pl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">model_0</span><span class="p">,</span> <span class="n">model_1</span><span class="p">]):</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">beta</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">model</span><span class="p">[</span><span class="s">'a'</span><span class="p">],</span> <span class="n">b</span><span class="o">=</span><span class="n">model</span><span class="p">[</span><span class="s">'b'</span><span class="p">]).</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_pl</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s">"model </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="n">legend</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">"$\theta$"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">"$p(\theta)$  "</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/model_averaging/priors.webp" alt="The priors used in this post" /></p>

<p>Given the two models $M_0$ and $M_1$ we may ask which one we prefer, given the data. The probability of the model given the data is given by</p>

\[p(M_k | y) = \frac{p(y | M_k)}{p(y)} p(M_k)\]

<p>where the quantity</p>

\[p(y | M_k)\]

<p>is the <strong>marginal likelihood</strong> of the model.</p>

<p>If we assign the same prior probability $p(M_k)\,,$
to each model,
since $p(y)$ is the same for both models,
then we can simply replace $p(M_k | y)$ with the
marginal likelihood.</p>

<p>As usual, an analytic calculation is only possible in a very limited number of models.</p>

<p>One may think to compute $p(M_k| y)$ by starting from $p(y | \theta, M_k)$ and integrating out $\theta$ but doing this naively is generally not a good idea, as
this method is unstable and prone to numerical errors.</p>

<p>We can however use the Sequential Monte Carlo to compare the two models,
since it allows to estimate the (log) marginal likelihood of the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">traces</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="p">[</span><span class="n">model_0</span><span class="p">,</span> <span class="n">model_1</span><span class="p">]:</span>
    <span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Beta</span><span class="p">(</span><span class="s">"theta"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">m</span><span class="p">[</span><span class="s">'a'</span><span class="p">],</span> <span class="n">beta</span><span class="o">=</span><span class="n">m</span><span class="p">[</span><span class="s">'b'</span><span class="p">])</span>
        <span class="n">yl</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">"yl"</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
        <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_smc</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">return_inferencedata</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>
        <span class="n">models</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">traces</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<p>Let us inspect as usual the traces.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging/trace_0.webp" alt="The trace for model 0" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging/trace_1.webp" alt="The trace for model 1" /></p>

<p>What one usually computes is the <strong>Bayes factor</strong> of the models, which is the ratio between the posterior probability of the model (which in this case is simply the
ratio between the marginal likelihoods).</p>

<table>
  <thead>
    <tr>
      <th>$BF = p(M_0)/p(M_1)$</th>
      <th>interpretation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$BF&lt;10^{0}$</td>
      <td>support to $M_1$ (see reciprocal)</td>
    </tr>
    <tr>
      <td>$10^{0}\leq BF&lt;10^{1/2}$</td>
      <td>Barely worth mentioning support to $M_0$</td>
    </tr>
    <tr>
      <td>$10^{1/2}\leq BF&lt;10^2$</td>
      <td>Substantial support to $M_0$</td>
    </tr>
    <tr>
      <td>$10^{2} \leq BF&lt;10^{3/2}$</td>
      <td>Strong support to $M_0$</td>
    </tr>
    <tr>
      <td>$10^{3/2} \leq BF&lt;10^2$</td>
      <td>Very strong support to $M_0$</td>
    </tr>
    <tr>
      <td>$\geq 10^2$</td>
      <td>Decisive support to $M_0$</td>
    </tr>
  </tbody>
</table>

<p>This can be easily done as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">log10</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span>
    <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">sample_stats</span><span class="p">.</span><span class="n">log_marginal_likelihood</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
    <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">sample_stats</span><span class="p">.</span><span class="n">log_marginal_likelihood</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span><span class="p">)))</span>
</code></pre></div></div>

<div class="code">
18.175240388473817
</div>

<p>As we can see, there is a substantial preference
for model 0.
We can better understand this result if we compare our estimate with the
frequentist confidence interval,
which we recall being \([0.0004, 0.0028]\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">traces</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">rope</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0004</span><span class="p">,</span> <span class="mf">0.0028</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/model_averaging/forest.webp" alt="The forest plot of the two models" /></p>

<p>We can see that the preferred model HDI corresponds
with the frequentist CI, while the interval 
predicted by the second model only partially
overlaps with the frequentist CI.</p>

<p>We can also inspect the posterior predictive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ppc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">models</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">m</span><span class="p">:</span>
        <span class="n">ppc</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">traces</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ppc</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yl'</span><span class="p">].</span><span class="n">mean</span><span class="p">([</span><span class="s">'chain'</span><span class="p">,</span> <span class="s">'draw'</span><span class="p">]).</span><span class="n">values</span>
</code></pre></div></div>
<div class="code">
array(7.445)
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ppc</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yl'</span><span class="p">].</span><span class="n">mean</span><span class="p">([</span><span class="s">'chain'</span><span class="p">,</span> <span class="s">'draw'</span><span class="p">]).</span><span class="n">values</span>
</code></pre></div></div>
<div class="code">
array(16.98925)
</div>

<p>We recall that the observed value for $y$ was 7,
which is much closer to the one provided by the preferred
model than to the one provided by Model 1.</p>

<h2 id="conclusions">Conclusions</h2>

<p>In this post we discussed the Bayes factor to choose between different models.
In the next post, we will discuss a more powerful method to compare models,
namely the Leave One Out cross validation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">pytensor</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>
<div class="code">
Last updated: Wed Nov 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.7
<br />
IPython version      : 8.24.0
<br />

<br />
xarray  : 2024.9.0
<br />
pytensor: 2.25.5
<br />
numpyro : 0.15.0
<br />
jax     : 0.4.28
<br />
jaxlib  : 0.4.28
<br />

<br />
pymc      : 5.17.0
<br />
numpy     : 1.26.4
<br />
matplotlib: 3.9.2
<br />
pandas    : 2.2.3
<br />
arviz     : 0.20.0
<br />

<br />
Watermark: 2.4.3
<br />

<br />
</div>

  </div><a class="u-url" href="/statistics/model_averaging" hidden></a>

  <br>
  <div id='autograph'>
          Stippe Oct 9, 2024

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>


  <div class="wrapper">
          <div class='footer-text'>
                  Opinions are mine, mistakes too, and if you find any feel free to report it via mail or via <del>Twitter</del> <a href="https://bsky.app/profile/stippe87.bsky.social">Bluesky</a>
                  <br>
                  Most of the material in the statistics section is an adaptation to Python of some pre-existing model.
                  <br>
                  I have tried to provide the necessary credits, but if you think that a relevant contribution is missing, please let me know.
                  <br>
                  No AI were harmed in creating this blog.
                  <br>
                  This is a blog, not a bakery: there are no cookies here!
                  <br>
                  The top image has been generated with a modified version Dan Gries' code, available at <a href="http://rectangleworld.com/blog/archives/538">http://rectangleworld.com</a>.
          </div>
          <br>

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Data-perspectives</li>
          <li><a class="u-email" href="mailto:dataperspectivesblog@gmail.com">dataperspectivesblog@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://stackoverflow.com/users/11065831/stefano" target="_blank" title="stackoverflow">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#stackoverflow"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://github.com/thestippe/" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://vis.social/@thestippe" target="_blank" title="mastodon">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#mastodon"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

  <script src="/docs/assets/javascript/scroll.js">
  </script>

</html>

