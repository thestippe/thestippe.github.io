<!DOCTYPE html>
<html lang="en"><head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-W9G73E5P44"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-W9G73E5P44');
</script>
          <link rel="icon" 
                type="image/png" 
                href="/docs/assets/images/dp_icon.png">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Multidimensional distributions | Data Perspectives</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Multidimensional distributions" />
<meta name="author" content="Data-perspectives" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Dealing with more than two categories" />
<meta property="og:description" content="Dealing with more than two categories" />
<link rel="canonical" href="http://localhost:4000/statistics/categories" />
<meta property="og:url" content="http://localhost:4000/statistics/categories" />
<meta property="og:site_name" content="Data Perspectives" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-01-07T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Multidimensional distributions" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Data-perspectives"},"dateModified":"2024-01-07T00:00:00+00:00","datePublished":"2024-01-07T00:00:00+00:00","description":"Dealing with more than two categories","headline":"Multidimensional distributions","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/statistics/categories"},"url":"http://localhost:4000/statistics/categories"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Perspectives" />


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
<a rel="me" href="https://vis.social/@thestippe" style="display: none;">Mastodon</a>
<link rel="manifest" href="manifest.json">
</head>
<body>

        <div id='upperBar'><header class="site-header">

        <div id='upperBarr'>
        <script src="https://d3js.org/d3.v7.js"></script>
                <div class="wrapper" style="display:flex;"><ul hidden='hidden' id="postList"><li>
                        Application of the Lotka-Volterra model;/statistics/lotka_volterra;1
                </li><li>
                        Differential equations;/statistics/ode;2
                </li><li>
                        MRP;/statistics/mrp;3
                </li><li>
                        Dirichlet Process Mixture Models;/statistics/dp;4
                </li><li>
                        Bayesian Additive Regression Trees;/statistics/bart;5
                </li><li>
                        Splines;/statistics/spline;6
                </li><li>
                        Gaussian processes regression;/statistics/gp_example;7
                </li><li>
                        Gaussian processes;/statistics/gp;8
                </li><li>
                        Nonparametric models;/statistics/nonparametric_intro;9
                </li><li>
                        Synthetic control;/statistics/synthetic_control;10
                </li><li>
                        Regression discontinuity design;/statistics/rdd;11
                </li><li>
                        Difference in difference;/statistics/difference_in_differences;12
                </li><li>
                        Instrumental variable regression;/statistics/instrumental_variable;13
                </li><li>
                        Randomized controlled trials;/statistics/randomized;14
                </li><li>
                        Causal inference;/statistics/causal_intro;15
                </li><li>
                        Experiment analysis with many blocking variables;/statistics/experiment_design_cont;16
                </li><li>
                        Experiment analysis;/statistics/experiment_design;17
                </li><li>
                        Introduction to Extreme Values theory;/statistics/extreme_intro;18
                </li><li>
                        Application of survival analysis with discrete times;/statistics/survival_example_2;19
                </li><li>
                        Application of survival analysis 1;/statistics/survival_example;20
                </li><li>
                        Introduction to survival analysis;/statistics/survival_analysis;21
                </li><li>
                        Random models and mixed models;/statistics/random_models;22
                </li><li>
                        Hierarchical models and meta-analysis;/statistics/hierarchical_metaanalysis;23
                </li><li>
                        Hierarchical models;/statistics/hierarchical_models;24
                </li><li>
                        Poisson regression;/statistics/poisson_regression;25
                </li><li>
                        Logistic regression;/statistics/logistic_regression;26
                </li><li>
                        Robust linear regression;/statistics/robust_regression;27
                </li><li>
                        Multi-linear regression;/statistics/multivariate_regression;28
                </li><li>
                        Linear regression with binary input;/statistics/regression_binary_input;29
                </li><li>
                        Introduction to the linear regression;/statistics/regression;30
                </li><li>
                        Model comparison, cont.;/statistics/model_averaging_cont;31
                </li><li>
                        Model comparison;/statistics/model_averaging;32
                </li><li>
                        Reparametrizing your model;/statistics/reparametrization;33
                </li><li>
                        Predictive checks;/statistics/predictive_checks;34
                </li><li>
                        Trace inspection;/statistics/trace_inspection;35
                </li><li>
                        Introduction to the Bayesian workflow;/statistics/bayesian_workflow;36
                </li><li>
                        Mixture models;/statistics/mixture;37
                </li><li>
                        Multidimensional distributions;/statistics/categories;38
                </li><li>
                        The Gaussian model;/statistics/reals;39
                </li><li>
                        The Negative Binomial model;/statistics/negbin;40
                </li><li>
                        The Poisson model;/statistics/poisson;41
                </li><li>
                        The Beta-Binomial model;/statistics/betabin;42
                </li><li>
                        Section introduction;/statistics/simple_models_intro;43
                </li><li>
                        Some notation about probability;/statistics/probability_reminder;44
                </li><li>
                        How does MCMC works;/statistics/mcmc_intro;45
                </li><li>
                        Introduction to Bayesian inference;/statistics/bayes_intro;46
                </li><li>
                        An overview to statistics;/statistics/preface;47
                </li><li>
                        The Gestalt principles;/dataviz/gestalt;48
                </li><li>
                        Design tricks;/dataviz/design-introduction;49
                </li><li>
                        How to choose a color map;/dataviz/palettes-introduction;50
                </li><li>
                        Introduction to color perception;/dataviz/color-introduction;51
                </li><li>
                        Drawing is redrawing;/dataviz/gender-economist;52
                </li><li>
                        Visual queries;/dataviz/visual-queries;53
                </li><li>
                        Channel effectiveness;/dataviz/effectiveness;54
                </li><li>
                        Evolutions of the line chart;/dataviz/linechart-evolution;55
                </li><li>
                        Beyond the 1D scatterplot;/dataviz/scatterplot-evolution;56
                </li><li>
                        Perception;/dataviz/perception;57
                </li><li>
                        Fundamental charts;/dataviz/fundamental-charts;58
                </li><li>
                        Marks and channels;/dataviz/marks-channels;59
                </li><li>
                        Data abstraction;/dataviz/data-types;60
                </li><li>
                        Data visualization;/dataviz/dataviz;61
                </li></ul>
                        <div style="display:flex">
                  <a href="/statistics/reals" class="prev">&#8249;</a>
                  
                                <a href="/"><img class="site-masthead" src="/docs/assets/images/logo_dp.png" alt="Data Perspectives" id="logo" /></a><div id='searchNav' style="flex;">
                                        <input type="search" id="search_0" class="searchBar" onkeydown="searchText()" placeholder="Search">
                                </div>

                                <div hidden='hidden' id="search_focus">0</div>



                        </div><nav class="site-nav" style="display:flex;">
                                <input type="checkbox" id="nav-trigger" class="nav-trigger" />
                                <label for="nav-trigger">
                                        <span class="menu-icon">
                                                <svg viewBox="0 0 18 15" width="18px" height="15px">
                                                        <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
                                                </svg>
                                        </span>
                                </label>

                                <div class="trigger"><a  class="page-link" href="/about">About me</a><a  class="page-link" href="/links">Resources</a>
                                <a href="/statistics/" class="page-link">Up</a>
                                
                                </div>
                        </nav>
                  <a href="/statistics/mixture" class="next">&#8250;</a>
                  
        </div>


        <script src="/docs/assets/javascript/search.js">
        </script>
                </div>

</header>
<div class="progress-container">
    <div class="progress-bar" id="myBar"></div>
    </div>
        </div>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
          <div id='topPage'></div>
        <a href='/index'>
<img src="/docs/assets/images/background_resized.webp" alt="backround" style="margin:auto;display:block;width:1200px">

</a>
    <h1 class="post-title p-name" itemprop="name headline">Multidimensional distributions</h1>
    <p class="post-meta"><time class="dt-published" datetime="2024-01-07T00:00:00+00:00" itemprop="datePublished">
        Jan 7, 2024
      </time></p>
    <p class="post-meta"> Reading time: <span class="reading-time" title="Estimated read time">
  
  10&prime;
</span>
</p>
  </header>
  <br>

  <div class="post-content e-content" itemprop="articleBody">
    <p>We will now discuss some multidimensional generalization of the previously introduced
distributions.</p>

<h2 id="the-multinomial-model">The multinomial model</h2>

<p>The categorical model can be seen as a generalization of the Bernoulli model,
and if the binomial model is the sum of $n$ Bernoulli trials,
the multinomial model is the sum of $n$ categorical trials.</p>

<details class="math-details">
<summary> The categorical distribution
</summary>

The categorical distribution is the most general distribution
over the set of $k$ distinct elements, and it is defined as

$$
p(x | \theta_1,\dots \theta_k) = \theta_i \, if \, x = i
$$
where $$x \in \{1,2,\dots,k \}\,.$$

Since the total probability must be one, we have that

$$\sum_{i=0}^k \theta_i = 1 $$

</details>

<p>As an example, let us consider the 2022 Formula One championship, and
let us see what’s the winning probability of the best pilots.
The dataset can be found <a href="https://github.com/toUpperCase78/formula1-datasets/blob/master/Formula1_2022season_raceResults.csv">here</a>,
and the results for the wins are</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: left">Driver</th>
      <th style="text-align: right">N. win</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: left">Carlos Sainz</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: left">Charles Leclerc</td>
      <td style="text-align: right">3</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: left">George Russell</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: left">Max Verstappen</td>
      <td style="text-align: right">15</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: left">Sergio Perez</td>
      <td style="text-align: right">2</td>
    </tr>
  </tbody>
</table>

<p>As a prior we will take a <strong>Dirichlet</strong> distribution,
which is the generalization of the beta distribution to $n$ elements.
Its support is the n-dimensional unit simplex</p>

\[\begin{align}
&amp; x = (x_1,\dots, x_n) \\
&amp; 0 \leq x_i \leq 1 \\
&amp; \sum_{i=1}^n x_i = 1
\end{align}\]

<p>The Dirichlet takes an $n$ dimensional vector of real positive elements $\alpha = (\alpha_1,\dots,\alpha_n)$,
and we will take $\alpha_1=\dots=\alpha_n = \frac{1}{n}\,.$</p>

<p>Let us now estimate the winning probability for each of them</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="n">df_f1</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="s">'https://raw.githubusercontent.com/toUpperCase78/formula1-datasets/master/Formula1_2022season_raceResults.csv'</span><span class="p">)</span>

<span class="n">df_red</span> <span class="o">=</span> <span class="n">df_f1</span><span class="p">[</span><span class="n">df_f1</span><span class="p">[</span><span class="s">'Position'</span><span class="p">]</span><span class="o">==</span><span class="s">'1'</span><span class="p">].</span><span class="n">groupby</span><span class="p">(</span><span class="s">'Driver'</span><span class="p">).</span><span class="n">count</span><span class="p">()</span>


<span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

</code></pre></div></div>

<p>In order to build our model, we must map each pilot to an integer, and this can be done
with pandas’ <strong>factorize</strong> function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">factors</span> <span class="o">=</span>  <span class="n">pd</span><span class="p">.</span><span class="n">factorize</span><span class="p">(</span><span class="n">df_f1</span><span class="p">[</span><span class="n">df_f1</span><span class="p">[</span><span class="s">'Position'</span><span class="p">]</span><span class="o">==</span><span class="s">'1'</span><span class="p">][</span><span class="s">'Driver'</span><span class="p">])</span>
<span class="n">y_obs</span> <span class="o">=</span> <span class="n">factors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<p>Now we can run our model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">cat_model</span><span class="p">:</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="s">'p'</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_red</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df_red</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Categorical</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_obs</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/categories/trace.webp" alt="The trace for the multinomial model" /></p>

<p>Let us take a better look at our estimates.
First of all, we will build a dataframe to match each factor to the corresponding
name.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_names</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="n">from_dict</span><span class="p">({</span><span class="s">'name'</span><span class="p">:</span> <span class="n">factors</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span><span class="p">,</span>
                             <span class="s">'number'</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">factors</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span><span class="p">))})</span>

</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: left">name</th>
      <th style="text-align: right">number</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: left">Charles Leclerc</td>
      <td style="text-align: right">0</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: left">Max Verstappen</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: left">Sergio Perez</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: left">Carlos Sainz</td>
      <td style="text-align: right">3</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: left">George Russell</td>
      <td style="text-align: right">4</td>
    </tr>
  </tbody>
</table>

<p>We can now make our forest plot, by keeping in mind that the $y$ axis goes from the
bottom to the top (we must therefore revert the order of our dataframe).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">elem</span><span class="si">}</span><span class="s">"</span> <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">df_names</span><span class="p">[</span><span class="s">'name'</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/categories/forest.webp" alt="The forest plot for the probabilities" /></p>

<p>The components are strongly correlated</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_pair</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s">'kde'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/categories/kde.webp" alt="The pair plot of the probabilities" /></p>

<p>We can now take a look at the posterior predictive check</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">cat_model</span><span class="p">:</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div>

<p>Let us not plot the posterior predictive</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">ppc</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">df_names</span><span class="p">[</span><span class="s">'number'</span><span class="p">]</span><span class="o">+</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">df_names</span><span class="p">[</span><span class="s">'number'</span><span class="p">])])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">df_names</span><span class="p">[</span><span class="s">'name'</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/categories/ppc_categorical.webp" alt="The ppc for the multinomial model" /></p>

<p>As you can see, our model accurately reproduces the observed data.</p>

<h2 id="multivariate-normal">Multivariate normal</h2>

<p>Since the normal distribution with zero mean only depends on $x$ via $x^2/\sigma^2\,,$
we can immediately generalize it by replacing $x^2/\sigma^2$ with any
positively-defined form $x \cdot \Sigma \cdot x\,,$
and the generalization to the general $\mu$ case is straightforward.</p>

<p>A less straightforward choice is how to provide a prior for $\Sigma\,,$
and the most common solution is to use the <a href="https://www.pymc.io/projects/docs/en/stable/api/distributions/generated/pymc.LKJCholeskyCov.html">LKJ distributed correlations
</a>.</p>

<p>We will use this model to verify if there is any association between the speed 
and the alcohol consumption in the car crash dataset, which comes with the seaborn library.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_car</span> <span class="o">=</span> <span class="n">sns</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s">'car_crashes'</span><span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">df_car</span><span class="p">[[</span><span class="s">'speeding'</span><span class="p">,</span> <span class="s">'alcohol'</span><span class="p">]])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/categories/car_crash.webp" alt="The pairplot of the relevant variables" /></p>

<p>As we can see, we cannot treat them independently, since the expected value
for one variable depends on the other variable.
As an example, if we fix “alcohol” to 5 we get an expected value for speed
which is different from the expected value that we would get by fixing alcohol
to 7.5.
The simplest way to implement this behavior for real data is by means of the multivariate
normal model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">mvnorm</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">sd_dist</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">chol</span><span class="p">,</span> <span class="n">corr</span><span class="p">,</span> <span class="n">sigmas</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">LKJCholeskyCov</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sd_dist</span><span class="o">=</span><span class="n">sd_dist</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">MvNormal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">chol</span><span class="o">=</span><span class="n">chol</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_car</span><span class="p">[[</span><span class="s">'speeding'</span><span class="p">,</span> <span class="s">'alcohol'</span><span class="p">]])</span>
    <span class="n">trace_car</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_car</span><span class="p">,</span> 
                       <span class="n">coords</span><span class="o">=</span><span class="p">{</span><span class="s">"sigma_corr_dim_0"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">"sigma_corr_dim_1"</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/categories/trace_car.webp" alt="The trace plot of the multivariate normal model" /></p>

<p>The trace looks good. We included the coordinates option because the diagonal
terms in the correlation matrix are, by construction, always one.
This causes some issue to arviz that, when plotting, assumes that what has been provided
to the plot function, is a random variable with more than one value.
We can now turn to the posterior predictive checks.
We will do this as follows</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">mvnorm</span><span class="p">:</span>
    <span class="n">y_p</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">MvNormal</span><span class="p">(</span><span class="s">'y_p'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">chol</span><span class="o">=</span><span class="n">chol</span><span class="p">)</span>
    <span class="n">ppc_car</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_car</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y'</span><span class="p">,</span> <span class="s">'y_p'</span><span class="p">])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">()</span>
<span class="n">az</span><span class="p">.</span><span class="n">plot_pair</span><span class="p">(</span><span class="n">ppc_car</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y_p'</span><span class="p">],</span> <span class="n">kind</span><span class="o">=</span><span class="s">'kde'</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s">'posterior_predictive'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_car</span><span class="p">[</span><span class="s">'speeding'</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">df_car</span><span class="p">[</span><span class="s">'alcohol'</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/categories/ppc_car.webp" alt="The PPC for the multivariate normal model" /></p>

<p>The PPC looks quite good too, except for few outliers. These should be carefully investigated
by considering a more robust model or by changing the model structure and including additional
covariates. Since this goes beyond the scope of this post, however, we will leave the reader deal with
this problem.</p>

<p>We can now inspect the posterior density. Since we expect the correlation
between the variables to be relevant, we will only show the 2d joint kernel density estimate
of the posterior.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_pair</span><span class="p">(</span><span class="n">trace_car</span><span class="p">,</span>
             <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'mu'</span><span class="p">,</span> <span class="s">'sigma'</span><span class="p">,</span> <span class="s">'sigma_corr'</span><span class="p">],</span>
            <span class="n">kind</span><span class="o">=</span><span class="s">'kde'</span><span class="p">,</span>
                       <span class="n">coords</span><span class="o">=</span><span class="p">{</span><span class="s">"sigma_corr_dim_0"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">"sigma_corr_dim_1"</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/categories/kde_car.webp" alt="The pair plot of the posterior distribution" /></p>

<h2 id="conclusions">Conclusions</h2>

<p>As we have seen, for the normal and for the binomial model,
it straightforward to immediately generalize the one dimensional model
to the multidimensional one.
This is not true for all the models, but often the multivariate normal
and the multinomial models are good starting points to build more
involved models.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">pytensor</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>

<div class="code">
Last updated: Fri Jul 05 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.4
<br />
IPython version      : 8.24.0
<br />

<br />
xarray  : 2024.5.0
<br />
pytensor: 2.20.0
<br />
numpyro : 0.15.0
<br />
jax     : 0.4.28
<br />
jaxlib  : 0.4.28
<br />

<br />
matplotlib: 3.9.0
<br />
numpy     : 1.26.4
<br />
seaborn   : 0.13.2
<br />
arviz     : 0.18.0
<br />
pandas    : 2.2.2
<br />
pymc      : 5.15.0
<br />

<br />
Watermark: 2.4.3
<br />
</div>

  </div><a class="u-url" href="/statistics/categories" hidden></a>

  <div>
          
          
  </div>

  <br>
  <div id='autograph'>
          Stippe Jan 7, 2024

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>


  <div class="wrapper">
          <div class='footer-text'>
                  Opinions are mine, mistakes too, and if you find any feel free to report it via mail or via Twitter.
                  <br>
                  Most of the material in the statistics section is an adaptation to Python of some pre-existing model.
                  <br>
                  I have tried to provide the necessary credits, but if you think that a relevant contribution is missing, please let me know.
                  <br>
                  No AI were harmed in creating this blog.
                  <br>
                  This is a blog, not a bakery: there are no cookies here!
                  <br>
                  The top image has been generated with a modified version Dan Gries' code, available at <a href="http://rectangleworld.com/blog/archives/538">http://rectangleworld.com</a>.
          </div>
          <br>

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Data-perspectives</li>
          <li><a class="u-email" href="mailto:dataperspectivesblog@gmail.com">dataperspectivesblog@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://twitter.com/SteffPy" target="_blank" title="twitter">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://stackoverflow.com/users/11065831/stefano" target="_blank" title="stackoverflow">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#stackoverflow"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://github.com/thestippe/" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://vis.social/@thestippe" target="_blank" title="mastodon">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#mastodon"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

  <script src="/docs/assets/javascript/scroll.js">
  </script>

</html>

