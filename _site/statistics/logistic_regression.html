<!DOCTYPE html>
<html lang="en"><head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-W9G73E5P44"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-W9G73E5P44');
</script>
          <link rel="icon" 
                type="image/png" 
                href="/docs/assets/images/dp_icon.png">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Logistic regression | Data Perspectives</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Logistic regression" />
<meta name="author" content="Data-perspectives" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How to perform regression on binary data" />
<meta property="og:description" content="How to perform regression on binary data" />
<link rel="canonical" href="http://localhost:4000/statistics/logistic_regression" />
<meta property="og:url" content="http://localhost:4000/statistics/logistic_regression" />
<meta property="og:site_name" content="Data Perspectives" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-01-27T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Logistic regression" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Data-perspectives"},"dateModified":"2024-01-27T00:00:00+00:00","datePublished":"2024-01-27T00:00:00+00:00","description":"How to perform regression on binary data","headline":"Logistic regression","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/statistics/logistic_regression"},"url":"http://localhost:4000/statistics/logistic_regression"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Perspectives" />


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
<a rel="me" href="https://vis.social/@thestippe" style="display: none;">Mastodon</a>
<link rel="manifest" href="manifest.json">
</head>
<body>

        <div id='upperBar'><header class="site-header">

        <div id='upperBarr'>
        <script src="https://d3js.org/d3.v7.js"></script>
                <div class="wrapper" style="display:flex;"><ul hidden='hidden' id="postList"><li>
                        Splines regression;/statistics/splines;1
                </li><li>
                        Introduction to non-parametric models;/statistics/nonparametric;2
                </li><li>
                        The autoregressive model;/statistics/autoregressive;3
                </li><li>
                        Introduction to time series modelling;/statistics/time_series;4
                </li><li>
                        Synthetic control;/statistics/synthetic_control;5
                </li><li>
                        Regression discontinuity ;/statistics/discontinuity_regression;6
                </li><li>
                        Difference in difference;/statistics/difference_in_differences;7
                </li><li>
                        Instrumental variable regression;/statistics/instrumental_variable;8
                </li><li>
                        Randomized controlled trials;/statistics/randomized;9
                </li><li>
                        Causal inference;/statistics/causal_intro;10
                </li><li>
                        Random models and mixed models;/statistics/random_models;11
                </li><li>
                        Introduction to Extreme Values theory;/statistics/extreme_intro;12
                </li><li>
                        Application of survival analysis 1;/statistics/survival_example;13
                </li><li>
                        Introduction to survival analysis;/statistics/survival_analysis;14
                </li><li>
                        Hierarchical models and meta-analysis;/statistics/hierarchical_metaanalysis;15
                </li><li>
                        Hierarchical models;/statistics/hierarchical_models;16
                </li><li>
                        Poisson regression;/statistics/poisson_regression;17
                </li><li>
                        Logistic regression;/statistics/logistic_regression;18
                </li><li>
                        Robust linear regression;/statistics/robust_regression;19
                </li><li>
                        Introduction to the linear regression;/statistics/regression;20
                </li><li>
                        Model comparison;/statistics/model_averaging;21
                </li><li>
                        Predictive checks;/statistics/predictive_checks;22
                </li><li>
                        Trace inspection;/statistics/trace_inspection;23
                </li><li>
                        Introduction to the Bayesian workflow;/statistics/bayesian_workflow;24
                </li><li>
                        Mixture models;/statistics/mixture;25
                </li><li>
                        Multidimensional distributions;/statistics/categories;26
                </li><li>
                        Exponential model, gaussian model and their evolutions;/statistics/reals;27
                </li><li>
                        The Negative Binomial model;/statistics/negbin;28
                </li><li>
                        The Poisson model;/statistics/poisson;29
                </li><li>
                        The Beta-Binomial model;/statistics/betabin;30
                </li><li>
                        Common continuous probabilities;/statistics/common_continuous_probabilities;31
                </li><li>
                        Common discrete probabilities;/statistics/common_discrete_probabilities;32
                </li><li>
                        How does MCMC works;/statistics/mcmc_intro;33
                </li><li>
                        Introduction to Bayesian inference;/statistics/bayes_intro;34
                </li><li>
                        Design tricks;/dataviz/design-introduction;35
                </li><li>
                        How to choose a color map;/dataviz/palettes-introduction;36
                </li><li>
                        Introduction to color perception;/dataviz/color-introduction;37
                </li><li>
                        Drawing is redrawing;/dataviz/gender-economist;38
                </li><li>
                        Visual queries;/dataviz/visual-queries;39
                </li><li>
                        The Gestalt principles;/dataviz/gestalt;40
                </li><li>
                        Channel effectiveness;/dataviz/effectiveness;41
                </li><li>
                        Evolutions of the line chart;/dataviz/linechart-evolution;42
                </li><li>
                        Beyond the 1D scatterplot;/dataviz/scatterplot-evolution;43
                </li><li>
                        Perception;/dataviz/perception;44
                </li><li>
                        Fundamental charts;/dataviz/fundamental-charts;45
                </li><li>
                        Marks and channels;/dataviz/marks-channels;46
                </li><li>
                        Data abstraction;/dataviz/data-types;47
                </li><li>
                        Data visualization;/dataviz/dataviz;48
                </li></ul>
                        <div style="display:flex">
                  <a href="/statistics/robust_regression" class="prev">&#8249;</a>
                  
                                <a href="/"><img class="site-masthead" src="/docs/assets/images/logo_dp.png" alt="Data Perspectives" id="logo" /></a><div id='searchNav' style="flex;">
                                        <input type="search" id="search_0" class="searchBar" onkeydown="searchText()" placeholder="Search">
                                </div>

                                <div hidden='hidden' id="search_focus">0</div>



                        </div><nav class="site-nav" style="display:flex;">
                                <input type="checkbox" id="nav-trigger" class="nav-trigger" />
                                <label for="nav-trigger">
                                        <span class="menu-icon">
                                                <svg viewBox="0 0 18 15" width="18px" height="15px">
                                                        <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
                                                </svg>
                                        </span>
                                </label>

                                <div class="trigger"><a  class="page-link" href="/about">About me</a><a  class="page-link" href="/links">Resources</a>
                                <a href="/statistics/" class="page-link">Up</a>
                                
                                </div>
                        </nav>
                  <a href="/statistics/poisson_regression" class="next">&#8250;</a>
                  
        </div>


        <script src="/docs/assets/javascript/search.js">
        </script>
                </div>

</header>
<div class="progress-container">
    <div class="progress-bar" id="myBar"></div>
    </div>
        </div>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
          <div id='topPage'></div>
        <a href='/index'>
<img src="/docs/assets/images/background_resized.webp" alt="backround" style="margin:auto;display:block;width:1200px">

</a>
    <h1 class="post-title p-name" itemprop="name headline">Logistic regression</h1>
    <p class="post-meta"><time class="dt-published" datetime="2024-01-27T00:00:00+00:00" itemprop="datePublished">
        Jan 27, 2024
      </time></p>
    <p class="post-meta"> Reading time: <span class="reading-time" title="Estimated read time">
  
  11&prime;
</span>
</p>
  </header>
  <br>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In the last posts we discussed how to build
the simplest regression model for a real variable
with the linear model.
This model can be used as a starting block
to perform regression on many other types of data,
and this can be done by building
a <strong>Generalized Linear Model</strong> (GLM).</p>

<p>GLMs can be constructed by starting
from any likelihood for the
data \(P(y | \theta)\,.\)</p>

<p>The parameter $\theta$ usually is bounded to
some specific range \(D\): we have
\(\theta \in [0, 1]\) for the Binomial likelihood,
while we have $\theta &gt; 0$ for the Poisson model.
On the other hand, the variable</p>

\[Z \sim \alpha + \beta X\]

<p>can generally take any real value.
However, by choosing a suitable function</p>

\[f : \mathbb{R} \rightarrow D\]

<p>we can map our random variable \(Z\) to
the desired domain \(D\,.\)</p>

<p>The general GLM can therefore reads</p>

\[\begin{align}
Y &amp; \sim P(\theta) \\
\theta &amp; = f\left(\alpha + \beta X\right)
\end{align}\]

<p>Of course $\alpha$ and $\beta$ and any other parameter
$\phi$ will be described by a suitable prior distribution.</p>

<p>Let us now see how to do this in practice.</p>

<h2 id="the-logistic-model">The logistic model</h2>

<p>The logistic model can be applied when there is a single binary dependent variable
which depends on one or more independent variables, which can be binary, integer or continuous.
In the logistic model the likelihood is taken as the binomial one,
while the mapping function $f$ is taken as the logistic function, plotted below:</p>

\[f(x) = \frac{1}{1+e^{-x}}\]

<p><img src="/docs/assets/images/statistics/logistic/logistic.webp" alt="The logistic function" /></p>

<p>We will apply the logistic regression to the Challenger O-ring dataset. 
On January 28th 1986 the shuttle broke during the launch, killing several people,
and the USA president formed a commission to investigate on the causes of the incident.
One of the member of the commission was the physicist Richard Feynman,
who proved that the incident was caused by a loss of flexibility of the shuttle
O-rings caused by the low temperature 
(see the <a href="https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disaster">Wikipedia page</a>)
Here we will take the data on the number of O-rings damaged in each mission of the Challenger
and we will provide an estimate on the probability that one o-ring becomes damaged as a function of the temperature.
The original data can be found <a href="https://archive.ics.uci.edu/dataset/92/challenger+usa+space+shuttle+o+ring">here</a>,
and we provide here the dataset grouped by temperature for completeness (the temperature is expressed in °F).</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">temperature</th>
      <th style="text-align: right">damaged</th>
      <th style="text-align: right">undamaged</th>
      <th style="text-align: right">count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">53</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: right">57</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">58</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">63</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">66</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">5</td>
      <td style="text-align: right">67</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">18</td>
      <td style="text-align: right">18</td>
    </tr>
    <tr>
      <td style="text-align: right">6</td>
      <td style="text-align: right">68</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">7</td>
      <td style="text-align: right">69</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">8</td>
      <td style="text-align: right">70</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">22</td>
      <td style="text-align: right">24</td>
    </tr>
    <tr>
      <td style="text-align: right">9</td>
      <td style="text-align: right">72</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">10</td>
      <td style="text-align: right">73</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">11</td>
      <td style="text-align: right">75</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">11</td>
      <td style="text-align: right">12</td>
    </tr>
    <tr>
      <td style="text-align: right">12</td>
      <td style="text-align: right">76</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">12</td>
      <td style="text-align: right">12</td>
    </tr>
    <tr>
      <td style="text-align: right">13</td>
      <td style="text-align: right">78</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">14</td>
      <td style="text-align: right">79</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
    <tr>
      <td style="text-align: right">15</td>
      <td style="text-align: right">81</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">6</td>
    </tr>
  </tbody>
</table>

<p>The dataset contains all the information collected before the Challenger disaster.
The logistic model is already implemented into PyMC, but to see how it works we will implement it from scratch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc.sampling_jax</span> <span class="k">as</span> <span class="n">pmjax</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>


<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">df_oring</span><span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./data/orings.csv'</span><span class="p">)</span>

<span class="c1"># Convert it to Celsius
</span>
<span class="n">df_oring</span><span class="p">[</span><span class="s">'deg'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_oring</span><span class="p">[</span><span class="s">'temperature'</span><span class="p">]</span><span class="o">-</span><span class="mi">32</span><span class="p">)</span><span class="o">*</span><span class="mi">5</span><span class="o">/</span><span class="mi">9</span>
</code></pre></div></div>

<p>I converted the temperature to Celsius degree because it is easier for me to 
reason in terms of Celsius degree.
Let us write down our model</p>

\[\begin{align}
Y_i \sim &amp; \mathcal{Binom}(p_i, n_i)\\
p_i = &amp; 
\frac{1}{1+e^{-\alpha - \beta X_i}} 
\\
\end{align}\]

<p>The <strong>odds ratio</strong> is defined as</p>

\[\begin{align}
\frac{p}{1-p} 
&amp;
=
\frac{1}{1+e^{-\alpha - \beta X}}\frac{1}{1-\frac{1}{1+e^{-\alpha - \beta X}}}
\\
&amp;
=
\frac{1}{1+e^{-\alpha - \beta X}}\frac{ 1+e^{-\alpha - \beta X} }{e^{-\alpha - \beta X}}
\\
&amp;
= e^{\alpha + \beta X}
\end{align}\]

<p>therefore</p>

\[\log\left(\frac{p}{1-p}\right) = \alpha + \beta X\]

<p>We can therefore identify $\alpha$ with the log odds at $T=0°C$
It doesn’t really makes sense to assume either a too big number or a too small one,
so we will take</p>

\[\alpha \sim \mathcal{N}(0, 15)\]

<p>On the other hand, $\beta$ represents the variation of the log odds with an increase of $1°C\,.$
We do expect a meaningful variation on a scale of $10°C\,,$ 
so we can generously take</p>

\[\beta \sim \mathcal{N}(0, 2)\]

<p>We are now ready to implement our model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">log_theta</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">df_oring</span><span class="p">[</span><span class="s">'deg'</span><span class="p">]</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">log_theta</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">df_oring</span><span class="p">[</span><span class="s">'count'</span><span class="p">],</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_oring</span><span class="p">[</span><span class="s">'undamaged'</span><span class="p">])</span>

<span class="k">with</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">trace_logistic</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_logistic</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/logistic/trace.webp" alt="The trace of the logistic model" /></p>

<p>The trace looks fine, we can now take a look at the posterior predictive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_pl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

<span class="k">with</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">x_pl</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s">'p'</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">mu</span><span class="p">)))</span>

<span class="k">with</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_logistic</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y'</span><span class="p">,</span> <span class="s">'mu'</span><span class="p">,</span> <span class="s">'p'</span><span class="p">])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span>
<span class="n">posterior_predictive</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">.</span><span class="n">p</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
                <span class="mi">1</span><span class="o">-</span>
<span class="n">posterior_predictive</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">.</span><span class="n">p</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
        <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span>
<span class="n">posterior_predictive</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">.</span><span class="n">p</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
        <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_oring</span><span class="p">[</span><span class="s">'deg'</span><span class="p">],</span> <span class="n">df_oring</span><span class="p">[</span><span class="s">'damaged'</span><span class="p">]</span><span class="o">/</span><span class="n">df_oring</span><span class="p">[</span><span class="s">'count'</span><span class="p">],</span>
           <span class="n">marker</span><span class="o">=</span><span class="s">'x'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'raw data estimate'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">1.01</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">"T $\degree$C"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">"Fraction of damaged O-rings"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/logistic/posterior_predictive.webp" alt="The trace of the logistic model" /></p>

<p>As we can see, the more we approach $0°\,,$ the more it is likely that an O-ring gets damaged.
The forecasted temperature for the launch day was $26-29 °F\,,$ corresponding to a range between
$-1.6$ °C and $-3.3$ °C.</p>

<p>We must however consider that one broken O-ring is not enough to create serious issues.
We can therefore estimate the probability as a function of the number of undamaged rings.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tm</span> <span class="o">=</span> <span class="p">(</span><span class="mi">26</span><span class="o">-</span><span class="mi">32</span><span class="p">)</span><span class="o">*</span><span class="mi">5</span><span class="o">/</span><span class="mi">9</span>
<span class="n">tM</span> <span class="o">=</span> <span class="p">(</span><span class="mi">29</span><span class="o">-</span><span class="mi">32</span><span class="p">)</span><span class="o">*</span><span class="mi">5</span><span class="o">/</span><span class="mi">9</span>

<span class="k">with</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">theta_m</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">tm</span><span class="o">*</span><span class="n">beta</span><span class="p">)))</span>
    <span class="n">ym</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">'ym'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta_m</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
    <span class="n">theta_M</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">tm</span><span class="o">*</span><span class="n">beta</span><span class="p">)))</span>
    <span class="n">yM</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">'yM'</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta_M</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

<span class="k">with</span> <span class="n">logistic</span><span class="p">:</span>
    <span class="n">ppc_t</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_logistic</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'ym'</span><span class="p">,</span> <span class="s">'yM'</span><span class="p">])</span>

<span class="c1"># We count how many O-rings are undamaged for each draw
</span>
<span class="n">hm</span> <span class="o">=</span> <span class="p">[(</span><span class="n">ppc_t</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'ym'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="n">k</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">)]</span>
<span class="n">hM</span> <span class="o">=</span> <span class="p">[(</span><span class="n">ppc_t</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'yM'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="n">k</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">)]</span>
<span class="n">h_0</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">)]</span>

<span class="c1"># And we now estimate the corresponding probability
</span>
<span class="n">df_h</span><span class="p">[</span><span class="s">'prob_m'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_h</span><span class="p">[</span><span class="s">'count_m'</span><span class="p">]</span><span class="o">/</span><span class="n">df_h</span><span class="p">[</span><span class="s">'count_m'</span><span class="p">].</span><span class="nb">sum</span><span class="p">()</span>
<span class="n">df_h</span><span class="p">[</span><span class="s">'prob_M'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_h</span><span class="p">[</span><span class="s">'count_M'</span><span class="p">]</span><span class="o">/</span><span class="n">df_h</span><span class="p">[</span><span class="s">'count_M'</span><span class="p">].</span><span class="nb">sum</span><span class="p">()</span>

<span class="n">df_h</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'n'</span><span class="p">:</span> <span class="n">h_0</span><span class="p">,</span> <span class="s">'count_m'</span><span class="p">:</span> <span class="n">hm</span><span class="p">,</span> <span class="s">'count_M'</span><span class="p">:</span> <span class="n">hM</span><span class="p">})</span>

<span class="c1"># Let us take a look at the best-case scenario
</span><span class="n">sns</span><span class="p">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">df_h</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'n'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'prob_M'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/logistic/best_case.webp" alt="The probaility as a function of the undamaged rings" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_h</span><span class="p">[</span><span class="n">df_h</span><span class="p">[</span><span class="s">'n'</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">][</span><span class="s">'prob_M'</span><span class="p">]</span>
</code></pre></div></div>

<div class="code">
0.949
</div>
<p>The most probable scenario is that all O-rings get damaged, and this 
is scenario has, according to our model, the $95\%$ or probability to happen.</p>

<p>We can conclude that, with the available information,
it was not safe to perform the launch.
This is however a <em>post-hoc</em> analysis (an analysis performed on some
data once the outcome is known), and one should be really careful
to draw conclusions based on this kind of analysis, as this easily results
into false positive errors (see <a href="https://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data">the Wikipedia page on this topic</a>).</p>

<h2 id="conclusions">Conclusions</h2>

<p>We introduced the Generalized Linear Model and we analyzed the Challenger dataset
by means of a logistic regression.
In the next post we will discuss about Poisson regression.</p>

  </div><a class="u-url" href="/statistics/logistic_regression" hidden></a>

  <div>
          
          
  </div>

  <br>
  <div id='autograph'>
          Stippe Jan 27, 2024

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>


  <div class="wrapper">
          <div class='footer-text'>

                  No AI were harmed in creating this blog.
                  <br>
                  This is a blog, not a bakery: there are no cookies here!
                  <br>
                  The top image has been generated with a modified version Dan Gries' code, available at <a href="http://rectangleworld.com/blog/archives/538">http://rectangleworld.com</a>.
          </div>
          <br>

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Data-perspectives</li>
          <li><a class="u-email" href="mailto:dataperspectivesblog@gmail.com">dataperspectivesblog@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://twitter.com/SteffPy" target="_blank" title="twitter">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://stackoverflow.com/users/11065831/stefano" target="_blank" title="stackoverflow">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#stackoverflow"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://github.com/thestippe/" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://vis.social/@thestippe" target="_blank" title="mastodon">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#mastodon"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

  <script src="/docs/assets/javascript/scroll.js">
  </script>

</html>

