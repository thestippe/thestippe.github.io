<!DOCTYPE html>
<html lang="en"><head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-W9G73E5P44"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-W9G73E5P44');
</script>
          <link rel="icon" 
                type="image/png" 
                href="/docs/assets/images/dp_icon.png">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Bayesian Additive Regression Trees | Data Perspectives</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Bayesian Additive Regression Trees" />
<meta name="author" content="Data-perspectives" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Flexible interpolation with regression trees" />
<meta property="og:description" content="Flexible interpolation with regression trees" />
<link rel="canonical" href="http://localhost:4000/statistics/bart" />
<meta property="og:url" content="http://localhost:4000/statistics/bart" />
<meta property="og:site_name" content="Data Perspectives" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-08-18T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Bayesian Additive Regression Trees" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Data-perspectives"},"dateModified":"2024-08-18T00:00:00+00:00","datePublished":"2024-08-18T00:00:00+00:00","description":"Flexible interpolation with regression trees","headline":"Bayesian Additive Regression Trees","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/statistics/bart"},"url":"http://localhost:4000/statistics/bart"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Perspectives" />


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
<a rel="me" href="https://vis.social/@thestippe" style="display: none;">Mastodon</a>
<link rel="manifest" href="manifest.json">
</head>
<body>

        <div id='upperBar'><header class="site-header">

        <div id='upperBarr'>
        <script src="https://d3js.org/d3.v7.js"></script>
                <div class="wrapper" style="display:flex;">

                
                
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                    
                    
                    
                    
                    

        <ul hidden='hidden' id="postList"><li>
                        Poisson regression;/statistics/poisson_regression;1
                </li><li>
                        Logistic regression;/statistics/logistic_regression;2
                </li><li>
                        Robust linear regression;/statistics/robust_regression;3
                </li><li>
                        Multi-linear regression;/statistics/multivariate_regression;4
                </li><li>
                        Linear regression with binary input;/statistics/regression_binary_input;5
                </li><li>
                        Horseshoe priors;/statistics/horseshoe;6
                </li><li>
                        Introduction to the linear regression;/statistics/regression;7
                </li><li>
                        MRP;/statistics/mrp;8
                </li><li>
                        Model comparison, cont.;/statistics/model_averaging_cont;9
                </li><li>
                        Model comparison;/statistics/model_averaging;10
                </li><li>
                        Application of the Lotka-Volterra model;/statistics/lotka_volterra;11
                </li><li>
                        Differential equations;/statistics/ode;12
                </li><li>
                        Introduction to GIS;/gis/gis_intro;13
                </li><li>
                        Re-parametrizing your model;/statistics/reparametrization;14
                </li><li>
                        Predictive checks;/statistics/predictive_checks;15
                </li><li>
                        Trace inspection;/statistics/trace_inspection;16
                </li><li>
                        Introduction to the Bayesian workflow;/statistics/bayesian_workflow;17
                </li><li>
                        Mixture models;/statistics/mixture;18
                </li><li>
                        Multidimensional distributions;/statistics/categories;19
                </li><li>
                        Dirichlet Process Mixture Models;/statistics/dp;20
                </li><li>
                        The Gaussian model;/statistics/reals;21
                </li><li>
                        Bayesian Additive Regression Trees;/statistics/bart;22
                </li><li>
                        Bonus: counting animals in a park;/statistics/hypergeom;23
                </li><li>
                        Splines;/statistics/spline;24
                </li><li>
                        The Negative Binomial model;/statistics/negbin;25
                </li><li>
                        Gaussian processes regression;/statistics/gp_example;26
                </li><li>
                        Gaussian processes;/statistics/gp;27
                </li><li>
                        The Poisson model;/statistics/poisson;28
                </li><li>
                        Nonparametric models;/statistics/nonparametric_intro;29
                </li><li>
                        The Beta-Binomial model;/statistics/betabin;30
                </li><li>
                        Structural time series;/statistics/structural_time_series;31
                </li><li>
                        Time series;/statistics/time_series;32
                </li><li>
                        Some notation about probability;/statistics/probability_reminder;33
                </li><li>
                        Synthetic control;/statistics/synthetic_control;34
                </li><li>
                        How does MCMC works;/statistics/mcmc_intro;35
                </li><li>
                        Regression discontinuity design;/statistics/rdd;36
                </li><li>
                        Introduction to Bayesian inference;/statistics/bayes_intro;37
                </li><li>
                        An overview to statistics;/statistics/preface;38
                </li><li>
                        Difference in difference;/statistics/difference_in_differences;39
                </li><li>
                        Instrumental variable regression;/statistics/instrumental_variable;40
                </li><li>
                        Randomized controlled trials;/statistics/randomized;41
                </li><li>
                        Causal inference and Bayesian networks;/statistics/causal_intro_2;42
                </li><li>
                        Causal inference;/statistics/causal_intro;43
                </li><li>
                        Experiment analysis with many blocking variables;/statistics/experiment_design_cont;44
                </li><li>
                        Experiment analysis;/statistics/experiment_design;45
                </li><li>
                        Introduction to Extreme Values theory;/statistics/extreme_intro;46
                </li><li>
                        Application of survival analysis with discrete times;/statistics/survival_example_2;47
                </li><li>
                        Application of survival analysis 1;/statistics/survival_example;48
                </li><li>
                        Introduction to survival analysis;/statistics/survival_analysis;49
                </li><li>
                        Random models and mixed models;/statistics/random_models;50
                </li><li>
                        Hierarchical models and meta-analysis;/statistics/hierarchical_metaanalysis;51
                </li><li>
                        Hierarchical models;/statistics/hierarchical_models;52
                </li><li>
                        Drawing geographic maps;/dataviz/geography;53
                </li><li>
                        The Gestalt principles;/dataviz/gestalt;54
                </li><li>
                        Design tricks;/dataviz/design-introduction;55
                </li><li>
                        How to choose a color map;/dataviz/palettes-introduction;56
                </li><li>
                        Introduction to color perception;/dataviz/color-introduction;57
                </li><li>
                        Drawing is redrawing;/dataviz/gender-economist;58
                </li><li>
                        Visual queries;/dataviz/visual-queries;59
                </li><li>
                        Channel effectiveness;/dataviz/effectiveness;60
                </li><li>
                        Evolutions of the line chart;/dataviz/linechart-evolution;61
                </li><li>
                        Beyond the 1D scatterplot;/dataviz/scatterplot-evolution;62
                </li><li>
                        Perception;/dataviz/perception;63
                </li><li>
                        Fundamental charts;/dataviz/fundamental-charts;64
                </li><li>
                        Marks and channels;/dataviz/marks-channels;65
                </li><li>
                        Data abstraction;/dataviz/data-types;66
                </li><li>
                        Data visualization;/dataviz/dataviz;67
                </li><li>
                        Section introduction;/statistics/simple_models_intro;68
                </li></ul>
                        <div style="display:flex">
                  <a href="/statistics/hypergeom" class="prev">&#8249;</a>
                  
                                <a href="/"><img class="site-masthead" src="/docs/assets/images/logo_dp.png" alt="Data Perspectives" id="logo" /></a><div id='searchNav' style="flex;">
                                        <input type="search" id="search_0" class="searchBar" onkeydown="searchText()" placeholder="Search">
                                </div>

                                <div hidden='hidden' id="search_focus">0</div>



                        </div><nav class="site-nav" style="display:flex;">
                                <input type="checkbox" id="nav-trigger" class="nav-trigger" />
                                <label for="nav-trigger">
                                        <span class="menu-icon">
                                                <svg viewBox="0 0 18 15" width="18px" height="15px">
                                                        <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
                                                </svg>
                                        </span>
                                </label>

                                <div class="trigger"><a  class="page-link" href="/about">About me</a><a  class="page-link" href="/links">Resources</a>
                                <a href="/statistics/" class="page-link">Up</a>
                                
                                </div>
                        </nav>
                  <a href="/statistics/reals" class="next">&#8250;</a>
                  
        </div>


        <script src="/docs/assets/javascript/search.js">
        </script>
                </div>

</header>
<div class="progress-container">
    <div class="progress-bar" id="myBar"></div>
    </div>
        </div>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
          <div id='topPage'></div>
        <a href='/index'>
<img src="/docs/assets/images/background_resized.webp" alt="backround" style="margin:auto;display:block;width:1200px">

</a>
    <h1 class="post-title p-name" itemprop="name headline">Bayesian Additive Regression Trees</h1>
    <p class="post-meta"><time class="dt-published" datetime="2024-08-18T00:00:00+00:00" itemprop="datePublished">
        Aug 18, 2024
      </time></p>
    <p class="post-meta"> Reading time: <span class="reading-time" title="Estimated read time">
  
  8&prime;
</span>
</p>
  </header>
  <br>

  <div class="post-content e-content" itemprop="articleBody">
    <p>BART is a black box Bayesian method proposed in 2010 to approximate functions, and it can be useful when
you need to interpolate your data, but it is hard to figure out a transparent way to do so.
BART assumes</p>

\[Y \sim f(X) + \varepsilon\]

<p>where $\varepsilon$ is normally distributed, and</p>

\[f(X) = \sum_i g_i(X, T_i, M_i)\]

<p>Here $T_i$ represents a binary tree, and $M_i$ the set of means associated to $T_i$
In practice, a binary tree can be seen as a set of if-else, and an example is</p>

\[g_0 =
\begin{cases}
X &lt; c_1 &amp; \mu_1 \\
X \geq c_1 &amp; 
\begin{cases}
X &lt; c_2 &amp; \mu_2 \\
X \geq c_2 &amp; \mu_3 \\
\end{cases}
\\
\end{cases}\]

<p>Bart is a Bayesian method because both $T_i$ and $M_i$ are regularized by using priors.
For a more in-depth discussion about BARTs, you can take a look at 
<a href="https://arxiv.org/pdf/2206.03619">this preprint</a>
or at the <a href="https://www.pymc.io/projects/bart/en/latest/index.html">PyMC-BART homepage</a>.</p>

<h2 id="the-diamond-dataset">The diamond dataset</h2>

<p>We will use BART to fit the diamond dataset, which is dataset proposed
in <a href="https://www.tandfonline.com/doi/full/10.1080/10691898.2001.11910659">this article</a>
to show some of the main issues you will have to deal with when fitting
real-World datasets.
I strongly encourage you to read this article, as it is a very instructive example
of some of the issues most data scientist faced when working to real problems.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">pymc_bart</span> <span class="k">as</span> <span class="n">pmb</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Diamond.csv'</span><span class="p">)</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">rownames</th>
      <th style="text-align: right">carat</th>
      <th style="text-align: left">colour</th>
      <th style="text-align: left">clarity</th>
      <th style="text-align: left">certification</th>
      <th style="text-align: right">price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">0.3</td>
      <td style="text-align: left">D</td>
      <td style="text-align: left">VS2</td>
      <td style="text-align: left">GIA</td>
      <td style="text-align: right">1302</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">0.3</td>
      <td style="text-align: left">E</td>
      <td style="text-align: left">VS1</td>
      <td style="text-align: left">GIA</td>
      <td style="text-align: right">1510</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">3</td>
      <td style="text-align: right">0.3</td>
      <td style="text-align: left">G</td>
      <td style="text-align: left">VVS1</td>
      <td style="text-align: left">GIA</td>
      <td style="text-align: right">1510</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">4</td>
      <td style="text-align: right">0.3</td>
      <td style="text-align: left">G</td>
      <td style="text-align: left">VS1</td>
      <td style="text-align: left">GIA</td>
      <td style="text-align: right">1260</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">0.31</td>
      <td style="text-align: left">D</td>
      <td style="text-align: left">VS1</td>
      <td style="text-align: left">GIA</td>
      <td style="text-align: right">1641</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'carat'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'price'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bart/price.webp" alt="" /></p>

<p>As we can see, it appears that the relation between carat number and price
is non-linear, and the price also looks heteroscedastic with respect to the price.
We will use BART both the mean and the variance of a normal distribution.
First of all, let us convert the categorical variables into a meaningful way:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'colour'</span><span class="p">]).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span>
               <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'clarity'</span><span class="p">]).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span>
               <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'certification'</span><span class="p">]).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span>
               <span class="n">df</span><span class="p">[</span><span class="s">'carat'</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">yobs</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'price'</span><span class="p">]</span><span class="o">/</span><span class="mi">1000</span>
</code></pre></div></div>

<p>We also scaled the observations in order to simplify the work to the algorithms.
We can now implement the model as follows</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">coords</span><span class="o">=</span><span class="p">{</span><span class="s">'obs'</span><span class="p">:</span> <span class="n">X</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="s">'cols'</span><span class="p">:</span> <span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">})</span> <span class="k">as</span> <span class="n">model_carat</span><span class="p">:</span>
    <span class="n">Xv</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Data</span><span class="p">(</span><span class="s">'Xv'</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">pmb</span><span class="p">.</span><span class="n">BART</span><span class="p">(</span><span class="s">"w"</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">Xv</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">yobs</span><span class="p">),</span> <span class="n">m</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">yobs</span><span class="p">)))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">"y"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">sigma</span><span class="o">=</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">observed</span><span class="o">=</span><span class="n">yobs</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_carat</span><span class="p">:</span>
    <span class="n">idata</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bart/trace.webp" alt="The trace of the BART model" /></p>

<p>It is really hard to verify if there is any numerical issue with the sampling.
It is in fact generally recommended to only use it for the non-BART part of the
model, which is absent here.
PyMC-BART comes in fact with its own routines for the convergence assessment.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pmb</span><span class="p">.</span><span class="n">plot_convergence</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">var_name</span><span class="o">=</span><span class="s">'w'</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bart/pmb_trace.webp" alt="The trace of the BART model
using PyMC-BART" /></p>

<p>The curves in the left-hand plot are entirely above the dashed line,
while the ones in the right-hand figure are mostly below the corresponding
dashed line, and this tells us that our computation can be considered as reliable.</p>

<p>Notice that we haven’t used numpyro as usual, as we cannot use it together
with PyMC-BART.
This is however not a problem, since PyMC is fast enough.</p>

<p>We can now inspect the posterior predictive distribution</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">posterior_mean</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">"w"</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s">"chain"</span><span class="p">,</span> <span class="s">"draw"</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">w_hdi</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">hdi</span><span class="p">(</span><span class="n">ary</span><span class="o">=</span><span class="n">idata</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s">"posterior"</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">"w"</span><span class="p">],</span> <span class="n">hdi_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_carat</span><span class="p">:</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata</span><span class="p">)</span>

<span class="n">pps</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span>
    <span class="n">ppc</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s">"posterior_predictive"</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">"y"</span><span class="p">]</span>
<span class="p">).</span><span class="n">T</span>

<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">Xv</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_hdi</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'carat'</span><span class="p">],</span>
    <span class="n">y</span><span class="o">=</span><span class="n">pps</span><span class="p">.</span><span class="n">values</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="n">hdi_prob</span><span class="o">=</span><span class="mf">0.90</span><span class="p">,</span>
    <span class="n">fill_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s">"alpha"</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span> <span class="s">"label"</span><span class="p">:</span> <span class="sa">r</span><span class="s">"Observations $90\%$ HDI"</span><span class="p">},</span>
<span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'carat'</span><span class="p">],</span>
    <span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">posterior_mean</span><span class="p">.</span><span class="n">values</span><span class="p">),</span>
    <span class="n">marker</span><span class="o">=</span><span class="s">'x'</span>
<span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'carat'</span><span class="p">],</span> <span class="n">yobs</span><span class="p">)</span>
<span class="c1"># ax.plot(df["youtube"], df["sales"], "o", c="C0", label="Raw Data")
</span><span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">"upper left"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bart/ppc.webp" alt="" /></p>

<p>Except from few extreme cases, our model seems appropriate to describe the observed price.
We can also assess the variable importance.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">pmb</span><span class="p">.</span><span class="n">plot_variable_importance</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">"VI"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bart/variable_importance.webp" alt="The variable importance plot" /></p>

<p>We can finally visualize the marginal dependence of the model on the single variables</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pmb</span><span class="p">.</span><span class="n">plot_pdp</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">yobs</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="n">grid</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
            <span class="n">var_discrete</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">14</span><span class="p">)))</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bart/plot_pdb.webp" alt="The marginal dependence plot" /></p>

<h2 id="conclusions">Conclusions</h2>
<p>We introduced BARTs, and we showed how to use them in PyMC by applying them
to the diamonds dataset.</p>

<h2 id="suggested-readings">Suggested readings</h2>
<ul>
  <li><cite>Quiroga, M., Garay, P.G., Alonso, J.M., Loyola, J.M., &amp; Martin, O.A. (2022). Bayesian additive regression trees for probabilistic programming.</cite></li>
  <li><cite>Chu, Singfat. (2001). Pricing the C’s of Diamond Stones. Journal of Statistics Education. 9. 10.1080/10691898.2001.11910659. </cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span>
</code></pre></div></div>

<div class="code">
Last updated: Wed Aug 21 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.4
<br />
IPython version      : 8.24.0
<br />

<br />
xarray: 2024.5.0
<br />

<br />
arviz     : 0.18.0
<br />
numpy     : 1.26.4
<br />
pandas    : 2.2.2
<br />
pymc      : 5.15.0
<br />
seaborn   : 0.13.2
<br />
matplotlib: 3.9.0
<br />
pymc_bart : 0.5.14
<br />

<br />
Watermark: 2.4.3
<br />
</div>

  </div><a class="u-url" href="/statistics/bart" hidden></a>

  <br>
  <div id='autograph'>
          Stippe Aug 18, 2024

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>


  <div class="wrapper">
          <div class='footer-text'>
                  Opinions are mine, mistakes too, and if you find any feel free to report it via mail or via Twitter.
                  <br>
                  Most of the material in the statistics section is an adaptation to Python of some pre-existing model.
                  <br>
                  I have tried to provide the necessary credits, but if you think that a relevant contribution is missing, please let me know.
                  <br>
                  No AI were harmed in creating this blog.
                  <br>
                  This is a blog, not a bakery: there are no cookies here!
                  <br>
                  The top image has been generated with a modified version Dan Gries' code, available at <a href="http://rectangleworld.com/blog/archives/538">http://rectangleworld.com</a>.
          </div>
          <br>

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Data-perspectives</li>
          <li><a class="u-email" href="mailto:dataperspectivesblog@gmail.com">dataperspectivesblog@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://twitter.com/SteffPy" target="_blank" title="twitter">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://stackoverflow.com/users/11065831/stefano" target="_blank" title="stackoverflow">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#stackoverflow"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://github.com/thestippe/" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://vis.social/@thestippe" target="_blank" title="mastodon">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#mastodon"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

  <script src="/docs/assets/javascript/scroll.js">
  </script>

</html>

