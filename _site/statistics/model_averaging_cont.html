<!DOCTYPE html>
<html lang="en"><head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-W9G73E5P44"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-W9G73E5P44');
</script>
          <link rel="icon" 
                type="image/png" 
                href="/docs/assets/images/dp_icon.png">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Model comparison, cont. | Data Perspectives</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Model comparison, cont." />
<meta name="author" content="Data-perspectives" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Cross validation in Bayesian statistics" />
<meta property="og:description" content="Cross validation in Bayesian statistics" />
<link rel="canonical" href="http://localhost:4000/statistics/model_averaging_cont" />
<meta property="og:url" content="http://localhost:4000/statistics/model_averaging_cont" />
<meta property="og:site_name" content="Data Perspectives" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-10T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Model comparison, cont." />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Data-perspectives"},"dateModified":"2024-10-10T00:00:00+00:00","datePublished":"2024-10-10T00:00:00+00:00","description":"Cross validation in Bayesian statistics","headline":"Model comparison, cont.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/statistics/model_averaging_cont"},"url":"http://localhost:4000/statistics/model_averaging_cont"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Perspectives" />


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
<a rel="me" href="https://vis.social/@thestippe" style="display: none;">Mastodon</a>
<link rel="manifest" href="manifest.json">
</head>
<body>

        <div id='upperBar'><header class="site-header">

        <div id='upperBarr'>
        <script src="https://d3js.org/d3.v7.js"></script>
                <div class="wrapper" style="display:flex;">

                
                
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                    
                    
                    
                    
                    

        <ul hidden='hidden' id="postList"><li>
                        Fitting complex models;/statistics/complex_models;1
                </li><li>
                        Directional statistics;/statistics/directional;2
                </li><li>
                        Horseshoe priors;/statistics/horseshoe;3
                </li><li>
                        MRP;/statistics/mrp;4
                </li><li>
                        Application of the Lotka-Volterra model;/statistics/lotka_volterra;5
                </li><li>
                        Differential equations;/statistics/ode;6
                </li><li>
                        Dirichlet Process Mixture Models;/statistics/dp;7
                </li><li>
                        Bayesian Additive Regression Trees;/statistics/bart;8
                </li><li>
                        Splines;/statistics/spline;9
                </li><li>
                        Gaussian processes regression;/statistics/gp_example;10
                </li><li>
                        Gaussian processes;/statistics/gp;11
                </li><li>
                        Nonparametric models;/statistics/nonparametric_intro;12
                </li><li>
                        Stochastic volatility models;/statistics/stochastic_volatility;13
                </li><li>
                        Time series;/statistics/time_series;14
                </li><li>
                        Structural time series;/statistics/structural_time_series;15
                </li><li>
                        Quantile regression;/statistics/extreme_quantile;16
                </li><li>
                        Introduction to Extreme Values theory;/statistics/extreme_intro;17
                </li><li>
                        Accelerated Failure Time models;/statistics/survival_example_aft;18
                </li><li>
                        Application of survival analysis with discrete times;/statistics/survival_example_2;19
                </li><li>
                        Application of survival analysis 1;/statistics/survival_example;20
                </li><li>
                        Introduction to survival analysis;/statistics/survival_analysis;21
                </li><li>
                        Regression discontinuity design;/statistics/rdd;22
                </li><li>
                        Difference in difference;/statistics/difference_in_differences;23
                </li><li>
                        Instrumental variable regression;/statistics/instrumental_variable;24
                </li><li>
                        Randomized controlled trials;/statistics/randomized;25
                </li><li>
                        Causal inference and Bayesian networks;/statistics/causal_intro_2;26
                </li><li>
                        Causal inference;/statistics/causal_intro;27
                </li><li>
                        Nested factor;/statistics/nested_factors;28
                </li><li>
                        Repeated measures;/statistics/repeated_measures;29
                </li><li>
                        Split plot design;/statistics/split_plot;30
                </li><li>
                        Crossover design;/statistics/crossover;31
                </li><li>
                        Latin square design;/statistics/latin_square;32
                </li><li>
                        Full factorial design;/statistics/full_factorial;33
                </li><li>
                        Completely randomized design;/statistics/crd;34
                </li><li>
                        Design of experiments;/statistics/doe;35
                </li><li>
                        Validity;/statistics/validity;36
                </li><li>
                        Stratification;/statistics/stratification;37
                </li><li>
                        Random sampling;/statistics/random_sampling;38
                </li><li>
                        Data collection;/statistics/data_collection;39
                </li><li>
                        Things that could go wrong;/statistics/problem_solving_issues;40
                </li><li>
                        The problem solving workflow;/statistics/problem_solving;41
                </li><li>
                        Time_series;/time_series;42
                </li><li>
                        Survival_example_2;/survival_example_2;43
                </li><li>
                        Survival_example;/survival_example;44
                </li><li>
                        Survival_analysis;/survival_analysis;45
                </li><li>
                        Structural_time_series;/structural_time_series;46
                </li><li>
                        Spline;/spline;47
                </li><li>
                        Ode;/ode;48
                </li><li>
                        Nonparametric_intro;/nonparametric_intro;49
                </li><li>
                        Mrp;/mrp;50
                </li><li>
                        Lotka_volterra;/lotka_volterra;51
                </li><li>
                        Horseshoe;/horseshoe;52
                </li><li>
                        Gp_example;/gp_example;53
                </li><li>
                        Gp;/gp;54
                </li><li>
                        Extreme_intro;/extreme_intro;55
                </li><li>
                        Dp;/dp;56
                </li><li>
                        Bart;/bart;57
                </li><li>
                        Mixed effects models with more than two levels;/statistics/three_levels;58
                </li><li>
                        Leveraging mixed-effect models;/statistics/bambi_multilevel;59
                </li><li>
                        Rdd;/rdd;60
                </li><li>
                        Difference_in_differences;/difference_in_differences;61
                </li><li>
                        Instrumental_variable;/instrumental_variable;62
                </li><li>
                        Randomized;/randomized;63
                </li><li>
                        Causal_intro_2;/causal_intro_2;64
                </li><li>
                        Causal_intro;/causal_intro;65
                </li><li>
                        Nested_factors;/nested_factors;66
                </li><li>
                        Repeated_measures;/repeated_measures;67
                </li><li>
                        Split_plot;/split_plot;68
                </li><li>
                        Crossover Design;/crossover-design;69
                </li><li>
                        Latin_square;/latin_square;70
                </li><li>
                        Full_factorial;/full_factorial;71
                </li><li>
                        Crd;/crd;72
                </li><li>
                        Doe;/doe;73
                </li><li>
                        Validity;/validity;74
                </li><li>
                        Random_sampling;/random_sampling;75
                </li><li>
                        Data_collection;/data_collection;76
                </li><li>
                        Problem_solving_issues;/problem_solving_issues;77
                </li><li>
                        Problem_solving;/problem_solving;78
                </li><li>
                        Three_levels;/three_levels;79
                </li><li>
                        Bambi_multilevel;/bambi_multilevel;80
                </li><li>
                        Random models and mixed models;/statistics/random_models;81
                </li><li>
                        Hierarchical models and meta-analysis;/statistics/hierarchical_metaanalysis;82
                </li><li>
                        OpenEO for SAR images;/gis/openeo_sar;83
                </li><li>
                        OpenEO 2: time series;/gis/openeo_ts;84
                </li><li>
                        Hierarchical models;/statistics/hierarchical_models;85
                </li><li>
                        OpenEO;/gis/openeo;86
                </li><li>
                        Poisson regression;/statistics/poisson_regression;87
                </li><li>
                        Open Street Map services;/gis/openstreetmap;88
                </li><li>
                        Logistic regression;/statistics/logistic_regression;89
                </li><li>
                        Open Web Consortium standards;/gis/owc_standards;90
                </li><li>
                        Robust linear regression;/statistics/robust_regression;91
                </li><li>
                        Map design;/gis/map_design;92
                </li><li>
                        Multi-linear regression;/statistics/multivariate_regression;93
                </li><li>
                        Operations on raster data;/gis/raster_ops;94
                </li><li>
                        Linear regression with binary input;/statistics/regression_binary_input;95
                </li><li>
                        Operations on vector data;/gis/vector_ops;96
                </li><li>
                        Introduction to the linear regression;/statistics/regression;97
                </li><li>
                        101 ways to reproject your data;/gis/pyproj;98
                </li><li>
                        Model comparison, cont.;/statistics/model_averaging_cont;99
                </li><li>
                        Model comparison;/statistics/model_averaging;100
                </li><li>
                        Raster data;/gis/raster_data;101
                </li><li>
                        Vector data;/gis/vector_data;102
                </li><li>
                        Choosing the right projection;/gis/projections;103
                </li><li>
                        Introduction to geographic data analysis;/gis/gis_intro;104
                </li><li>
                        Re-parametrizing your model;/statistics/reparametrization;105
                </li><li>
                        Predictive checks;/statistics/predictive_checks;106
                </li><li>
                        Trace inspection;/statistics/trace_inspection;107
                </li><li>
                        Introduction to the Bayesian workflow;/statistics/bayesian_workflow;108
                </li><li>
                        Mixture models;/statistics/mixture;109
                </li><li>
                        Multidimensional distributions;/statistics/categories;110
                </li><li>
                        The Gaussian model;/statistics/reals;111
                </li><li>
                        Bonus: counting animals in a park;/statistics/hypergeom;112
                </li><li>
                        The Negative Binomial model;/statistics/negbin;113
                </li><li>
                        The Poisson model;/statistics/poisson;114
                </li><li>
                        The Beta-Binomial model;/statistics/betabin;115
                </li><li>
                        Section introduction;/statistics/simple_models_intro;116
                </li><li>
                        Some notation about probability;/statistics/probability_reminder;117
                </li><li>
                        How does MCMC works;/statistics/mcmc_intro;118
                </li><li>
                        Introduction to Bayesian inference;/statistics/bayes_intro;119
                </li><li>
                        An overview to statistics;/statistics/preface;120
                </li><li>
                        The Gestalt principles;/dataviz/gestalt;121
                </li><li>
                        Design tricks;/dataviz/design-introduction;122
                </li><li>
                        How to choose a color map;/dataviz/palettes-introduction;123
                </li><li>
                        Introduction to color perception;/dataviz/color-introduction;124
                </li><li>
                        Drawing is redrawing;/dataviz/gender-economist;125
                </li><li>
                        Visual queries;/dataviz/visual-queries;126
                </li><li>
                        Channel effectiveness;/dataviz/effectiveness;127
                </li><li>
                        Evolutions of the line chart;/dataviz/linechart-evolution;128
                </li><li>
                        Beyond the 1D scatterplot;/dataviz/scatterplot-evolution;129
                </li><li>
                        Perception;/dataviz/perception;130
                </li><li>
                        Fundamental charts;/dataviz/fundamental-charts;131
                </li><li>
                        Marks and channels;/dataviz/marks-channels;132
                </li><li>
                        Data abstraction;/dataviz/data-types;133
                </li><li>
                        Data visualization;/dataviz/dataviz;134
                </li></ul>
                        <div style="display:flex">
                  <a href="/statistics/model_averaging" class="prev">&#8249;</a>
                  
                                <a href="/"><img class="site-masthead" src="/docs/assets/images/logo_dp.png" alt="Data Perspectives" id="logo" /></a><div id='searchNav' style="flex;">
                                        <input type="search" id="search_0" class="searchBar" onkeydown="searchText()" placeholder="Search">
                                </div>

                                <div hidden='hidden' id="search_focus">0</div>



                        </div><nav class="site-nav" style="display:flex;">
                                <input type="checkbox" id="nav-trigger" class="nav-trigger" />
                                <label for="nav-trigger">
                                        <span class="menu-icon">
                                                <svg viewBox="0 0 18 15" width="18px" height="15px">
                                                        <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
                                                </svg>
                                        </span>
                                </label>

                                <div class="trigger"><a  class="page-link" href="/about">About me</a><a  class="page-link" href="/links">Resources</a>
                                <a href="/statistics/" class="page-link">Up</a>
                                
                                </div>
                        </nav>
                  <a href="/statistics/regression" class="next">&#8250;</a>
                  
        </div>


        <script src="/docs/assets/javascript/search.js">
        </script>
                </div>

</header>
<div class="progress-container">
    <div class="progress-bar" id="myBar"></div>
    </div>
        </div>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
          <div id='topPage'></div>
        <a href='/index'>
<img src="/docs/assets/images/background_resized.webp" alt="backround" style="margin:auto;display:block;width:1200px">

</a>
    <h1 class="post-title p-name" itemprop="name headline">Model comparison, cont.</h1>
    <p class="post-meta"><time class="dt-published" datetime="2024-10-10T00:00:00+00:00" itemprop="datePublished">
        Oct 10, 2024
      </time></p>
    <p class="post-meta"> Reading time: <span class="reading-time" title="Estimated read time">
  
  12&prime;
</span>
</p>
  </header>
  <br>

  <div class="post-content e-content" itemprop="articleBody">
    <p>We previously discussed the Bayes Factors as a tool to choose between different models.
This method has however many issues, and it is generally not recommended to use it.
We will now discuss a very powerful method, namely the Leave One Out cross validation</p>

<h2 id="leave-one-out-cross-validation">Leave One Out cross-validation</h2>
<p>This method is generally preferred to the above one, as it has been pointed out
that Bayes factors are appropriate only when one of the models is true,
while in real world problems we don’t have any certainty about which is the model that
generated the data, assuming that it makes sense to claim that it exists such a model.
Moreover, the sampler used to compute the Bayes factor, namely Sequential Monte Carlo,
is generally less stable than the standard one used by PyMC, which is the NUTS sampler.
There are other, more philosophical reasons, pointed out by Gelman in <a href="https://statmodeling.stat.columbia.edu/2017/07/21/bayes-factor-term-came-references-generally-hate/">this post</a>,
but for now we won’t dig into this kind of discussion.</p>

<p>The LOO method is much more in the spirit of the Machine Learning, where
one splits the sample into a training set and a test set.
The train set is used to find the parameters, while the second one is
used to assess the performances of the model for new data.
This method, namely the <strong>cross validation</strong>, is by far the most
reliable one, and we generally recommend to use it.</p>

<p>LOO and cross validation adhere to the principles of scientific method,
where we use the predictions of our models to compare and criticize them.</p>

<p>It is however very common that the dataset is too small to allow
a full cross-validation.
The LOO cross validation is equivalent to the computation of</p>

\[ELPD = \sum_i \log p(y_i \vert y_{-i})\]

<p>where \(p(y_i\vert y_{-i})\) is the posterior predictive probability
of the point \(y_i\) relative to the model fitted by removing \(y_i\,.\)</p>

<p>We already anticipated this method in the post on the
<a href="/statistics/negbin">negative binomial model</a>,
but we will discuss it here more in depth.</p>

<p>In this example, we are looking for the distribution of the log-return
of an indian company.
We will first try and use a normal distribution. We will then use a more general
t-Student distribution to fit the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>
<span class="kn">import</span> <span class="nn">yfinance</span> <span class="k">as</span> <span class="n">yf</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tk</span> <span class="o">=</span> <span class="n">yf</span><span class="p">.</span><span class="n">Ticker</span><span class="p">(</span><span class="s">"AJMERA.NS"</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">tk</span><span class="p">.</span><span class="n">get_shares_full</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="s">"2023-01-01"</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s">"2024-07-01"</span><span class="p">)</span>

<span class="n">logret</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">diff</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">values</span><span class="p">))</span>

<span class="n">sns</span><span class="p">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">logret</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s">'density'</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging_cont/logret.webp" alt="" /></p>

<p>The distribution shows heavy tails, it is therefore quite clear that a normal distribution
might not be appropriate.
We will however start from the simplest model, and use it as a benchmark for a more involved model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">norm</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">yobs</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'yobs'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">logret</span><span class="p">)</span>

<span class="k">with</span> <span class="n">norm</span><span class="p">:</span>
    <span class="n">idata_norm</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_norm</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging_cont/trace_norm.webp" alt="" /></p>

<p>The trace doesn’t show any issue. Let us try with a Student-T distribution</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">t</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Gamma</span><span class="p">(</span><span class="s">'nu'</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">yobs</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'yobs'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">logret</span><span class="p">)</span>

<span class="k">with</span> <span class="n">t</span><span class="p">:</span>
    <span class="n">idata_t</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_t</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>

</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging_cont/trace_t.webp" alt="" /></p>

<p>Also in this case the trace doesn’t show any relevant issue.
Let us now check if we are able to reproduce the observed data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">norm</span><span class="p">:</span>
    <span class="n">idata_norm</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata_norm</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">idata_norm</span><span class="p">,</span> <span class="n">num_pp_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/model_averaging_cont/ppc_norm.webp" alt="" /></p>

<p>Our model seems totally unable to fit the data due to the presence of heavy tails.
Let us now verify if the second model does a better job.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">t</span><span class="p">:</span>
    <span class="n">idata_t</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata_t</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">idata_t</span><span class="p">,</span> <span class="n">num_pp_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/model_averaging_cont/ppc_t.webp" alt="" /></p>

<p>As you can see, there is much more agreement with the data, so this model looks
more appropriate.
Let us now see if the LOO cross validation confirms our first impression.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">norm</span><span class="p">:</span>
    <span class="n">pm</span><span class="p">.</span><span class="n">compute_log_likelihood</span><span class="p">(</span><span class="n">idata_norm</span><span class="p">)</span>

<span class="k">with</span> <span class="n">t</span><span class="p">:</span>
    <span class="n">pm</span><span class="p">.</span><span class="n">compute_log_likelihood</span><span class="p">(</span><span class="n">idata_t</span><span class="p">)</span>

<span class="n">df_comp_loo</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">compare</span><span class="p">({</span><span class="s">"norm"</span><span class="p">:</span> <span class="n">idata_norm</span><span class="p">,</span> <span class="s">"t"</span><span class="p">:</span> <span class="n">idata_t</span><span class="p">})</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_compare</span><span class="p">(</span><span class="n">df_comp_loo</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/model_averaging_cont/loo.webp" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_comp_loo</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right">rank</th>
      <th style="text-align: right">elpd_loo</th>
      <th style="text-align: right">p_loo</th>
      <th style="text-align: right">elpd_diff</th>
      <th style="text-align: right">weight</th>
      <th style="text-align: right">se</th>
      <th style="text-align: right">dse</th>
      <th style="text-align: left">warning</th>
      <th style="text-align: left">scale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">t</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">929.203</td>
      <td style="text-align: right">2.6402</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0.870715</td>
      <td style="text-align: right">30.6267</td>
      <td style="text-align: right">0</td>
      <td style="text-align: left">False</td>
      <td style="text-align: left">log</td>
    </tr>
    <tr>
      <td style="text-align: left">norm</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">740.195</td>
      <td style="text-align: right">7.50734</td>
      <td style="text-align: right">189.008</td>
      <td style="text-align: right">0.129285</td>
      <td style="text-align: right">30.2578</td>
      <td style="text-align: right">25.8493</td>
      <td style="text-align: left">False</td>
      <td style="text-align: left">log</td>
    </tr>
  </tbody>
</table>

<p>We can also use the LOO Probability Integral Transform (LOO-PIT).
The main idea behind this method is that, if the $y_i$s are distributed
according to $p(\tilde{y} \vert y_{-i}),$ then the LOO PIT</p>

\[P(y_i \leq y^* \vert y_{-i}) = \int_{-\infty}^{y_i} d\tilde{y} p(\tilde{y} \vert y_{-i})\]

<p>should be a uniform distribution.
A very nice explanation of this method can be found in
<a href="https://oriolabril.github.io/gsoc2019_blog/2019/07/31/loo-pit.html">this blog</a>.
In the reference there are unfortunately some missing figure where one can
clearly understand how does the LOO-PIT relates to the posterior predictive
distribution.
We therefore decided to make a similar plot</p>

<p><img src="/docs/assets/images/statistics/model_averaging_cont/ecdf_comp.webp" alt="" /></p>

<p>Le left column corresponds to the posterior predictive distribution, the central one to the LOO-PIT and the right one to the LOO-PIT ECDF.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_loo_pit</span><span class="p">(</span><span class="n">idata_norm</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"yobs"</span><span class="p">,</span> <span class="n">ecdf</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging_cont/loo_pit_norm.webp" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_loo_pit</span><span class="p">(</span><span class="n">idata_t</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"yobs"</span><span class="p">,</span> <span class="n">ecdf</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging_cont/loo_pit_t.webp" alt="" /></p>

<p>It is clear that the normal model is over-dispersed with respect to the observed data,
while the t-Student model gives a LOO-PIT which is compatible with the uniform distribution.</p>

<p>Another related plot which may be useful is the difference between two models’ Expected Log Pointwise Density (ELPD),
defined as</p>

\[\int d\theta \log(p(y_i \vert \theta)) p(\theta \vert y) \approx \frac{1}{S} \sum_s \log(p(y_i \vert \theta^s))\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_elpd</span><span class="p">({</span><span class="s">'norm'</span><span class="p">:</span> <span class="n">idata_norm</span><span class="p">,</span> <span class="s">'t'</span><span class="p">:</span> <span class="n">idata_t</span><span class="p">})</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging_cont/plot_elpd.webp" alt="" /></p>

<p>Since there are many points below 0, we can see that we should favor the t-Student’s model.
There are also few points far below 0, and the t model gives much better results for them.
It is in fact likely that those points are far away from the mean value,
where the normal distribution has very small probability density,
while the t-Student model allows for heavier tails and therefore are more likely to be observed
according to this model.</p>

<p>Notice that Arviz plots the ELPD difference against the point index, it would
be instead better to plot the ELPD difference against one model’s ELPD,
since the ELPD difference only makes sense when compared to one model’s ELPD.
We can however easily overcome this issue as follows</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">elpd_norm</span> <span class="o">=</span> <span class="n">idata_norm</span><span class="p">.</span><span class="n">log_likelihood</span><span class="p">[</span><span class="s">'yobs'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">))</span>
<span class="n">elpd_t</span> <span class="o">=</span> <span class="n">idata_t</span><span class="p">.</span><span class="n">log_likelihood</span><span class="p">[</span><span class="s">'yobs'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">elpd_t</span><span class="p">,</span> <span class="n">elpd_norm</span><span class="o">-</span><span class="n">elpd_t</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'x'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'ELPD diff'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'t ELPD'</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/model_averaging_cont/plot_elpd_mine.webp" alt="" /></p>

<p>This plot contains much more information with respect to the previous
one, and it tells us that the Student model 
does a better job in reproducing both the points
with very close to the center (those with very high ELPD)
and those far away to the center (those with very small ELPD),
while the normal model only focuses on the intermediate points.</p>

<p>The ELPD has an issue which is not present when using the LOO:
we are using our data twice. For this reason, the LOO method
is generally preferred over any method which relies on the simple
log density of the posterior.</p>

<h2 id="some-warning">Some warning</h2>

<p>While the LOO method gives reasonable results when the number of variables
is not too high, it is known that all the information criteria tend to overfit
when the number of variables grows too much (see Gronau <em>et al.</em>).</p>

<p>Moreover, you should always consider what you need to do with your model.
If you simply need to get the best possible prediction out of your model,
you should probably go for the most general model
and integrate over all the uncertainties.</p>

<p>You should also keep in mind that, if you have nested models, that is
models where one model is a special case of the other, it is generally
recommended by the Bayesian scientific community to stick to the more
general one, regardless on what your metrics tell you,
at least unless you have a reason to put some constraints to your model.</p>

<p>An in-depth discussion about this topic can be found in <a href="https://www.youtube.com/watch?v=D0kVMie93Yk&amp;list=PLBqnAso5Dy7O0IVoVn2b-WtetXQk5CDk6&amp;index=18">Aki Vehtari’s course
on YouTube</a>
(lectures 9.1-9.3).</p>

<h2 id="conclusions">Conclusions</h2>

<p>We discussed how to use the Leave One Out method to compare two models
and how should we read the most relevant plots related to this criterion.
We also discussed some limitation of this model.</p>

<h2 id="recommended-readings">Recommended readings</h2>

<ul>
  <li><cite> Gronau, Q.F., Wagenmakers, EJ. Limitations of Bayesian Leave-One-Out Cross-Validation for Model Selection. Comput Brain Behav 2, 1–11 (2019). https://doi.org/10.1007/s42113-018-0011-7</cite></li>
  <li><cite> Vehtari A., Gelman A., Gabry J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. https://arxiv.org/abs/1507.04544v5</cite></li>
  <li><cite> Navarro, D.J. Between the Devil and the Deep Blue Sea: Tensions Between Scientific Judgement and Statistical Model Selection. Comput Brain Behav 2, 28–34 (2019). https://doi.org/10.1007/s42113-018-0019-z </cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">pytensor</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>
<div class="code">
Last updated: Wed Nov 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.7
<br />
IPython version      : 8.24.0
<br />

<br />
xarray  : 2024.9.0
<br />
pytensor: 2.25.5
<br />
numpyro : 0.15.0
<br />
jax     : 0.4.28
<br />
jaxlib  : 0.4.28
<br />

<br />
numpy     : 1.26.4
<br />
arviz     : 0.20.0
<br />
pandas    : 2.2.3
<br />
seaborn   : 0.13.2
<br />
pymc      : 5.17.0
<br />
yfinance  : 0.2.40
<br />
matplotlib: 3.9.2
<br />

<br />
Watermark: 2.4.3
<br />
</div>

  </div><a class="u-url" href="/statistics/model_averaging_cont" hidden></a>

  <br>
  <div id='autograph'>
          Stippe Oct 10, 2024

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>


  <div class="wrapper">
          <div class='footer-text'>
                  Opinions are mine, mistakes too, and if you find any feel free to report it via mail or via <del>Twitter</del> <a href="https://bsky.app/profile/stippe87.bsky.social">Bluesky</a>
                  <br>
                  Most of the material in the statistics section is an adaptation to Python of some pre-existing model.
                  <br>
                  I have tried to provide the necessary credits, but if you think that a relevant contribution is missing, please let me know.
                  <br>
                  No AI were harmed in creating this blog.
                  <br>
                  This is a blog, not a bakery: there are no cookies here!
                  <br>
                  The top image has been generated with a modified version Dan Gries' code, available at <a href="http://rectangleworld.com/blog/archives/538">http://rectangleworld.com</a>.
          </div>
          <br>

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Data-perspectives</li>
          <li><a class="u-email" href="mailto:dataperspectivesblog@gmail.com">dataperspectivesblog@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://stackoverflow.com/users/11065831/stefano" target="_blank" title="stackoverflow">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#stackoverflow"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://github.com/thestippe/" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://vis.social/@thestippe" target="_blank" title="mastodon">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#mastodon"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

  <script src="/docs/assets/javascript/scroll.js">
  </script>

</html>

