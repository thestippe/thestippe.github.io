<!DOCTYPE html>
<html lang="en"><head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-W9G73E5P44"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-W9G73E5P44');
</script>
          <link rel="icon" 
                type="image/png" 
                href="/docs/assets/images/dp_icon.png">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Common discrete probabilities | Data Perspectives</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Common discrete probabilities" />
<meta name="author" content="Data-perspectives" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Derivation of some useful one-dimensional discrete probabilities" />
<meta property="og:description" content="Derivation of some useful one-dimensional discrete probabilities" />
<link rel="canonical" href="http://localhost:4000/statistics/common_discrete_probabilities" />
<meta property="og:url" content="http://localhost:4000/statistics/common_discrete_probabilities" />
<meta property="og:site_name" content="Data Perspectives" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-01-08T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Common discrete probabilities" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Data-perspectives"},"dateModified":"2024-01-08T00:00:00+00:00","datePublished":"2024-01-08T00:00:00+00:00","description":"Derivation of some useful one-dimensional discrete probabilities","headline":"Common discrete probabilities","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/statistics/common_discrete_probabilities"},"url":"http://localhost:4000/statistics/common_discrete_probabilities"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Perspectives" />


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
<a rel="me" href="https://vis.social/@thestippe" style="display: none;">Mastodon</a>
<link rel="manifest" href="manifest.json">
</head>
<body>

        <div id='upperBar'><header class="site-header">

        <div id='upperBarr'>
        <script src="https://d3js.org/d3.v7.js"></script>
                <div class="wrapper" style="display:flex;"><ul hidden='hidden' id="postList"><li>
                        How does MCMC works;/statistics/mcmc_intro;1
                </li><li>
                        Introduction to bayesian statistics;/statistics/bayes_intro;2
                </li><li>
                        The autoregressive model;/statistics/autoregressive;3
                </li><li>
                        Introduction to time series modelling;/statistics/time_series;4
                </li><li>
                        Synthetic control;/statistics/synthetic_control;5
                </li><li>
                        Regression discontinuity ;/statistics/discontinuity_regression;6
                </li><li>
                        Difference in difference;/statistics/difference_in_differences;7
                </li><li>
                        Instrumental variable regression;/statistics/instrumental_variable;8
                </li><li>
                        Randomized controlled trials;/statistics/randomized;9
                </li><li>
                        Causal inference;/statistics/causal_intro;10
                </li><li>
                        Random models and mixed models;/statistics/random_models;11
                </li><li>
                        Introduction to Extreme Values theory;/statistics/extreme_intro;12
                </li><li>
                        Application of survival analysis 1;/statistics/survival_example;13
                </li><li>
                        Introduction to survival analysis;/statistics/survival_analysis;14
                </li><li>
                        Hierarchical models and meta-analysis;/statistics/hierarchical_metaanalysis;15
                </li><li>
                        Hierarchical models;/statistics/hierarchical_models;16
                </li><li>
                        Poisson regression;/statistics/poisson_regression;17
                </li><li>
                        Logistic regression;/statistics/logistic_regression;18
                </li><li>
                        Robust linear regression;/statistics/robust_regression;19
                </li><li>
                        Introduction to the linear regression;/statistics/regression;20
                </li><li>
                        Model comparison;/statistics/model_averaging;21
                </li><li>
                        Predictive checks;/statistics/predictive_checks;22
                </li><li>
                        Trace inspection;/statistics/trace_inspection;23
                </li><li>
                        Introduction to the Bayesian workflow;/statistics/bayesian_workflow;24
                </li><li>
                        Mixture models;/statistics/mixture;25
                </li><li>
                        Multidimensional distributions;/statistics/categories;26
                </li><li>
                        Exponential model, gaussian model and their evolutions;/statistics/reals;27
                </li><li>
                        The Negative Binomial model;/statistics/negbin;28
                </li><li>
                        The Poisson model;/statistics/poisson;29
                </li><li>
                        The Beta-Binomial model;/statistics/betabin;30
                </li><li>
                        Common continuous probabilities;/statistics/common_continuous_probabilities;31
                </li><li>
                        Common discrete probabilities;/statistics/common_discrete_probabilities;32
                </li><li>
                        Design tricks;/dataviz/design-introduction;33
                </li><li>
                        How to choose a color map;/dataviz/palettes-introduction;34
                </li><li>
                        Introduction to color perception;/dataviz/color-introduction;35
                </li><li>
                        Drawing is redrawing;/dataviz/gender-economist;36
                </li><li>
                        Visual queries;/dataviz/visual-queries;37
                </li><li>
                        The Gestalt principles;/dataviz/gestalt;38
                </li><li>
                        Channel effectiveness;/dataviz/effectiveness;39
                </li><li>
                        Evolutions of the line chart;/dataviz/linechart-evolution;40
                </li><li>
                        Beyond the 1D scatterplot;/dataviz/scatterplot-evolution;41
                </li><li>
                        Perception;/dataviz/perception;42
                </li><li>
                        Fundamental charts;/dataviz/fundamental-charts;43
                </li><li>
                        Marks and channels;/dataviz/marks-channels;44
                </li><li>
                        Data abstraction;/dataviz/data-types;45
                </li><li>
                        Data visualization;/dataviz/dataviz;46
                </li></ul>
                        <div style="display:flex">
                                <a href="/"><img class="site-masthead" src="/docs/assets/images/logo_dp.png" alt="Data Perspectives" id="logo" /></a><div id='searchNav' style="flex;">
                                        <input type="search" id="search_0" class="searchBar" onkeydown="searchText()" placeholder="Search">
                                </div>

                                <div hidden='hidden' id="search_focus">0</div>



                        </div><nav class="site-nav" style="display:flex;">
                                <input type="checkbox" id="nav-trigger" class="nav-trigger" />
                                <label for="nav-trigger">
                                        <span class="menu-icon">
                                                <svg viewBox="0 0 18 15" width="18px" height="15px">
                                                        <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
                                                </svg>
                                        </span>
                                </label>

                                <div class="trigger"><a  class="page-link" href="/about">About me</a><a  class="page-link" href="/links">Resources</a>
                                <a href="/statistics/" class="page-link">Up</a>
                                
                                </div>
                        </nav>
                  <a href="/statistics/common_continuous_probabilities" class="next">&#8250;</a>
                  
        </div>


        <script src="/docs/assets/javascript/search.js">
        </script>
                </div>

</header>
<div class="progress-container">
    <div class="progress-bar" id="myBar"></div>
    </div>
        </div>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
          <div id='topPage'></div>
        <a href='/index'>
<img src="/docs/assets/images/background_resized.webp" alt="backround" style="margin:auto;display:block;width:1200px">

</a>
    <h1 class="post-title p-name" itemprop="name headline">Common discrete probabilities</h1>
    <p class="post-meta"><time class="dt-published" datetime="2024-01-08T00:00:00+00:00" itemprop="datePublished">
        Jan 8, 2024
      </time></p>
    <p class="post-meta"> Reading time: <span class="reading-time" title="Estimated read time">
  
  5&prime;
</span>
</p>
  </header>
  <br>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In order to be able to build some model, it is important to 
know some elementary probability distribution, together
with the underlying process that generated it.
We will now discuss some of the most important discrete probability distributions.
We will also provide the lowest order moments,
as they can be really useful when trying to perform rough estimates.</p>

<h2 id="bernoulli-distribution">Bernoulli distribution</h2>

<p>Let us consider an experiment which may have two possible outcomes.
The outcomes are traditionally named <em>success</em> and <em>fail</em>,
so 
\(\Omega = \left\{fail, success\right\}\)
For this kind of experiment it is convenient to define
the random variable</p>

\[X(fail) = 0, X(success) = 1\]

<p>The most general pmf for this random variable is the <strong>Bernoulli</strong>
distribution</p>

\[p(x | p) = 
\begin{cases}
p &amp; x = 1\\
q = 1-p &amp; x = 0 \\
\end{cases}\]

<p>where our parameter $p$ may take any value in $[0, 1]\,.$</p>

<p>Notice that the general formula for $p(x | p)$ defines a family of distributions
rather than a distribution unless the parameter $p$ is fixed to a particular value.
It is customary however to name it as a distribution, and we will usually stick to this convention.</p>

\[\mathbb{E}[X] = \sum_{x=0, 1} x p(x | p) = p\]

\[Var[X] = \sum_{x=0, 1} (x-p)^2 p(x | p) = p^2(1-p) + (1-p)^2 p = p(1-p)\]

<h2 id="binomial-distribution">Binomial distribution</h2>

<p>Let us now consider $n$ independent Bernoulli trials $X_1,\dots,X_n$,
the Binomial distribution describes the probability to
obtain $k$ successes out of the $n$ trials.
In order to derive it, we observe that, thanks to our choice of the Bernoulli
random variables $X_i\,,$ we can consider
\(X = \sum_{i=1}^n X_i \,.\)</p>

<p>The probability that $X_1=1,\dots,X_k=1,X_{k+1}=0,\dots,X_n=0\,.$
is given by $p^k(1-p)^{n-k}\,.$
We are however not interested into the above probability,
but in the probability that <em>any</em> group of $k$ variables
takes value 1 while the remaining takes the value 0,
and this implies that we must multiply the above probability by the
number of possible groups of $k$ elements out of $n$ objects,
namely by \(C(n, k) = \binom{n}{k} = \frac{n!}{(n-k)!k!}\,.\)</p>

\[p(k | n, p) = \binom{n}{k} p^k(1-p)^{n-k}\]

<p>where $p \in [0, 1]\,,$ $n=1,2,\dots$ and $k=0,\dots,n$</p>

<p>Since the Binomial distribution is the sum of $n$ independent Bernoulli distribution,
it is straightforward to get</p>

\[\mathbb{E}[X] = n \mathbb{E}[X_1] = n p\]

<p>and</p>

\[Var[X] = n Var[X_1] = n p (1-p)\]

<h2 id="negative-binomial-distribution">Negative binomial distribution</h2>

<p>The negative binomial distribution gives the probability
to observe $k$ failures before observing $r$ (fixed) successes.
By assumption, the $k+r$-th event is a success, and it happens with probability $p$.
As before we can show that the probability to observe
$k$ failures out of $r+k-1$ events is given by $\binom{r+k-1}{k}(1-p)^{k}p^{r-1}$
so</p>

\[p(k | r, p) = \binom{r+k-1}{k} (1-p)^k p^r\]

<p>In this case we have $p\in [0, 1]\,,$ $r=1,2,\dots$ and $k=0,1,\dots\,.$</p>

<p>The negative binomial distribution with $r=1$ is named the <strong>geometric</strong>
distribution.
The geometric distribution turns out to be very useful,
as you can think about the general negative binomial distribution as a sum of r
geometric distribution.
If $Y_1$ is the waiting time for the first success, $Y_2$ the one for the second success,\dots,
$Y_r$ the waiting time for the $r$-th success, then you can consider the total waiting
time
\(X = Y_1 + Y_2 + \dots + Y_r\,.\)</p>

<p>We have that</p>

\[\begin{align}
\mathbb{E}[Y_1] &amp;= \sum_{k=0}^\infty k p (1-p)^k = (1-q) \sum_{k=0}^\infty k q^k\\
&amp;
= (1-q) q \sum_{k=0}^\infty k q^{k-1}
\\ &amp;
= (1-q) q \partial_q \sum_{k=0}^\infty q^k
\\ &amp;
= (1-q) q \partial_q (1-q)^{-1} 
\\ &amp;
= (1-q) q (1-q)^{-2}
\\ &amp;
= \frac{q}{1-q} 
\\ &amp;
= \frac{1-p}{p}
\end{align}\]

<p>In the same way we get</p>

\[Var[Y_1] = \frac{1-p}{p^2}\]

<p>So</p>

\[\mathbb{E}[X] = r \frac{1-p}{p}\]

<p>and</p>

\[Var[X] = r\frac{(1-p)}{p^2}\]

<h2 id="poisson-distribution">Poisson distribution</h2>

<p>Let us consider an experiment,
and let us assume that, in a time $\delta t$, on average,
 $\mu $ events happen.</p>

<p>We assume that each event is independent on the others,
the probability that the site gets $k$ events
must be given by:</p>

\[P(X=k) \propto \frac{\mu^k}{k!}\]

<p>where the denominator has been introduce
since we donâ€™t care the order of the events.
We can get the overall normalization constant by normalizing the probability to one</p>

\[C \sum_{k=0}^\infty \frac{\mu^k}{k!} = C e^{\mu}= 1\]

<p>so</p>

\[p(k | \mu) =e^{-\mu } \frac{ \mu ^k}{k!}\]

<p>with $\mu &gt; 0$ and $k=0,1,\dots\,.$</p>

<p>We have</p>

\[\begin{align}
\mathbb{E}[X] &amp; = 
e^{-\mu} \sum_{k=0}^\infty k \frac{\mu^k}{k!}
\\ &amp;
=e^{-\mu} \sum_{k=1}^\infty k \frac{\mu^k}{k!}
\\ &amp;
=e^{-\mu} \sum_{k=1}^\infty \frac{\mu^k}{(k-1)!}
\\ &amp;
=\mu e^{-\mu} \sum_{k=1}^\infty \frac{\mu^{k-1}}{(k-1)!}
\\ &amp;
=\mu e^{-\mu} \sum_{k=0}^\infty \frac{\mu^{k}}{k!}
\\ &amp;
= \mu
\end{align}\]

<p>Analogously we can obtain</p>

\[Var[X] = \mathbb{E}[X^2] - \mathbb{E}[X]^2 = \mu\]

<h2 id="discrete-uniform">Discrete uniform</h2>

<p>The discrete uniform distribution is the one which assigns equal
probability to $n$ possible outcomes.
If we define the random variable $X=1,\dots,n\,,$ we immediately have</p>

\[p(x) = \frac{1}{n}\]

<p>Also in this case the computation of the mean is quite straightforward</p>

\[\mathbb{E}[X] = \sum_{k=1}^n \frac{k}{n} =\frac{1}{n} \frac{n (n+1)}{2} =\frac{n+1}{2}\]

<p>The computation of the variance is a little bit more tedious, but at the end the result is</p>

\[Var[X] = \mathbb{E}[X^2] - \mathbb{E}[X]^2 = \frac{(n+1)(2n+1)}{6} - (\frac{n+1}{2})^2 = \frac{n^2-1}{12}\]

<h2 id="categorical-distribution">Categorical distribution</h2>

<p>The categorical distribution is the most general distribution
of $n$ events.
Given the random variable $X=0,\,\dots,n-1\,,$ 
and a vector \((p_0,\dots,p_{n-1})\) with $p_i \in [0, 1]$ and $\sum_{i=0}^{n-1} p_i = 1$
we assign</p>

\[p(x=i | p_0,\dots,p_{n-1}) = p_i\]

<p>The categorical distribution corresponds to the Bernoulli distribution
for $n=2\,,$ while if $p_0=p_1=\dots=p_{n-1}=\frac{1}{n}$ it reduces
to the discrete uniform distribution.</p>

<p>In this case there is no simple formula for the expected values.</p>

<h2 id="conclusions">Conclusions</h2>
<p>We have discussed some of the most common discrete distributions.
In the next post we will discuss some relevant continuous distribution.</p>

  </div><a class="u-url" href="/statistics/common_discrete_probabilities" hidden></a>

  <div>
          
          
  </div>

  <br>
  <div id='autograph'>
          Stippe Jan 8, 2024

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>


  <div class="wrapper">
          <div class='footer-text'>

                  No AI were harmed in creating this blog.
                  <br>
                  This is a blog, not a bakery: there are no cookies here!
                  <br>
                  The top image has been generated with a modified version Dan Gries' code, available at <a href="http://rectangleworld.com/blog/archives/538">http://rectangleworld.com</a>.
          </div>
          <br>

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Data-perspectives</li>
          <li><a class="u-email" href="mailto:dataperspectivesblog@gmail.com">dataperspectivesblog@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://twitter.com/SteffPy" target="_blank" title="twitter">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://stackoverflow.com/users/11065831/stefano" target="_blank" title="stackoverflow">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#stackoverflow"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://github.com/thestippe/" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://vis.social/@thestippe" target="_blank" title="mastodon">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#mastodon"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

  <script src="/docs/assets/javascript/scroll.js">
  </script>

</html>

