<!DOCTYPE html>
<html lang="en"><head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-W9G73E5P44"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-W9G73E5P44');
</script>
          <link rel="icon" 
                type="image/png" 
                href="/docs/assets/images/dp_icon.png">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Robust linear regression | Data Perspectives</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Robust linear regression" />
<meta name="author" content="Data-perspectives" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Reducing sensitivity to large deviations" />
<meta property="og:description" content="Reducing sensitivity to large deviations" />
<link rel="canonical" href="http://localhost:4000/statistics/robust_regression" />
<meta property="og:url" content="http://localhost:4000/statistics/robust_regression" />
<meta property="og:site_name" content="Data Perspectives" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-11-06T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Robust linear regression" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Data-perspectives"},"dateModified":"2024-11-06T00:00:00+00:00","datePublished":"2024-11-06T00:00:00+00:00","description":"Reducing sensitivity to large deviations","headline":"Robust linear regression","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/statistics/robust_regression"},"url":"http://localhost:4000/statistics/robust_regression"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Perspectives" />


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
<a rel="me" href="https://vis.social/@thestippe" style="display: none;">Mastodon</a>
<link rel="manifest" href="manifest.json">
</head>
<body>

        <div id='upperBar'><header class="site-header">

        <div id='upperBarr'>
        <script src="https://d3js.org/d3.v7.js"></script>
                <div class="wrapper" style="display:flex;">

                
                
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                    
                    
                    
                    
                    

        <ul hidden='hidden' id="postList"><li>
                        Random models and mixed models;/statistics/random_models;1
                </li><li>
                        Hierarchical models and meta-analysis;/statistics/hierarchical_metaanalysis;2
                </li><li>
                        Hierarchical models;/statistics/hierarchical_models;3
                </li><li>
                        Poisson regression;/statistics/poisson_regression;4
                </li><li>
                        Logistic regression;/statistics/logistic_regression;5
                </li><li>
                        Robust linear regression;/statistics/robust_regression;6
                </li><li>
                        Multi-linear regression;/statistics/multivariate_regression;7
                </li><li>
                        Linear regression with binary input;/statistics/regression_binary_input;8
                </li><li>
                        Horseshoe priors;/statistics/horseshoe;9
                </li><li>
                        Introduction to the linear regression;/statistics/regression;10
                </li><li>
                        MRP;/statistics/mrp;11
                </li><li>
                        Model comparison, cont.;/statistics/model_averaging_cont;12
                </li><li>
                        Model comparison;/statistics/model_averaging;13
                </li><li>
                        Application of the Lotka-Volterra model;/statistics/lotka_volterra;14
                </li><li>
                        Differential equations;/statistics/ode;15
                </li><li>
                        Introduction to GIS;/gis/gis_intro;16
                </li><li>
                        Re-parametrizing your model;/statistics/reparametrization;17
                </li><li>
                        Predictive checks;/statistics/predictive_checks;18
                </li><li>
                        Trace inspection;/statistics/trace_inspection;19
                </li><li>
                        Introduction to the Bayesian workflow;/statistics/bayesian_workflow;20
                </li><li>
                        Mixture models;/statistics/mixture;21
                </li><li>
                        Multidimensional distributions;/statistics/categories;22
                </li><li>
                        Dirichlet Process Mixture Models;/statistics/dp;23
                </li><li>
                        The Gaussian model;/statistics/reals;24
                </li><li>
                        Bayesian Additive Regression Trees;/statistics/bart;25
                </li><li>
                        Bonus: counting animals in a park;/statistics/hypergeom;26
                </li><li>
                        Splines;/statistics/spline;27
                </li><li>
                        The Negative Binomial model;/statistics/negbin;28
                </li><li>
                        Gaussian processes regression;/statistics/gp_example;29
                </li><li>
                        Gaussian processes;/statistics/gp;30
                </li><li>
                        The Poisson model;/statistics/poisson;31
                </li><li>
                        Nonparametric models;/statistics/nonparametric_intro;32
                </li><li>
                        Time series;/statistics/time_series;33
                </li><li>
                        The Beta-Binomial model;/statistics/betabin;34
                </li><li>
                        Section introduction;/statistics/simple_models_intro;35
                </li><li>
                        Structural time series;/statistics/structural_time_series;36
                </li><li>
                        Some notation about probability;/statistics/probability_reminder;37
                </li><li>
                        Synthetic control;/statistics/synthetic_control;38
                </li><li>
                        How does MCMC works;/statistics/mcmc_intro;39
                </li><li>
                        Regression discontinuity design;/statistics/rdd;40
                </li><li>
                        Introduction to Bayesian inference;/statistics/bayes_intro;41
                </li><li>
                        An overview to statistics;/statistics/preface;42
                </li><li>
                        Difference in difference;/statistics/difference_in_differences;43
                </li><li>
                        Instrumental variable regression;/statistics/instrumental_variable;44
                </li><li>
                        Randomized controlled trials;/statistics/randomized;45
                </li><li>
                        Causal inference and Bayesian networks;/statistics/causal_intro_2;46
                </li><li>
                        Causal inference;/statistics/causal_intro;47
                </li><li>
                        Experiment analysis with many blocking variables;/statistics/experiment_design_cont;48
                </li><li>
                        Experiment analysis;/statistics/experiment_design;49
                </li><li>
                        Introduction to Extreme Values theory;/statistics/extreme_intro;50
                </li><li>
                        Application of survival analysis with discrete times;/statistics/survival_example_2;51
                </li><li>
                        Application of survival analysis 1;/statistics/survival_example;52
                </li><li>
                        Introduction to survival analysis;/statistics/survival_analysis;53
                </li><li>
                        Drawing geographic maps;/dataviz/geography;54
                </li><li>
                        The Gestalt principles;/dataviz/gestalt;55
                </li><li>
                        Design tricks;/dataviz/design-introduction;56
                </li><li>
                        How to choose a color map;/dataviz/palettes-introduction;57
                </li><li>
                        Introduction to color perception;/dataviz/color-introduction;58
                </li><li>
                        Drawing is redrawing;/dataviz/gender-economist;59
                </li><li>
                        Visual queries;/dataviz/visual-queries;60
                </li><li>
                        Channel effectiveness;/dataviz/effectiveness;61
                </li><li>
                        Evolutions of the line chart;/dataviz/linechart-evolution;62
                </li><li>
                        Beyond the 1D scatterplot;/dataviz/scatterplot-evolution;63
                </li><li>
                        Perception;/dataviz/perception;64
                </li><li>
                        Fundamental charts;/dataviz/fundamental-charts;65
                </li><li>
                        Marks and channels;/dataviz/marks-channels;66
                </li><li>
                        Data abstraction;/dataviz/data-types;67
                </li><li>
                        Data visualization;/dataviz/dataviz;68
                </li></ul>
                        <div style="display:flex">
                  <a href="/statistics/multivariate_regression" class="prev">&#8249;</a>
                  
                                <a href="/"><img class="site-masthead" src="/docs/assets/images/logo_dp.png" alt="Data Perspectives" id="logo" /></a><div id='searchNav' style="flex;">
                                        <input type="search" id="search_0" class="searchBar" onkeydown="searchText()" placeholder="Search">
                                </div>

                                <div hidden='hidden' id="search_focus">0</div>



                        </div><nav class="site-nav" style="display:flex;">
                                <input type="checkbox" id="nav-trigger" class="nav-trigger" />
                                <label for="nav-trigger">
                                        <span class="menu-icon">
                                                <svg viewBox="0 0 18 15" width="18px" height="15px">
                                                        <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
                                                </svg>
                                        </span>
                                </label>

                                <div class="trigger"><a  class="page-link" href="/about">About me</a><a  class="page-link" href="/links">Resources</a>
                                <a href="/statistics/" class="page-link">Up</a>
                                
                                </div>
                        </nav>
                  <a href="/statistics/logistic_regression" class="next">&#8250;</a>
                  
        </div>


        <script src="/docs/assets/javascript/search.js">
        </script>
                </div>

</header>
<div class="progress-container">
    <div class="progress-bar" id="myBar"></div>
    </div>
        </div>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
          <div id='topPage'></div>
        <a href='/index'>
<img src="/docs/assets/images/background_resized.webp" alt="backround" style="margin:auto;display:block;width:1200px">

</a>
    <h1 class="post-title p-name" itemprop="name headline">Robust linear regression</h1>
    <p class="post-meta"><time class="dt-published" datetime="2024-11-06T00:00:00+00:00" itemprop="datePublished">
        Nov 6, 2024
      </time></p>
    <p class="post-meta"> Reading time: <span class="reading-time" title="Estimated read time">
  
  14&prime;
</span>
</p>
  </header>
  <br>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In some case your data may be not good enough to provide you reliable estimates with normal linear regression,
and this is the case of the conclusions drawn from
<a href="https://www.cambridge.org/core/journals/american-political-science-review/article/abs/political-institutions-and-voter-turnout-in-the-industrial-democracies/D6725BBF93F2F90F03A69B0794728BF7">this</a>
article, where the author concludes that there is a significant correlation between
the voter turnout in a country and its average income inequality.
This example is a classical example of misleading result of a regression,
where the author does not provide a plot of the data, taken from
<a href="https://www.google.it/books/edition/Data_Visualization/3XOYDwAAQBAJ?hl=it&amp;gbpv=1&amp;dq=Data+visualization,+a+practical+introduction&amp;printsec=frontcover">Healy, “Data visualization, a practical introduction”</a>.
The data below is extracted the data from the figure of Healy’s book.
South Africa corresponds to the last point.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">turnout</th>
      <th style="text-align: right">inequality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0.85822</td>
      <td style="text-align: right">1.95745</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: right">0.837104</td>
      <td style="text-align: right">1.95745</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">0.822021</td>
      <td style="text-align: right">2.41135</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">0.87632</td>
      <td style="text-align: right">2.76596</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">0.901961</td>
      <td style="text-align: right">2.95035</td>
    </tr>
    <tr>
      <td style="text-align: right">5</td>
      <td style="text-align: right">0.776772</td>
      <td style="text-align: right">3.21986</td>
    </tr>
    <tr>
      <td style="text-align: right">6</td>
      <td style="text-align: right">0.72549</td>
      <td style="text-align: right">3.14894</td>
    </tr>
    <tr>
      <td style="text-align: right">7</td>
      <td style="text-align: right">0.72549</td>
      <td style="text-align: right">2.92199</td>
    </tr>
    <tr>
      <td style="text-align: right">8</td>
      <td style="text-align: right">0.61991</td>
      <td style="text-align: right">2.93617</td>
    </tr>
    <tr>
      <td style="text-align: right">9</td>
      <td style="text-align: right">0.574661</td>
      <td style="text-align: right">2.31206</td>
    </tr>
    <tr>
      <td style="text-align: right">10</td>
      <td style="text-align: right">0.880845</td>
      <td style="text-align: right">3.60284</td>
    </tr>
    <tr>
      <td style="text-align: right">11</td>
      <td style="text-align: right">0.803922</td>
      <td style="text-align: right">3.5461</td>
    </tr>
    <tr>
      <td style="text-align: right">12</td>
      <td style="text-align: right">0.778281</td>
      <td style="text-align: right">3.47518</td>
    </tr>
    <tr>
      <td style="text-align: right">13</td>
      <td style="text-align: right">0.739065</td>
      <td style="text-align: right">3.68794</td>
    </tr>
    <tr>
      <td style="text-align: right">14</td>
      <td style="text-align: right">0.819005</td>
      <td style="text-align: right">4.41135</td>
    </tr>
    <tr>
      <td style="text-align: right">15</td>
      <td style="text-align: right">0.645551</td>
      <td style="text-align: right">3.91489</td>
    </tr>
    <tr>
      <td style="text-align: right">16</td>
      <td style="text-align: right">0.669683</td>
      <td style="text-align: right">5.64539</td>
    </tr>
    <tr>
      <td style="text-align: right">17</td>
      <td style="text-align: right">0.14178</td>
      <td style="text-align: right">9.30496</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pymc.sampling_jax</span> <span class="k">as</span> <span class="n">pmjax</span>

<span class="n">df_turnout</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/inequality.csv'</span><span class="p">)</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">df_turnout</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/inequality_pairplot.webp" alt="The dataset pairplot" /></p>

<p>By simply plotting the data we can clearly see that there is one point,
the South Africa, which is far away from the other,
and this may have a huge impact on the fit.
Let us see this, and how one may avoid this kind of error.</p>

<h2 id="the-normal-linear-regression">The normal linear regression</h2>

<p>Let us start by assuming that the inequality is distributed
according to a normal linear model,
analogous to the one already discussed in the <a href="/linear_regression">regression post</a>.</p>

\[Y \sim \mathcal{N}(\mu, \sigma)\]

<p>where</p>

\[\mu = \alpha + \beta X\]

<p>We will assume that the precision $\tau = 1/\sigma$ is distributed according to a Half Normal
distribution. Since the inequality goes from 0 to 10, assuming a
standard deviation of $5$ for $\tau$ should be sufficient.
On the other hand, we will make the quite generous assumption that</p>

\[\alpha \sim \mathcal{N}(0, 20)\]

\[\beta \sim \mathcal{N}(0, 20)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_norm</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'tau'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'turnout'</span><span class="p">].</span><span class="n">values</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'inequality'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_norm</span><span class="p">:</span>
    <span class="n">idata_norm</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span>
                           <span class="n">idata_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s">'log_likelihood'</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_norm</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/trace_norm.webp" alt="The trace of the normal model" /></p>

<p>The trace doesn’t show any relevant issue, and for our purposes it is
sufficient this check.
Let us check our fit</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_plt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_norm</span><span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y_pred'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">x_plt</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>
    

<span class="k">with</span> <span class="n">model_norm</span><span class="p">:</span>
    <span class="n">idata_norm</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata_norm</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y'</span><span class="p">,</span> <span class="s">'y_pred'</span><span class="p">],</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">))</span>

    
    
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">idata_norm</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">idata_norm</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
                <span class="n">idata_norm</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'turnout'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">df_turnout</span><span class="p">[</span><span class="s">'inequality'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/ppc_norm.webp" alt="The posterior preditcive distribution of our model" /></p>

<p>The error bands correctly reproduce almost all the data. However,
since the South Africa is far away from the other countries,
it may happen that its behavior strongly influences the fit.</p>

<p>Let us now use a more robust model.
In order to make it more robust, which in this context means
less sensitive to isolated data, let us take a t-Student likelihood
instead of a normal one.</p>

<p>We will leave the parameters $\alpha\,, \beta$ and $\tau = \frac{1}{\sigma}$
unchanged, but we must choose a prior for the number of degrees of
freedom $\nu\,.$</p>

<p>We want a robust estimate, so we want a prior with a small
number of degrees of freedom. However, $\nu \approx 0$
can be hard to handle from a numeric perspective,
since the resulting distribution decreases very slowly 
as one steps away from the peak.
For the above reason, we choose a Gamma prior with $\alpha=4$
and $\beta=2\,.$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_robust</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'tau'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Gamma</span><span class="p">(</span><span class="s">'nu'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'turnout'</span><span class="p">].</span><span class="n">values</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'inequality'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">tau</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_robust</span><span class="p">:</span>
    <span class="n">idata_robust</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> 
                             <span class="n">idata_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s">'log_likelihood'</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span> 
                             <span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_robust</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/trace_robust.webp" alt="The trace of the robust model" /></p>

<p>The trace doesn’t show relevant issues, so we can compute the posterior predictive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model_robust</span><span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s">'y_pred'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">x_plt</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">tau</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_robust</span><span class="p">:</span>
    <span class="n">idata_robust</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata_robust</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y'</span><span class="p">,</span> <span class="s">'y_pred'</span><span class="p">],</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">))</span>


<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">idata_robust</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">median</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">idata_robust</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
                <span class="n">idata_robust</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'turnout'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">df_turnout</span><span class="p">[</span><span class="s">'inequality'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/ppc_robust.webp" alt="The PPC of the robust model" /></p>

<p>This distribution does a better job in reproducing the data, but
it tells a very different story from the normal model.</p>

<p>While in fact in the above model an increase of the turnout
translated into a reduction of the average inequality,
with this robust model this conclusion does not appear so clearly.</p>

<p>Let us try and see what does the LOO can tell us.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_compare</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">compare</span><span class="p">({</span><span class="s">'Normal model'</span><span class="p">:</span> <span class="n">idata_norm</span><span class="p">,</span> <span class="s">'Robust model'</span><span class="p">:</span> <span class="n">idata_robust</span><span class="p">})</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_compare</span><span class="p">(</span><span class="n">df_compare</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/loo.webp" alt="The plot of the LOO cross-validation" /></p>

<p>The LOO is slightly better for the normal model,
they are however very similar. Let us try and understand why.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_compare</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right">rank</th>
      <th style="text-align: right">elpd_loo</th>
      <th style="text-align: right">p_loo</th>
      <th style="text-align: right">elpd_diff</th>
      <th style="text-align: right">weight</th>
      <th style="text-align: right">se</th>
      <th style="text-align: right">dse</th>
      <th style="text-align: left">warning</th>
      <th style="text-align: left">scale</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Normal model</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">-30.9967</td>
      <td style="text-align: right">4.92972</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0.889385</td>
      <td style="text-align: right">4.39811</td>
      <td style="text-align: right">0</td>
      <td style="text-align: left">True</td>
      <td style="text-align: left">log</td>
    </tr>
    <tr>
      <td style="text-align: left">Robust model</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">-32.3574</td>
      <td style="text-align: right">6.88334</td>
      <td style="text-align: right">1.36077</td>
      <td style="text-align: right">0.110615</td>
      <td style="text-align: right">4.66233</td>
      <td style="text-align: right">1.855</td>
      <td style="text-align: left">False</td>
      <td style="text-align: left">log</td>
    </tr>
  </tbody>
</table>

<p>The difference is $1.65\,,$ and the difference due to the number
of the degrees of freedom is the difference of the $p_{loo}\,,$
which is approximately 2.2, so the entire preference is due
to the lower number of degrees of freedom of the normal distribution.</p>

<p>We can see, however, that the LOO estimate for the normal
model has a warning. This generally happens because the ELPD
estimate is not exact, and it’s only reliable when 
removing one point does not affect too much log predictive density.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loo_normal</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">loo</span><span class="p">(</span><span class="n">idata_norm</span><span class="p">)</span>

<span class="n">loo_normal</span>
</code></pre></div></div>

<div class="code">
Computed from 16000 posterior samples and 18 observations log-likelihood matrix.
<br />

<br />
         Estimate       SE
<br />
elpd_loo   -31.00     4.40
<br />
p_loo        4.93        -
<br />

<br />
There has been a warning during the calculation. Please check the results.
<br />
------
<br />

<br />
Pareto k diagnostic values:
<br />
                         Count   Pct.
<br />
(-Inf, 0.5]   (good)       16   88.9%
<br />
 (0.5, 0.7]   (ok)          1    5.6%
<br />
   (0.7, 1]   (bad)         0    0.0%
<br />
   (1, Inf)   (very bad)    1    5.6%
<br />
</div>

<p>There are two points which strongly affect our parameters,
and one reasonable assumption is that one of those is the South Africa.</p>

<p>Let us try and see what does it happen once we remove it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_norm_red</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'alpha'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'beta'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'tau'</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'turnout'</span><span class="p">].</span><span class="n">values</span><span class="p">[:</span><span class="mi">17</span><span class="p">]</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'inequality'</span><span class="p">].</span><span class="n">values</span><span class="p">[:</span><span class="mi">17</span><span class="p">],</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_norm_red</span><span class="p">:</span>
    <span class="n">idata_norm_red</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                               <span class="n">idata_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s">'log_likelihood'</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span>
                               <span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span>
                               <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    
<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_norm_red</span><span class="p">)</span>

</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/trace_norm_red.webp" alt="The trace for the new normal model" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model_norm_red</span><span class="p">:</span>
    <span class="n">y_pred_red</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y_pred'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">alpha</span><span class="o">+</span><span class="n">x_plt</span><span class="o">*</span><span class="n">beta</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>


<span class="k">with</span> <span class="n">model_norm_red</span><span class="p">:</span>
    <span class="n">idata_norm_red</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata_norm_red</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'y'</span><span class="p">,</span> <span class="s">'y_pred'</span><span class="p">],</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">))</span>
    
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">idata_norm_red</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_plt</span><span class="p">,</span> <span class="n">idata_norm_red</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span>
                <span class="n">idata_norm_red</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.975</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">]),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_turnout</span><span class="p">[</span><span class="s">'turnout'</span><span class="p">].</span><span class="n">values</span><span class="p">,</span> <span class="n">df_turnout</span><span class="p">[</span><span class="s">'inequality'</span><span class="p">].</span><span class="n">values</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/robust_regression/ppc_norm_red.webp" alt="The PPC for the new model" /></p>

<p>This result looks much more to the robust estimate than to 
the full normal estimate.
While in the full normal model the parameter beta was
not compatible with 0, both for the robust and for the reduced
normal model it is.
This implies that those models contradict the full normal model,
which shows a negative association between the turnover
and the average income inequality.
Since the conclusion of the full normal model are heavily 
affected by the South Africa, before drawing
any conclusion one should carefully assess whether it
makes sense or not. Is the South Africa really representative or
is it a special case?</p>

<h2 id="conclusions">Conclusions</h2>

<p>We have discussed how to perform a robust linear regression,
and we have shown with an example that using it instead of a normal
linear regression makes our model more stable to the presence
of non-representative items.</p>

<h2 id="suggested-readings">Suggested readings</h2>
<ul>
  <li><cite>Healy, K. (2019). Data Visualization: A Practical Introduction. Princeton University Press.
</cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">pytensor</span>
</code></pre></div></div>
<div class="code">
Last updated: Wed Nov 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.7
<br />
IPython version      : 8.24.0
<br />

<br />
xarray  : 2024.9.0
<br />
pytensor: 2.25.5
<br />

<br />
pandas    : 2.2.3
<br />
seaborn   : 0.13.2
<br />
matplotlib: 3.9.2
<br />
pymc      : 5.17.0
<br />
numpy     : 1.26.4
<br />
arviz     : 0.20.0
<br />

<br />
Watermark: 2.4.3
<br />
</div>

  </div><a class="u-url" href="/statistics/robust_regression" hidden></a>

  <br>
  <div id='autograph'>
          Stippe Nov 6, 2024

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>


  <div class="wrapper">
          <div class='footer-text'>
                  Opinions are mine, mistakes too, and if you find any feel free to report it via mail or via Twitter.
                  <br>
                  Most of the material in the statistics section is an adaptation to Python of some pre-existing model.
                  <br>
                  I have tried to provide the necessary credits, but if you think that a relevant contribution is missing, please let me know.
                  <br>
                  No AI were harmed in creating this blog.
                  <br>
                  This is a blog, not a bakery: there are no cookies here!
                  <br>
                  The top image has been generated with a modified version Dan Gries' code, available at <a href="http://rectangleworld.com/blog/archives/538">http://rectangleworld.com</a>.
          </div>
          <br>

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Data-perspectives</li>
          <li><a class="u-email" href="mailto:dataperspectivesblog@gmail.com">dataperspectivesblog@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://twitter.com/SteffPy" target="_blank" title="twitter">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://stackoverflow.com/users/11065831/stefano" target="_blank" title="stackoverflow">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#stackoverflow"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://github.com/thestippe/" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://vis.social/@thestippe" target="_blank" title="mastodon">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#mastodon"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://bsky.app/profile/stippe87.bsky.social" target="_blank" title="bluesky">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#bluesky"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

  <script src="/docs/assets/javascript/scroll.js">
  </script>

</html>

