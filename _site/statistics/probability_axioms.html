<!DOCTYPE html>
<html lang="en"><head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-W9G73E5P44"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-W9G73E5P44');
</script>
          <link rel="icon" 
                type="image/png" 
                href="/docs/assets/images/dp_icon.png">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Kolmogorov axioms | Data Perspectives</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Kolmogorov axioms" />
<meta name="author" content="Data-perspectives" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The mathematical foundations of statistics" />
<meta property="og:description" content="The mathematical foundations of statistics" />
<link rel="canonical" href="http://localhost:4000/statistics/probability_axioms" />
<meta property="og:url" content="http://localhost:4000/statistics/probability_axioms" />
<meta property="og:site_name" content="Data Perspectives" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-01-06T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Kolmogorov axioms" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Data-perspectives"},"dateModified":"2024-01-06T00:00:00+00:00","datePublished":"2024-01-06T00:00:00+00:00","description":"The mathematical foundations of statistics","headline":"Kolmogorov axioms","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/statistics/probability_axioms"},"url":"http://localhost:4000/statistics/probability_axioms"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Perspectives" />


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
<a rel="me" href="https://vis.social/@thestippe" style="display: none;">Mastodon</a>
<link rel="manifest" href="manifest.json">
</head>
<body>

        <div id='upperBar'><header class="site-header">

        <div id='upperBarr'>
        <script src="https://d3js.org/d3.v7.js"></script>
                <div class="wrapper" style="display:flex;"><ul hidden='hidden' id="postList"><li>
                        The autoregressive model;/statistics/autoregressive;1
                </li><li>
                        Introduction to time series modelling;/statistics/time_series;2
                </li><li>
                        Synthetic control;/statistics/synthetic_control;3
                </li><li>
                        Regression discontinuity ;/statistics/discontinuity_regression;4
                </li><li>
                        Difference in difference;/statistics/difference_in_differences;5
                </li><li>
                        Instrumental variable regression;/statistics/instrumental_variable;6
                </li><li>
                        Randomized controlled trials;/statistics/randomized;7
                </li><li>
                        Causal inference;/statistics/causal_intro;8
                </li><li>
                        Random models and mixed models;/statistics/random_models;9
                </li><li>
                        Introduction to Extreme Values theory;/statistics/extreme_intro;10
                </li><li>
                        Application of survival analysis 1;/statistics/survival_example;11
                </li><li>
                        Introduction to survival analysis;/statistics/survival_analysis;12
                </li><li>
                        Hierarchical models and meta-analysis;/statistics/hierarchical_metaanalysis;13
                </li><li>
                        Hierarchical models;/statistics/hierarchical_models;14
                </li><li>
                        Poisson regression;/statistics/poisson_regression;15
                </li><li>
                        Logistic regression;/statistics/logistic_regression;16
                </li><li>
                        Robust linear regression;/statistics/robust_regression;17
                </li><li>
                        Introduction to the linear regression;/statistics/regression;18
                </li><li>
                        Model comparison;/statistics/model_averaging;19
                </li><li>
                        Predictive checks;/statistics/predictive_checks;20
                </li><li>
                        Trace inspection;/statistics/trace_inspection;21
                </li><li>
                        Introduction to the Bayesian workflow;/statistics/bayesian_workflow;22
                </li><li>
                        Mixture models;/statistics/mixture;23
                </li><li>
                        Multidimensional distributions;/statistics/categories;24
                </li><li>
                        Exponential model, gaussian model and their evolutions;/statistics/reals;25
                </li><li>
                        The Negative Binomial model;/statistics/negbin;26
                </li><li>
                        The Poisson model;/statistics/poisson;27
                </li><li>
                        The Beta-Binomial model;/statistics/betabin;28
                </li><li>
                        Introduction to Bayesian statistics;/statistics/bayesian_intro;29
                </li><li>
                        The central limit theorem;/statistics/central_limit;30
                </li><li>
                        Introduction to decision theory;/statistics/decision_theory;31
                </li><li>
                        Common continuous probabilities;/statistics/common_continuous_probabilities;32
                </li><li>
                        Common discrete probabilities;/statistics/common_discrete_probabilities;33
                </li><li>
                        Interpretations of probability;/statistics/probability_interpretations;34
                </li><li>
                        Kolmogorov axioms;/statistics/probability_axioms;35
                </li><li>
                        What is statistics;/statistics/statistics_intro;36
                </li><li>
                        Design tricks;/dataviz/design-introduction;37
                </li><li>
                        How to choose a color map;/dataviz/palettes-introduction;38
                </li><li>
                        Introduction to color perception;/dataviz/color-introduction;39
                </li><li>
                        Drawing is redrawing;/dataviz/gender-economist;40
                </li><li>
                        Visual queries;/dataviz/visual-queries;41
                </li><li>
                        The Gestalt principles;/dataviz/gestalt;42
                </li><li>
                        Channel effectiveness;/dataviz/effectiveness;43
                </li><li>
                        Evolutions of the line chart;/dataviz/linechart-evolution;44
                </li><li>
                        Beyond the 1D scatterplot;/dataviz/scatterplot-evolution;45
                </li><li>
                        Perception;/dataviz/perception;46
                </li><li>
                        Fundamental charts;/dataviz/fundamental-charts;47
                </li><li>
                        Marks and channels;/dataviz/marks-channels;48
                </li><li>
                        Data abstraction;/dataviz/data-types;49
                </li><li>
                        Data visualization;/dataviz/dataviz;50
                </li></ul>
                        <div style="display:flex">
                  <a href="/statistics/statistics_intro" class="prev">&#8249;</a>
                  
                                <a href="/"><img class="site-masthead" src="/docs/assets/images/logo_dp.png" alt="Data Perspectives" id="logo" /></a><div id='searchNav' style="flex;">
                                        <input type="search" id="search_0" class="searchBar" onkeydown="searchText()" placeholder="Search">
                                </div>

                                <div hidden='hidden' id="search_focus">0</div>



                        </div><nav class="site-nav" style="display:flex;">
                                <input type="checkbox" id="nav-trigger" class="nav-trigger" />
                                <label for="nav-trigger">
                                        <span class="menu-icon">
                                                <svg viewBox="0 0 18 15" width="18px" height="15px">
                                                        <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
                                                </svg>
                                        </span>
                                </label>

                                <div class="trigger"><a  class="page-link" href="/about">About me</a><a  class="page-link" href="/links">Resources</a>
                                <a href="/statistics/" class="page-link">Up</a>
                                
                                </div>
                        </nav>
                  <a href="/statistics/probability_interpretations" class="next">&#8250;</a>
                  
        </div>


        <script src="/docs/assets/javascript/search.js">
        </script>
                </div>

</header>
<div class="progress-container">
    <div class="progress-bar" id="myBar"></div>
    </div>
        </div>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
          <div id='topPage'></div>
        <a href='/index'>
<img src="/docs/assets/images/background_resized.webp" alt="backround" style="margin:auto;display:block;width:1200px">

</a>
    <h1 class="post-title p-name" itemprop="name headline">Kolmogorov axioms</h1>
    <p class="post-meta"><time class="dt-published" datetime="2024-01-06T00:00:00+00:00" itemprop="datePublished">
        Jan 6, 2024
      </time></p>
    <p class="post-meta"> Reading time: <span class="reading-time" title="Estimated read time">
  
  6&prime;
</span>
</p>
  </header>
  <br>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In a previous post we mentioned the Kolmogorov axioms.
In this post we will discuss them and look at some immediate consequence
of these assumptions.
We will also discuss the definition of random variable,
and provide other important definition related to this concept.
We will assume that the reader already had a course about Lebesgue integrals
and multidimensional analysis, and we will only recall the basic definitions.</p>

<h2 id="the-axioms-of-probability">The axioms of probability</h2>

<p>\(\mathcal{F}\) is a <strong>\(\sigma\)-algebra</strong> over \(\Omega\) if it is a nonempty collection
of subsets of \(\Omega\) such that</p>
<ul>
  <li>$A \in \mathcal{F} \Rightarrow A^c \in \mathcal{F}$</li>
  <li>If $A_j$ is a succession in $\mathcal{F}$ then $\bigcup_j A_j \in \mathcal{F}$</li>
  <li>If $A_j$ is a succession in $\mathcal{F}$ then $\bigcap_j A_j \in \mathcal{F}$</li>
</ul>

<p>\(P\) is a <strong>measure</strong> over \((\Omega, \mathcal{F})\) if</p>
<ul>
  <li>$P(A) \geq 0\, \forall A \in \mathcal{F}$</li>
  <li>$P(\emptyset) = 0 $</li>
  <li>If $A_j$ is a countable collection of disjoint elements of $\mathcal{F}\,,$ then $P\left(\bigcup_j A_j \right) = \sum_j P(A_j)$</li>
</ul>

<p>\((\Omega, \mathcal{F}, P)\) is a <strong>measure space</strong> if</p>
<ul>
  <li>\(\Omega\) is a set</li>
  <li>\(\mathcal{F}\) is a \(\sigma\)-algebra over \(\Omega\)</li>
  <li>\(P\) is a measure over \((\Omega, \mathcal{F})\)</li>
</ul>

<p>We define a <strong>probability space</strong> is any measure space
\((\Omega, \mathcal{F}, P)\) such that \(P(\Omega) = 1\,.\)</p>

<h2 id="consequences">Consequences</h2>

<p>\(1 = P(\Omega) = P(A \cup A^c) = P(A) + P(A^c)\)
so $P(A^c) = 1-P(A)\,.$
Since both the elements must be non-negative we have that $0 \leq P(A) \leq 1\,.$</p>

<p>If $A \subseteq B$ we can write \(B = A \cup (B \setminus A)\) and \(A \cap (B \setminus A) = \emptyset\,,\) so \(P(B) = P(A) + P(B \setminus A)\,.\) Since both the elements of the RHS of the equation
must be non-negative we have that \(P(B) \geq P(A)\,.\)</p>

<p>\mathcal{F}or any elements $A, B \in \mathcal{F}$ we can write
\(A \cup B = A \cup (B \setminus A)\) with $A \cap (B \setminus A) = \emptyset\,,$
so \(P(A\cup B) = P(A) + P(B\setminus A)\,.\)
We also have that \(B = (B \cap A) \cup (B \setminus A)\)
with \((B \cap A) \cup (B \setminus A) = \emptyset\) so
\(P(B) = P(B\cap A) + P(B\setminus A)\) or \(P(B \setminus A) = P(B) - P(B\cap A)\)
and this implies</p>

\[P(A \cup B) = P(A) + P(B) - P(A \cap B)\]

<h2 id="random-variables">Random variables</h2>

<p>Working in $\Omega$ can be cumbersome, as we didn’t required it to be equipped
with any operation, while we would like to be able to perform some computation.
For this reason it is useful to define a <strong>real random variable</strong>,
and this is defined as a measurable map
\(X : \Omega \rightarrow E \subset \mathbb{R}^n\,.\)
If $n=1$ we say $X$ is a scalar or univariate random variable,
if $n &gt; 1$ we say it’s a vector or multivariate
random variable, and $n$ is the dimension of the variable.
We define the <strong>realization</strong> of $X$ the value
taken by $X$ at $\omega \in \Omega\,,$
namely the value $x=X(\omega)\,.$</p>

<p>If $X$ is a one-dimensional random variable, we can define
\(F(x) = P(X \leq x)\,,\) and $F$ is defined as the <strong>cumulative distribution
function</strong> (cdf for short) of $X\,.$
$F$ has the properties that</p>
<ul>
  <li>$0 \leq F(x) \leq 1 \forall x \in E$</li>
  <li>$F(-\infty) = 0$</li>
  <li>$F(+\infty) = 1$</li>
</ul>

<p>Moreover $P(x_1  &lt; X \leq x_2) = F(x_2) - F(x_1)$</p>

<p>If $X$ is discrete, we can decompose
\(F(x) = \sum_{x_j \leq x} p_X(x_j)\) and $p_X$ is defined as the
<strong>probability mass function</strong> (pmf for short) of $X\,.$</p>

<p>If $X$ is continuous, we can decompose
\(F(x) = \int_{y\leq x} dy p_X(y)\) and $p_X$ is defined as the
<strong>probability distribution function</strong> (pdf for short) of $X\,.$</p>

<p>In the following we will often omit the subscript $X$ and we will indicate
the pdf/pmf as $p\,.$</p>

<p>We can immediately generalize to the $n$ dimensional case,
where $X = (X_1,\dots,X_n)$, and define
$F(x_1,\dots,x_n) = P(X_1 \leq x_1,\dots X_n \leq x_n)\,.$</p>

<p>In the discrete case the pmf generalizes to
\(F(x_1,\dots,x_n) = \sum_{y_1 \leq x_1,\dots,y_n \leq x_n} p(y_1,\dots,y_n)\,,\)
while in the continuous one we have
\(F(x_1,\dots,x_n) = \int_{y_j \leq x_j} \prod_j dy_j p(y_1,\dots,y_n)\,.\)</p>

<p>If $X$ is a continuous random variable, we define the <strong>expected value</strong> of any measurable function $f$
as</p>

\[\mathbb{E}[f(X)] = \int_E p(x) f(x)\]

<p>In the discrete case we simply replace the integral with the sum.</p>

<p>We define the <strong>mean</strong> as
\(\mathbb{E}[X] = \int dx x p(x)\)</p>

<p>If $X$ is a one dimensional random variable, we define the <strong>variance</strong> as
\(\mathbb{E}[(X-\mathbb{E}[X])^2]\,,\)
while the $k$-th raw moment is defined as
\(\mathbb{E}[X^k]\,.\)
Notice that, in general, there is no warranty that moments of order
$k&gt;0$ exists.</p>

<p>On the other hand, if $X$ is multidimensional, we define the component $(i, j)$ of the  <strong>covariance matrix</strong>
as
\(\mathbb{E}[(X_i - \mathbb{E}[X_i])(X_j - \mathbb{E}[X_j])]\,.\)</p>

<p>The <strong>correlation</strong> between $X_i $ and $X_j$ is defined as</p>

\[\frac{
\mathbb{E}[(X_i -\mathbb{E}[X_i])(X_j - \mathbb{E}(X_j))]
}{\sqrt{ \mathbb{E}(X_i - \mathbb{E}[X_i])^2 \mathbb{E}(X_j - \mathbb{E}[X_j])^2  }}\]

<p>Notice that we can write
\(F(x_1,\dots, x_n) = \mathbb{E}[\theta_{x_1}(X_1)\dots \theta_{x_n}(X_n)]\)
where $\theta_x$ is the Heaviside step function</p>

\[\theta_x(y) =
\begin{cases}
&amp; 1 &amp; y \leq x\\
&amp; 0 &amp; y &gt; x \\
\end{cases}
\,.\]

<p>We define the <strong>characteristic function</strong></p>

\[\varphi(t) = \mathbb{E}[e^{i t \cdot X}]\]

<p>If $X$ is a one dimensional variable, assuming that all the moments are finite, we have that
\(\varphi(t) = \sum_{j=0}^\infty i^j \frac{\mathbb{E}[X^j] t^j}{j!}\,.\)
so we can recover any moment as a derivative of the characteristic function.</p>

<p>Another very important expected value is the <strong>entropy</strong></p>

\[\mathbb{E}[-\log(p(X))]\]

<p>If $X$ is an $n$ dimensional continuous random variable and $Y = g(X) = (g_1(X),\dots, g_n(X))$
is an invertible transformation with inverse
$h = (h_1,\dots, h_n)$, then</p>

\[p_Y(y) = p_X(h(y)) |J|\]

<p>where</p>

\[J = \det \left(\frac{\partial h_i(y)}{\partial y_j} \right)\]

<h2 id="joint-probabilities">Joint probabilities</h2>

<p>Given a multivariate distribution $p(x_1,\dots x_j,\dots,x_n)$
we define the <strong>marginal</strong> pdf of $x_j$ as</p>

<p>\(p(x_j) = \int \prod_{i \neq j} dx_i p(x_1,\dots x_j, \dots,x_n)\,.\)
Notice that $p(x_j)$ defines a probability density function,
as it is non-negative and in integrates to 1.</p>

<p>We can also write $p(x_j)$ as
\(p(x_j) = \mathbb{E}[\delta(X_j-x_j)]\)
where $\delta$ is the Dirac delta distribution.</p>

<p>Of course, the extension of the marginal pdf to more than one dimension
is straightforward.</p>

<p>Given two random variables $X$ and $Y\,,$ we stay that they are
<strong>independent</strong> if \(p_{X, Y}(x, y) = p_X(x) p_Y(y)\,.\)
Notice that, if two variables are independent, we can factor any expected
value.</p>

<p>If $Z = (X, Y)\,,$ we define $p(z) = p(x, y)$ as the <strong>joint</strong> probability
distribution of $X$ and $Y\,.$
Given a multivariate random variable $Z=(X, Y)\,,$ with joint pdf $p(z) = p(x, y)$ we define the <strong>conditional</strong> pdf of $X$ with respect to $Y$ as</p>

\[p(x | y) = \frac{p(x, y)}{p(y)}\]

<p>where $p(y) = \int dx p(x, y)$ is the marginal pdf of $y\,.$
We stress that \(p(x | y)\) defines a pdf for $x\,,$ since
\(\int dx p(x | y) = \int dx \frac{p(x, y)}{p(y)} = \frac{p(y)}{p(y)} = 1\,,\)
but it doesn’t define
a probability for $y\,,$ as $\int dy p(x | y) \neq 1\,.$</p>

<p>Notice that, since
\(p(x | y) = \frac{p(x, y)}{p(y)}\) and \(p(y | x) = \frac{p(x, y)}{p(x)}\,,\)
we can write</p>

\[p(x | y) = \frac{p(x | y)}{p(y)} p(x)\]

<p>and this is the <strong>Bayes’ theorem</strong> for the pdf.</p>

<p>Given a set of $n$ independent random variables with pdf $p_1,\dots,p_n\,,$
we say that they are <strong>identically distributed</strong> if</p>

\[p_i(x)  = p_j(x) \forall i, j\]

<p>and in this case we can write
\(p(x_1,\dots,x_n) = \prod_{i=1}^n p_1(x_i)\,.\)</p>

<h2 id="conclusions">Conclusions</h2>

<p>We defined a probability space and we showed some basic results
of the definition of probability.
We also gave the definition of random variable, and discussed some
basic concept in probability theory.</p>

  </div><a class="u-url" href="/statistics/probability_axioms" hidden></a>

  <div>
          
          
  </div>

  <br>
  <div id='autograph'>
          Stippe Jan 6, 2024

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>


  <div class="wrapper">
          <div class='footer-text'>

                  No AI were harmed in creating this blog.
                  <br>
                  This is a blog, not a bakery: there are no cookies here!
                  <br>
                  The top image has been generated with a modified version Dan Gries' code, available at <a href="http://rectangleworld.com/blog/archives/538">http://rectangleworld.com</a>.
          </div>
          <br>

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Data-perspectives</li>
          <li><a class="u-email" href="mailto:dataperspectivesblog@gmail.com">dataperspectivesblog@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://twitter.com/SteffPy" target="_blank" title="twitter">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://stackoverflow.com/users/11065831/stefano" target="_blank" title="stackoverflow">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#stackoverflow"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://github.com/thestippe/" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://vis.social/@thestippe" target="_blank" title="mastodon">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#mastodon"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

  <script src="/docs/assets/javascript/scroll.js">
  </script>

</html>

