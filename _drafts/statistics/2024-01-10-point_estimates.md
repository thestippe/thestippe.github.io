---
layout: post
title: "Point estimates"
categories: /statistics/
tags: /point_estimates/
# image: "/docs/assets/images/perception/eye.jpg"
description: "Introduction to the theory of point estimation"
publish: False
published: False
---

In a previous post we said that, given a set of observations $X_1,\dots,X_n\,,$
and assuming that the observations have been generated by a distribution
belonging to a family of distributions
$$
\mathcal{P} = \left\{P_\theta : \theta \in \Theta\right\}\,,
$$
we would like to use our dataset to restrict the possible values of $\theta$
or, more generally, of some function of $\theta$ $g(\theta)\,.$

Notice that this is not always possible, as it may happen that, for
$\theta_1 \neq \theta_2\,,$ we have $P_{\theta_1} = P_{\theta_2}\,.$
In this case the family is said to be **unidentifiable**.
If a family is not unidentifiable we say it is **identifiable**.

The first attempt we may try is to simply provide a single value for it,
and this is called a **point estimate** for $g(\theta)$.

## Statistics, estimators and estimations

The first assumption that we will make is that our set of observations
is made up by $n$ independent identically distributed random variables.
In this case we say that our observation are a **random sample**
of a random variable $X\,.$

We define a **statistic** any real function of the sample $$T = T(X_1,\dots,X_n)\,.$$
If $$x_1,\dots,x_n$$ are a realization of our random variables,
we define $$T(x_1,\dots,x_n)$$ a **realization of T**.
In other words, a realization of a statistic is the value taken by the statistic
for a particular observed set of data.

We want to use our data to find a value of
our **estimand** $$g(\theta)\,,$$
so we want to use a statistics (a function of the sample)
and in this case $T$ is said to be an **estimator** (or, more precisely,
a point estimator) for $g(\theta)\,.$
The realized value of our estimator $T(x_1,\dots,x_n)$
is said to be a **point estimate** of $g(\theta)\,.$

In order to clarify the ideas, let us assume that we have a sample
of $n$ iid observations distributed according to a normal
distribution with variance $\sigma^2$, and we want to find an estimate 
of the mean $\mu\,.$
Some possible estimators would be
- the arithmetic mean $$T(X) = \frac{1}{n}\sum_{i=1}^n X_i$$
- the midrange $$T(X) = \frac{\max(X) + \min(X)}{2}$$
- the first observation $T(X) = X_1$
- but also $$T(X) = 0$$

From the above example, we see that our definition of estimator is too broad,
and we should constrain the possible estimators with some additional 
properties.

## Bias of an estimator

Given a set of iid observations $X=(X_1,\dots,X_n)$ distributed according to $P_\theta$
and given $T$ an estimator for $g(\theta)\,,$ we define the **bias** of an estimator as
$$\mathbb{E}[T(X)-g(\theta)]\,.$$
If the bias is 0 for any $\theta\in\Theta$ we say that $T$ is an **unbiased estimator** for $g(\theta)\,.$

## Consistency

Unbiasedness itself is not enough, however.
Let us consider in the above example the case $T(X) = X_1$
compared to $T(X) = \frac{1}{n} \sum_i X_i\,.$
They both have the expected value $\mu\,,$
but as $n$ grows, the arithmetic mean becomes more and more precise,
while $T(X) = X_1$ always has variance equal to $\sigma^2$.

In fact, if $X_i,i=1,\dots,n$ is a set of iid variables
with finite mean $\mu$ and finite variance $\sigma^2$, given

$$\bar{X} =\frac{1}{n} \sum_{i=1}^n X_i$$

we have

$$
\mathbb{E}[\bar{X}] = \mathbb{E}\left[ \frac{1}{n}\sum_{i}X_i \right ] = \mu
$$

$$
Var[\bar{X}] =
\mathbb{E}\left[ \left(\frac{1}{n} \sum_{i=1}^n X_i - \mu \right )^2 \right ]
=
\mathbb{E}\left[ \left(\frac{1}{n} \sum_{i=1}^n (X_i - \mu) \right )^2 \right ]
=
\frac{1}{n^2}\mathbb{E}\left[ \sum_{i=1}^n (X_i - \mu)^2  \right ] = \frac{\sigma^2}{n}
$$

Above we used $\mathbb{E}[(X_i-\mu)(X_j-\mu)] = 0$ if $i\neq j\,,$ since
the variables are mutually independent.
The one above is the **weak law of large numbers**.

We may therefore also require that our estimate becomes more and more precise
as the sample size grows.
In mathematical terms, the concept "becomes more and more precise"
requires to be specified more precisely.
We say that an estimator is **consistent** if, 
$$\forall \varepsilon > 0$$

$$
\lim_{n \rightarrow \infty} P(|T(X_n) - g(\theta)|<\varepsilon) = 1\,  \forall \theta \in \Theta
$$

Notice that consistency does not imply unbiasedness, as there may be a bias
for finite $n$ but, in the limit, the bias may disappear.
The most known case for this is the naive estimator for the variance.

$$
\hat{S}_n = \frac{1}{n}\sum_{i=1}^n \left(X_i-\bar{X}\right)^2
$$

In order to show it, let us take $\mu=0$, as this doesn't affect the result,
since we can always define $Y_i = X_i - \mu$ and $\bar{Y} = \bar{X}-\mu$ and leave $S_n$
unchanged.

$$
\begin{align}
\mathbb{E}[\hat{S}_n] & =
\mathbb{E}\left[ \frac{1}{n}\sum_i (X_i - \bar{X})^2 \right]
=
\frac{1}{n}\mathbb{E}\left[ \sum_i (X_i^2 + \bar{X}^2 -2 X_i \bar{X}) \right]
=
\sigma^2 
+ \mathbb{E}\left[ \bar{X}^2 \right]
-2 \sum_i \frac{1}{n}\mathbb{E}\left[  X_i \bar{X} \right]
=
\sigma^2 
+ \frac{\sigma^2}{n}
-\frac{2}{n} \sum_i \mathbb{E}\left[  X_i^2 \right]
\\
&
= \sigma^2 \left(1-\frac{1}{n}\right) = \sigma^2 \left(\frac{n-1}{n}\right)
\end{align}
$$

Above we used
$$\mathbb{E}[X_i^2] = Var[X_i] = \sigma^2\,,$$
$$\mathbb{E}[\bar{X}^2] = Var[\bar{X}^2] = \frac{\sigma^2}{n}$$
and
$$\mathbb{E}[X_i \bar{X}] = \mathbb{E}[X_i^2] = Var[X_i] = \sigma^2\,.$$

The unbiased (and consistent) version of $\hat{S}_n$ is

$$
S_{n} = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2
$$

It is generally not guaranteed that an unbiased estimator for $g(\theta)$
exists. If it exists, then $g(\theta)$ is said to be **estimable**.

## Unbiased estimators

Assume that you found two unbiased estimators $T(X)$ and $U(X)$ for $g(\theta)\,,$
how do you choose which one you should use?
A possible (not necessarily the best one)
criteria is to choose the one which gives you the most accurate estimate.

We can quantify the accuracy of the estimator by estimating its variance $Var[T(X)]\,,$
and we say that $T(X)$ is the **Uniformly Minimum Variance Unbiased Estimator** (UMVUE)
if $$Var[T(X)] < Var[U(X)]\, \forall \theta \in \Theta, \forall U(X)$$ unbiased
estimator of $g(\theta)\,.$

Notice that

$$
\begin{align}
MSE[T] & = 
\mathbb{E}[(T(X)-g(\theta))^2]
=
\mathbb{E}[((T(X)-\mathbb{E}[T(X)])-(g(\theta)-\mathbb{E}[T(X)]))^2]
\\
&
=
\mathbb{E}[(T(X)-\mathbb{E}[T(X)])^2]+\mathbb{E}[(g(\theta)-\mathbb{E}[T(X)])^2]
-2\mathbb{E}[(T(X)-\mathbb{E}[T(X)])(g(\theta)-\mathbb{E}[T(X)])
]
\\
&
=
\mathbb{E}[(T(X)-\mathbb{E}[T(X)])^2]+(g(\theta)-\mathbb{E}[T(X)])^2
\\
&
= Var[T(X)] + Bias[T(X), g(\theta)]^2
\end{align}
$$

From the above relation we see that an unbiased estimator that minimizes
the Mean Square Error with respect to $g(\theta)$ also minimizes the variance,
so it's the UMVUE.

As we will see when we will discuss decision theory, in some case, you may desire to reduce the MSE as much as you can, and you may end up with a biased estimator
with a smaller variance than any unbiased estimator, so you may prefer a
biased estimator to an unbiased one.

This is sometimes named as the bias-variance trade-off.
