---
layout: post
title: "Interpretations of probability"
categories: /statistics/
subgategory: Introduction
section: 2
tags: /interpretations/
date: "2024-01-07"
# image: "/docs/assets/images/perception/eye.jpg"
description: "What does probability mean?"
published: false
---

As we previously discussed, the word "random" has more than one meaning.
It shouldn't be surprising the fact that the probability, which is the mathematical
object which describes randomness, has more than one interpretation.
Here we will discuss the two most important interpretations, namely the frequentist or objectivist one
and the Bayesian or subjectivist one.

## Families of distributions

Before we discuss about the interpretations of statistics, we should introduce some
additional mathematical concept.
We mentioned that, in statistical inference, we encode the assumptions about
how our data relates to the population by means of a statistical model.
We can put the previous statement into a more formal way.
We will assume that our sample has been generated by some probability distribution
$P\,,$ and we assume that $P$ belongs to a family of probabilities
$$\mathcal{P} = \left\{ P_\theta : \theta \in \Theta \right\}\,.$$
This implies that we are trying to put some constrain to the possible regions of $\Theta$ 
that are allowed by our data.

## The frequentist interpretation

In the frequentist interpretation we assume that it exists a "true" value $\theta_0$
for our parameter, and that the sample has been generated by $P_{\theta_0}\,,$
and that the probability of an event corresponds to the relative frequency of that event.
This definition requires that, at least in principle, we must be able to repeat the experiment.
In this case, if the outcome of the experiment is discrete, we identify

$$ P(\omega_i) = \lim_{n \rightarrow \infty} \frac{n_i}{n} $$

In the above formula, $$n$$ is the number of times the experiment has been repeated, while $$n_i$$
is the number of times the experiment had $$\omega_i$$ as outcome.

In the continuous case we cannot take a single point, as it is a zero-measure set,
so we refine

$$
P(A) = \lim_{n \rightarrow \infty} \frac{n_A}{n}
$$

Here $$A \subset \Omega$$, $$n_A$$ is the number of times the outcome was in $A$
and, as before, $n$ is the number of times the experiment has been repeated.

Of course, this doesn't necessarily mean that we believe that the data has been really generated
by $$P_{\theta_0}\,,$$ but it rather means that we consider this an appropriate description of the
system that we are analyzing.
This of course makes a lot of sense in quantum mechanics, as we may assume that 
the Planck constant has the same value regardless on the experiment we perform to measure it.
It is however more questionable when one moves away from the reign of the subatomic physics,
and one asks questions like "What is the probability that tomorrow I will wear a grey t-shirt?".

A complete unknown may give any number based on his/her own experience, 
a dear friend who knows I like grey would probably give a quite high number,
but if I already know that all my t-shirts are in the washing machine I would say zero,
since I already know it's an impossible event.
Since the above definition of probability doesn't look appropriate within this context,
we should look for another definition of probability, which is more meaningful for this problem.

## The Bayesian interpretation
In the Bayesian interpretation, the probability represent the degree of belief that a certain event occurs.
According to this interpretation, we may not be sure about the value of $\theta\,,$ and we simply
attach a degree of confidence to it.
If $$p(\theta)$$ represents our prior beliefs before observing the outcome of the experiment $x$
and indicate with $$p(x | \theta)$$ the probability of $x$ according to $$P_\theta \in \mathcal{P}\,,$$
we can use Bayes' theorem

$$ p(\theta | x) = \frac{p(x | \theta)}{p(x)} p(\theta) \propto p(x | \theta) p(\theta)$$

where we dropped $$p(x)$$ since it is a constant independent on $\theta$ which can be computed
by requiring that $p(\theta | x)$ is properly normalized:

$$ p(x) = \int d\theta p(\theta | x) = \int d\theta p(x | \theta) p(\theta) \,.$$

## A historical note
While during the last century the statistics community was divided into two school of thoughts, 
those who believed in the frequentist interpretation and those who believed into the Bayesian one,
things have changed quite a lot during the last decades.
A 20th century Bayesian statistician would have probably tried and constrain $p(\theta)$
as much as he/she could.
The modern Bayesian school, however, only uses $p(\theta)$ to *regularize*
the problem or, in other words, to avoid crazy answers, as modern Bayesians look for solutions which are only mildly dependent on the choice
of the prior.
An important step in Bayesian inference is, in fact to figure out the dependence of the solution on
the choice of the prior, and one should communicate this as clearly as possible.

Nowadays most statisticians would simply seek for the most appropriate tool to face the problem.

## Conclusions
We discussed the two main interpretations of probability, and explained which is the
modern point of view of the scientific community.
In the future posts we will start discussing about the methodologies of each one of the above schools
and about their pros and cons.
