<p>In the last post, we introduced GPs. In this post we will see how to use them in order
to perform regression in a non-parametric fashion.
We will use the Nile dataset, which contains the Nile flow, expressed in $10^8 m^3$ measurements from
1871 to 1970.</p>

<h2 id="implementation">Implementation</h2>

<p>Let us first of all download the dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">pymc_experimental.distributions</span> <span class="k">as</span> <span class="n">pmx</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/datasets/Nile.csv"</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s">'time'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'time'</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'time'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'value'</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/gp_example/nile.webp" alt="" /></p>

<p>The data seems almost stationary, except from a small discontinuity just before 1900,
and it also shows some auto-correlation, as they don’t look i.i.d. at all,
and it looks like there is no obvious periodicity.</p>

<p>The dataset contains 100 points, and we will use the first 85 to fit our model,
while the last 15 points will be used to assess the performances of our model in predicting
the future flow.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n</span> <span class="o">=</span> <span class="mi">85</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">n</span><span class="p">:]</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s">'time'</span><span class="p">]</span><span class="o">-</span><span class="n">df_train</span><span class="p">[</span><span class="s">'time'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">])</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="s">'time'</span><span class="p">]</span><span class="o">-</span><span class="n">df_train</span><span class="p">[</span><span class="s">'time'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">])</span>
<span class="n">x_test</span> <span class="o">/=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">/=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
</code></pre></div></div>

<p>We normalized the regression variables so that it is bounded between -1 and 1.
It will be convenient to have it normalized in this way, as it will simplify some
parameter estimate.</p>

<p>One of the main issues of GPs is given by their performances. 
However, when you are working with local kernels, by truncating the Fourier series expansion of 
the Kernel, you can obtain what is usually named as “Hilbert Space GPs”,
and this allows a faster implementation of the GPs.
This is not possible for all the kernels, as the Fourier series must exist.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">lam</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'lam'</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s">'tau'</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'rho'</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">gp</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">gp</span><span class="p">.</span><span class="n">HSGP</span><span class="p">(</span><span class="n">m</span><span class="o">=</span><span class="p">[</span><span class="mi">25</span><span class="p">],</span> <span class="n">L</span><span class="o">=</span><span class="p">[</span><span class="mf">1.2</span><span class="p">],</span> <span class="n">mean_func</span><span class="o">=</span><span class="n">pm</span><span class="p">.</span><span class="n">gp</span><span class="p">.</span><span class="n">mean</span><span class="p">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">rho</span><span class="p">),</span><span class="n">cov_func</span><span class="o">=</span><span class="n">tau</span><span class="o">*</span><span class="n">pm</span><span class="p">.</span><span class="n">gp</span><span class="p">.</span><span class="n">cov</span><span class="p">.</span><span class="n">ExpQuad</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">lam</span><span class="p">))</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">prior</span><span class="p">(</span><span class="s">'mu'</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">x_train</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s">'sigma'</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">df_train</span><span class="p">[</span><span class="s">'value'</span><span class="p">]</span><span class="o">/</span><span class="mi">1000</span><span class="p">)</span>
</code></pre></div></div>

<p>We used a squared exponential kernel, where we assume that the GP fluctuations
are of the order of 0.5.
The parameter L must be chosen so that all the points are included into $[-L, L]\,,$
and this is why we normalized the regression variable as above.
We assumed that the mean of the GP has absolute value less than 2, and this seems reasonable
given the dataset.
We only kept 25 terms in the Fourier expansions, and later we will see how to verify
if we did a meaningful choice.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">idata</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">nuts_sampler</span><span class="o">=</span><span class="s">'numpyro'</span><span class="p">,</span>
                     <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/gp_example/trace.webp" alt="" /></p>

<p>It looks like there are few divergences, but this is not a big issue, as their number
is very small and the traces don’t show relevant issues.</p>

<p>Since we truncated the Fourier series, we would like that the last few coefficients
of the series expansion are close to 0, otherwise we would have an indications
that the series has been truncated too early.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'mu_hsgp_coeffs_'</span><span class="p">])</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/gp_example/coeffs.webp" alt="" /></p>

<p>The coefficients are almost zero starting from $i = 20\,,$
so the truncation seems ok.
We can now inspect the posterior predictive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">idata</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata</span><span class="p">))</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s">'time'</span><span class="p">],</span> <span class="mi">1000</span><span class="o">*</span><span class="n">idata</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">)),</span>
               <span class="mi">1000</span><span class="o">*</span><span class="n">idata</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y'</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.97</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">)),</span>
               <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s">'time'</span><span class="p">],</span> <span class="mi">1000</span><span class="o">*</span><span class="n">idata</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">)))</span>
<span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'time'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'value'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="n">df_train</span><span class="p">[</span><span class="s">'time'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">df_train</span><span class="p">[</span><span class="s">'time'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/gp_example/ppc.webp" alt="" /></p>

<p>In the “train” region we can reproduce with quite a high accuracy the observed data,
and there is no obvious sign of overfitting issues.
We can now use the remaining years to verify the performances of our model 
when predicting new data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">mu_pred</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="n">conditional</span><span class="p">(</span><span class="s">'mu_pred'</span><span class="p">,</span> <span class="n">Xnew</span><span class="o">=</span><span class="n">x_test</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">'y_pred'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu_pred</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'mu_pred'</span><span class="p">,</span> <span class="s">'y_pred'</span><span class="p">])</span>

<span class="n">ypred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">idata</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">20000</span><span class="p">,</span> <span class="mi">85</span><span class="p">)),</span>
<span class="n">ppc</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s">'y_pred'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">20000</span><span class="p">,</span> <span class="mi">15</span><span class="p">))],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'time'</span><span class="p">],</span> <span class="mi">1000</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">ypred</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
               <span class="mi">1000</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">ypred</span><span class="p">,</span><span class="n">q</span><span class="o">=</span><span class="mf">0.97</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
               <span class="n">color</span><span class="o">=</span><span class="s">'lightgray'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'time'</span><span class="p">],</span> <span class="mi">1000</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ypred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df_test</span><span class="p">[</span><span class="s">'time'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'time'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'value'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="n">df</span><span class="p">[</span><span class="s">'time'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'time'</span><span class="p">].</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/gp_example/ppc_pred.webp" alt="" /></p>

<p>The credible interval seems large enough to accommodate all the observed data,
and it does not explode. We can be therefore quite confident into the performances of our model.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We used GPs to perform regression over the Nile dataset. We introduced HSGPs,
and we briefly explained how to use them and how to assess the goodness of the 
approximation.</p>

<h2 id="suggested-readings">Suggested readings</h2>
<ul>
  <li><cite><a href="https://gaussianprocess.org/gpml/">Rasmussen, C. E., Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press.</a>
</cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>

<div class="code">
Last updated: Tue Aug 20 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.4
<br />
IPython version      : 8.24.0
<br />

<br />
xarray : 2024.5.0
<br />
numpyro: 0.15.0
<br />
jax    : 0.4.28
<br />
jaxlib : 0.4.28
<br />

<br />
seaborn          : 0.13.2
<br />
pymc_experimental: 0.1.1
<br />
pymc             : 5.15.0
<br />
numpy            : 1.26.4
<br />
arviz            : 0.18.0
<br />
pandas           : 2.2.2
<br />
matplotlib       : 3.9.0
<br />

<br />
Watermark: 2.4.3
<br />
</div>
