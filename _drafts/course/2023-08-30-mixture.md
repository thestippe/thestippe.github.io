---
layout: post
title: "Mixture models"
categories: course/composite/
tags: /mixture/
---

In some case we may have that our population is composed by sub-populations, each one with his own distribution. If we are not able to identify the subgroup we can use a mixture model to take into account of this.

## Normal Mixture Models

The first class of MM we will look at is the normal mixture model.
In this kind of model we are trying and describe some real observable $y$,
and the population is divided into $K$ sub-populations,
and each element has probability $w_i, i=1,...,K$, to belong to the i-th
sub-population.
For each sub-population, we are assuming that $y$ is normally distributed,
with mean $\mu_i$ and variance $\sigma_i$.

Let us apply this model to seaborn's geyser dataset

```python
import random
import xarray as xr
import pandas as pd
import scipy
import numpy as np
import pymc as pm
import pymc.sampling_jax as pmj
import arviz as az
from matplotlib import pyplot as plt
import seaborn as sns

plt.style.use("seaborn-v0_8-darkgrid")

cmap = sns.color_palette("rocket")

rng = np.random.default_rng(42)

geyser = sns.load_dataset('geyser')

geyser.head()
```

|    |   duration |   waiting | kind   |
|---:|-----------:|----------:|:-------|
|  0 |      3.6   |        79 | long   |
|  1 |      1.8   |        54 | short  |
|  2 |      3.333 |        74 | long   |
|  3 |      2.283 |        62 | short  |
|  4 |      4.533 |        85 | long   |

The dataset represents the waiting time between eruptions and the duration of the
eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA.


```python
sns.pairplot(geyser, hue='kind')
```

![The geyser dataset](/docs/assets/images/mixture/normal_mixture/geiser.png)

We can see that we both the duration and the waiting time between eruptions
are well separated for the two categories, and for each category
they look normally distributed.
The label "kind" is of course a human label, and it's not a measured
quantity, so let us assume that we have no idea about it, and that
we must find the distribution of the eruption duration for each category.
In order to do this, we will use a normal mixture model,
with two sub-populations, each one with its own mean and variance.


```python
with pm.Model() as mix_model:
    sigma = pm.Gamma('sigma', alpha=1, beta=0.5, shape=2)
    mu = pm.Normal('mu', mu=2*np.arange(2), sigma=2, shape=2)
    pi = pm.Dirichlet('pi', a=np.ones(shape=2)/2.0)
    phi = pm.Normal.dist(mu=mu, sigma=sigma, shape=2)
    y = pm.Mixture('y', w=pi, comp_dists = phi, observed=geyser['duration'])
    trace_mix = pm.sample(draws=2000, tune=2000, chains=4, random_seed=rng)

az.plot_trace(trace_mix)
```

![The trace of our model](/docs/assets/images/mixture/normal_mixture/trace_geiser.png)

```python
az.summary(trace_mix)
```

|          |   mean |    sd |   hdi_3% |   hdi_97% |   mcse_mean |   mcse_sd |   ess_bulk |   ess_tail |   r_hat |
|:---------|-------:|------:|---------:|----------:|------------:|----------:|-----------:|-----------:|--------:|
| mu[0]    |  2.021 | 0.027 |    1.97  |     2.07  |           0 |         0 |       8026 |       6010 |       1 |
| mu[1]    |  4.275 | 0.034 |    4.21  |     4.337 |           0 |         0 |      10403 |       6927 |       1 |
| sigma[0] |  0.244 | 0.024 |    0.2   |     0.287 |           0 |         0 |       7226 |       6688 |       1 |
| sigma[1] |  0.438 | 0.028 |    0.386 |     0.49  |           0 |         0 |       7052 |       6363 |       1 |
| pi[0]    |  0.35  | 0.029 |    0.293 |     0.401 |           0 |         0 |      10929 |       6276 |       1 |
| pi[1]    |  0.65  | 0.029 |    0.599 |     0.707 |           0 |         0 |      10929 |       6276 |       1 |

Both the mean and the variance of our normal distributions looks well separated,
and their traces looks good.
Let us look at the posterior predictive distribution.

```python
with mix_model:
    ppc_mix = pm.sample_posterior_predictive(trace_mix)
az.plot_ppc(ppc_mix, legend=False, num_pp_samples=2000)
```


![The PPC of our model](/docs/assets/images/mixture/normal_mixture/geiser_ppc.png)

The posterior predictive is not perfect, probably a Student-t mixture would
have been more appropriate, but it catches the general behavior of the data.

## Zero inflated models
Another commonly used mixture model are the so-called zero-inflated models.

When dealing with count data it may happen that the count of the zeros is over-represented with respect to our model. Usually this happens because there is some kind of filter in the data, as an example a failure in our counter. Zero inflated models include this possibility in the count model, so we can build a zero-inflated Poisson model or a zero-inflated negative binomial model. Zero-inflated models are a class of mixture models for discrete data where, given the starting probability
$P_0(x \vert \theta)$
the likelihood takes the following form:

$$
P(x \vert w, \theta) = (1-w) \delta_{x, 0} + w P_0(x \vert \theta)
$$

One can also build zero-inflated continuous models by replacing the discrete delta function with the Dirac delta, but we will not cover this topic.
We will apply this model to the fish (sometimes called camper) dataset.
The dataset contains data on 250 groups that went to a park. Each group was questioned about how many fish they caught (count), how many children were in the group (child), how many people were in the group (persons), if they used a live bait and whether or not they brought a camper to the park (camper).

```python
df = pd.read_csv('https://stats.idre.ucla.edu/stat/data/fish.csv')
df.head()
```

|    |   nofish |   livebait |   camper |   persons |   child |        xb |        zg |   count |
|---:|---------:|-----------:|---------:|----------:|--------:|----------:|----------:|--------:|
|  0 |        1 |          0 |        0 |         1 |       0 | -0.896315 |  3.0504   |       0 |
|  1 |        0 |          1 |        1 |         1 |       0 | -0.558345 |  1.74615  |       0 |
|  2 |        0 |          1 |        0 |         1 |       0 | -0.401731 |  0.279939 |       0 |
|  3 |        0 |          1 |        1 |         2 |       1 | -0.956298 | -0.601526 |       0 |
|  4 |        0 |          1 |        0 |         1 |       0 |  0.436891 |  0.527709 |       1 |

We won't perform a regression, as we are only interested in assessing
the distribution of the fish number.

```python
sns.histplot(df['count'])
```

![The fish number distribution](/docs/assets/images/mixture/zero_inflated/fish.png)

The number of observed 0 looks much higher than the number of ones, so we will
use a zero-inflated negative binomial to fit the data.

```python
with pm.Model() as inflated_model:
    w = pm.Beta('w', alpha=1, beta=1)
    mu = pm.Exponential('mu', lam=0.1)
    alpha = pm.Exponential('alpha', lam=1)
    y = pm.ZeroInflatedNegativeBinomial('y', psi=w, mu=mu, alpha=alpha, observed=df['count'])
    trace_inflated = pm.sample(draws=5000, tune=5000, chains=4, random_seed=rng)
az.plot_trace(trace_inflated)
```

![The trace of our model](/docs/assets/images/mixture/zero_inflated/trace.png)

```pyhton
az.summary(trace_inflated)
```

|       |   mean |    sd |   hdi_3% |   hdi_97% |   mcse_mean |   mcse_sd |   ess_bulk |   ess_tail |   r_hat |
|:------|-------:|------:|---------:|----------:|------------:|----------:|-----------:|-----------:|--------:|
| w     |  0.898 | 0.079 |    0.748 |     1     |       0.002 |     0.002 |       1967 |        714 |       1 |
| mu    |  3.823 | 0.684 |    2.636 |     5.131 |       0.015 |     0.011 |       2719 |       1802 |       1 |
| alpha |  0.219 | 0.044 |    0.147 |     0.3   |       0.002 |     0.001 |       1417 |        599 |       1 |

Our trace doesn't show any issue, so we can check if our model is able to reproduce
the data.

```python
with inflated_model:
    ppc_inflated = pm.sample_posterior_predictive(trace_inflated, random_seed=rng)

fig = plt.figure()
x = np.arange(len(df['count']))
ax = fig.add_subplot(111)
ax.hist(ppc_inflated.posterior_predictive['y'].values.reshape(-1), density=True, bins=np.arange(30), label='PPC')
ax.hist(df['count'], density=True, bins=np.arange(50), alpha=0.5, label='data')
legend = plt.legend()
```


![The posterior predictive](/docs/assets/images/mixture/zero_inflated/ppc.png)

The model reproduces very accurately the observed data,
and as we can see the variable $w$ spans from 0.75 to 1, so it looks
appropriate to use a zero-inflated model rather than a pure negative binomial.
