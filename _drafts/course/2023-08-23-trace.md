---
layout: page
title: "Trace evaluation"
categories: course/intro/
tags: /trace_evaluation/
---

Up to now we limited ourselves to a visual assessment of the trace quality.
Here we will give some explicit example of some problem in the sampling
and we will provide some tool to investigate the issues in the sampling.
We will also give some recipes in order to improve the quality of our MCMC traces.

Up to now we simply used the PyMC sample function as

```python
with my_model:
   trace = pm.sample()
```

We did this for the sake of simplicity, but it is not the best way to proceed
when one deals with real applications. As we have seen, when writing a report,
one should always provide:

- the number of chains
- the number of warm-up steps
- the number of draws

In order to ensure the reproducibility of the results, one should also provide
the random seed of the sampler.

A better approach is

```python
import numpy as np

n_chains = 4
n_draws = 5000
n_tune = 5000

rng = np.random.default_rng(42)

with my_model:
   trace = pm.sample(draws=n_draws, tune=n_tune, chains=n_chains, random_seed=rng)
```

Gelman's suggestion is to use n_tune equal to n_draws, as this is
the best compromise between time and precision.

The choice of n_draws generally depends on the problem, but 
[Kruschke](https://www.nature.com/articles/s41562-021-01177-7)
recommends to choose it in such a way that the Effective Sample Size,
(which can be inferred by the arviz summary function) is at least 10000,
so to ensure the stability of the HDI estimate.

The default value of four is generally a good starting point for the chain number.
A smaller number would not make reliable the estimate for the $\hat{R}$ statistics,
that we will explain in this post.

In this post I tried and force PyMC to do its worst in the sampling.
This is quite hard, as the default NUTS sampler is very good, so I had to tune by hand the sampler in
some crazy (and obviously quite bad) ways.
So don't focus on the models or on the sampling options, but you should rather focus on the traces.

Let us first setup our environment

```python
import numpy as np
import pandas as pd
import seaborn as sns
import pymc as pm
import arviz as az
from matplotlib import pyplot as plt 
from pytensor.tensor.math import gammaln
from sklearn.datasets import load_iris
from itables import init_notebook_mode
import pymc.sampling_jax as pmj

rng = np.random.default_rng(1)

plt.style.use("seaborn-v0_8-darkgrid")

```

We can now proceed with the first model

```python
df_hurricanes = pd.read_csv('data/frequency-north-atlantic-hurricanes.csv')

y_obs = df_hurricanes["Number of US Hurricanes (HUDRAT, NOAA)"].dropna().values

with pm.Model() as model_hurricanes:
    mu = pm.Gamma('mu', alpha=1, beta=1/10)
    p = pm.Poisson("y", mu, observed=y_obs)
trace_hurricanes = pm.sample(draws=200, tune=500, chains=4, return_inferencedata=True,
                                  step=[pm.HamiltonianMC(adapt_step_size=False, step_scale=05,
                                  target_accept=0.99)], random_seed=rng)
az.plot_trace(trace_hurricanes)
```

![Bad trace one](/docs/assets/images/trace/trace_bad1.jpg)

In this case the issue is quite obvious:
the traces are simply stuck at the initial points.
This won't easily happen with the NUTS sampler, but if you are using any adaptive
sampler you should simply run more tuning draws or reduce the step.

Let us use the previous model to illustrate another possible problem:

```python
with model_hurricanes:
    trace_hurricanes0 = pm.sample(draws=200, tune=200, chains=4, 
                                  return_inferencedata=True, 
                                  step=[pm.HamiltonianMC(adapt_step_size=False, step_scale=0.1,
                                                         target_accept=0.99)], random_seed=rng)
az.plot_trace(trace_hurricanes0)
```

![Bad trace two](/docs/assets/images/trace/trace_bad2.jpg)

Here we clearly see four non-stationary chains. Also in this case you should run more tuning draws and likely more draws in general. Also the acceptance ratio is clearly too high, and we could reduce it (as a general recommendation 0.65 should be optimal).

This kind of issue is very easy to spot, but there are other kind of issue which
are less easy to spot by a simple visual inspection of the trace:

```python
with model_hurricanes:
    trace_hurricanes1 = pm.sample(draws=200, 
                                  tune=500, 
                                  chains=4, r
                                  eturn_inferencedata=True, 
                                  step=[pm.HamiltonianMC(adapt_step_size=False, step_scale=0.1,
                                                         target_accept=0.99)], random_seed=rng)
az.plot_trace(trace_hurricanes1)
```

![Bad trace two](/docs/assets/images/trace/trace_bad3.jpg)

Globally the traces may appear good, but by a more accurate inspection one
can see that, on a region of some percent of the entire sample,
the average is different from the global average.
This is synonym of non-negligible autocorrelation, which may lead to
a wrong variance estimate.
Let us see some useful tools to easily spot this.

First of all, we should look at the autocorrelation plot

```python
az.plot_autocorr(trace_hurricanes1)
```
![Bad trace two](/docs/assets/images/trace/acorr_bad3.jpg)

The grey band shows the estimate of the autocorrelation coefficients above the
maximum shown point, and it is quite large.
We can also look at the trace summary

```python
az.summary(trace_hurricanes1)
```

|    |   mean |    sd |   hdi_3% |   hdi_97% |   mcse_mean |   mcse_sd |   ess_bulk |   ess_tail |   r_hat |
|:---|-------:|------:|---------:|----------:|------------:|----------:|-----------:|-----------:|--------:|
| mu |  1.748 | 0.102 |    1.552 |     1.936 |       0.003 |     0.002 |       1418 |        814 |       1 |

The ESS is very small, and this should warn us.
This is even more clear by looking at the rank plot, where in a good trace
we would expect that all the bars show equal height

```python
az.plot_rank(trace_hurricanes1)
```

![Bad trace two](/docs/assets/images/trace/rank_bad3.jpg)

We can see that some bars are far away from the black dashed line,
and this indicates that trace is not reliable.

Let us look at some less trivial problem:

```python
df_a = pd.read_csv('https://raw.githubusercontent.com/Gajapathy-Selvaraj/Stock_Market_Datasets_NSE/main/NIFTY_50(INDEX)from2000.csv')
df_a['LogRet'] = np.log(df_a['Close']).diff()
y = 100*df_a['LogRet'].values[-100:]
from variance_gamma import VarianceGamma

with pm.Model() as vg0_model:
    r = pm.MvNormal('r',mu=np.array([0.0, 0.0]), cov=np.array([[1., 0.5], [0.5, 1]]))
    mu = r[0]
    theta = r[1]
    v = pm.Gamma('v', 2, 1)
    sigma = pm.Gamma('sigma',2,1)
    mean = pm.Deterministic('mean', mu+v*theta)
    variance = pm.Deterministic('variance', v**2*(sigma**2+2*theta**2))
    
    logret = VarianceGamma('logret',r=v, theta=theta,sigma=sigma,mu=mu,observed=y)
tr_vg = pmj.sample_numpyro_nuts(draws=300, tune=300, random_seed=rng)
az.plot_trace(tr_vg, var_names=['v', 'sigma'])
```

![Bad trace four](/docs/assets/images/trace/trace_bad4.jpg)

This is of course a terrible trace. We have sampled only 300 draws in order to make
it evident, but it remains also with a higher number of draws.
The main issue in this model is that the two plotted parameters are highly
correlated, as they equally contribute to the variance.
The best choice in this case is a re-parametrization of the model,
in such a way that one can disentangle the two parameters and allow the sampler
to easily do its job.
This may also happen when one has pathologies in the model
or when uses a too broad prior for a poorly constrained parameter.

```python
with pm.Model() as model:
    mu = pm.Cauchy('mu', alpha=0, beta=2)
    sigma = pm.HalfCauchy('sigma', beta=10)
    y = pm.Normal('y', mu=mu, sigma=sigma, observed = np.array([-1, 1, 2]))
    trace = pm.sample(random_seed=rng, draws=500, tune=500)
az.plot_trace(trace)
```

![Bad trace five](/docs/assets/images/trace/trace_bad6.jpg)

There is one point which lies far away from the other points,
so choosing a less generous prior would help in this case.

The last example that we will see is more tricky, as it is not a problem,
theoretically.

```python
iris = pd.read_csv('https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv')

rng1 = np.random.default_rng(41)

with pm.Model() as mix_model:
    sigma = pm.Exponential('sigma', lam=1)
    mu = pm.Normal('mu', mu=2*np.arange(3), sigma=1, shape=3)
    pi = pm.Dirichlet('pi', a=np.ones(shape=3)/3.0)
    phi = pm.Normal.dist(mu=mu, sigma=sigma, shape=3)
    y = pm.Mixture('y', w=pi, comp_dists = phi, observed=iris['petal.length'])
    tr_mix = pm.sample(tune=200, random_seed=rng1)
az.plot_trace(tr_mix)
```

![Bad trace six](/docs/assets/images/trace/trace_bad5.jpg)

In this very simple model the trace simply got stuck into a low probability region
for a while and then performed a jump to a different region.
This kind of problem is quite rare, but when it happens it may be hard to spot
unless you see the jump as in this case.

If you use many traces it may be easier to spot, and possible solutions
are re-parametrizations of your model, drawing longer samples or increasing
the acceptance ratio.
