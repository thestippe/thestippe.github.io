---
layout: page
title: "The Bayesian workflow and reporting guidelines"
categories: course/intro/
tags: /workflow/
---

In the following we will give an overview on the collection of procedures
and suggestions to improve the robustness and the reproducibility of your analysis.
This set of tools, which goes under the name of **Bayesian workflow**,
is extensively discussed in [this](https://arxiv.org/pdf/2011.01808.pdf)
preprint by Gelman _et al._ as well as in [this](https://www.nature.com/articles/s41562-021-01177-7) article on Nature.

Of course, we won't go as deep as the previously cited articles, but we will
give a practical overview of the main steps that you should always follow
in order to make sure that your conclusions are reliable and reproducible.

Many data scientists often get the data and just try to find out something out of it, and of course with enough data they will. However, the finding won't be reliable, as it is known that **if you torture long enough your data, it will confess to anything** (this is known as p-value hacking).

A better approach is to start by a **well defined question**, and this question should be stated even before collecting the data in order to avoid to look for some effect in the data rather than looking for a solution to our question.

Now many of you will rush to collect the data, but we can do better! In most cases someone had the same question and already came up with a meaningful solution. Don't waste time in reinventing the wheel, but dig into the literature, ask to other who may have had the same problem or an analogous one, and use this information as a starting point.

Then we can collect our data, by keeping in mind that **garbage in, garbage out**. In other words, our model will be at most as useful as our data.

Now it's finally time to write down our model. By doing this you should stuck to Occam's razor: in most cases the simplest solution is the best one.
We always want to start with the simplest model as possible, and we should add structure to it only in order to solve specific issues. In the software engineering language this is known as the KISS principle, and KISS doesn't refer to the glam rock band but means **Keep It Simple, Stupid!**
A simpler model will be likely more explainable, and this is a very important feature for use-cases, as it will allow us to think at the meaning of each parameter and eventually make some guess on how to add structure to the model by modifying it. Moreover, a model with few parameters is easier to debug than a model with hundreds of parameters.

Our model will contain priors, but in most cases we won't have enough field-specific knowledge in order to know a priori if our guess is good enough, so a very important step in the Bayesian workflow is to perform the **prior predictive check**. This is a very easy task to do and it won't be time consuming, but it allows us to check if the hyperparameters in our model are able to include our data.
In other words, if our model predicts the outcome variable $Y$ in the range $[-10, 10]$ in the 95% of the simulations but our true data are outside of this range than we should definitely change our hyperparameters.
As a rule of thumb, at least the 50% of the data should fall in the 50% highest density region of our prior predictive sample.
A useful procedure is to simulate some data based on our knowledge
(so before looking at the true data) and make sure
that our model is able to reproduce them.

Now it's finally time to draw our samples. Don't waste time by drawing large samples from the beginning, but run short samples when debugging your model and only once everything looks good you can draw the final sample (Nature reccomends at least ten thousands samples in order to have a sufficiently large one, and distributing those samples in 4 chains is usually enough). Gelman reccomends to only keep the last half of each of our sample, since it is the best compromise between time and precision.

After running the simulation, in order to check that everything is OK we should do the **trace evaluation**. A visual check is very important, but there are also other checks that we can and eventually should do in order to check that we did not ran into troubles and it's not necessary to increase the sample size or re-parametrize our model in order to make it more stable.

Once everything is good we can perform the **posterior predictive check**, and they will allow us to make sure that our model reproduces the salient features of our data. As previously stated, all models are wrong, so we won't be able to reproduce all of the features of the data, but we should be able to reproduce the relevant ones, where by relevant we mean with respect to our questions.

In most cases we won't be dealing with only one model, but we will be comparing more than one model to see which feature is best reproduced by each model.
This part of the flow is called **model comparison** or **model averaging** although in most cases we won't be really averaging over the models.

If our model looks good enough we can stuck here and use the informations that we extracted from our model till we get more data and are our model does not encode enough structure to reproduce the salient features of the new data. Otherwise we should go back to and adjust our model in order to be able to answer to our questions.

We should finally perform a **sensitivity analysis** and assess how our conclusions depends on the choice of the prior.

In the next posts we will dive deeper in how to perform in Python:
- trace evaluation
- prior and posterior check
- model comparison and averaging
