---
layout: page
title: "Notation"
categories: course/appendices/
tags: /notation/
---


I will adhere to Gelman's notation, and divide the quantities in _observable or potentially observable quantities_,
which will be indicated with Latin letters,
and in _unobservable quantities_, which will indicated with Greek letters.

The observable quantity that we are modelling will be usually indicated with the letter $y$
and it is called the _outcome variable_.

When doing regression we also have data that we are not interested in modelling.
These quantities, namely the _covariates_, _explanatory variables_ or _regressor variables_, will be indicated with the letter $x$
if we only refer to one variable, they will be otherwise indicated with $x^i$.
We will sometimes indicate $\mathbf{x}$ the vector of the covariates.


We will also follow Gelman's convention for the probability notation and indicate all the probability density functions
and probability mass functions with the letter $p\,,$ regardless if they indicate
a prior or a likelihood.

Thus, if we have no covariates, we will make inference by using the following form of the Bayes theorem:

$$
p(\theta \vert y) \propto p(y \vert \theta) p(\theta)
$$

where $p(y \vert \theta)$ is the _likelihood_, $p(\theta)$ is the _prior_ and $p(\theta \vert y)$ is the _posterior_.

Usually $y$ is made up by a set of observations, and each observation will be indicated with $y_i$.
If each observation is independent on the other observations we have that

$$p(y \vert \theta) = \prod_{i=1}^N p(y_i \vert \theta)\,.$$

Unobserved data will be indicated with $\tilde{y}$ 
The probability of some unobserved $\tilde{y}$ conditional to the observed data is called the _posterior predictive_ distribution,
and can be written as

$$ p(\tilde{y} \vert y) = \int d\theta p(\tilde{y} \vert \theta) p(\theta \vert y)\,. $$

On the other hand, the _prior predictive_ distribution is given by

$$ p(\tilde{y}) = \int d\theta p(\tilde{y} \vert \theta) p(\theta) \,.$$

Following Gelman, we indicate

$$
E[u] = \int du u p(u)
$$

and more generally

$$
E[f(u)] = \int du f(u) p(u)
$$

In particular

$$
var[u] = E[(u-E[u])^2] = \int du (u-E[u])^2 p(u)
$$
