<p>BART is a black box Bayesian method proposed in 2010 to approximate functions, and it can be useful when
you need to interpolate your data, but it is hard to figure out a transparent way to do so.
BART assumes</p>

\[Y \sim f(X) + \varepsilon\]

<p>where $\varepsilon$ is normally distributed, and</p>

\[f(X) = \sum_i g_i(X, T_i, M_i)\]

<p>Here $T_i$ represents a binary tree, and $M_i$ the set of means associated to $T_i$
In practice, a binary tree can be seen as a set of if-else, and an example is</p>

\[g_0 =
\begin{cases}
X &lt; c_1 &amp; \mu_1 \\
X \geq c_1 &amp; 
\begin{cases}
X &lt; c_2 &amp; \mu_2 \\
X \geq c_2 &amp; \mu_3 \\
\end{cases}
\\
\end{cases}\]

<p>Bart is a Bayesian method because both $T_i$ and $M_i$ are regularized by using priors.
For a more in-depth discussion about BARTs, you can take a look at 
<a href="https://arxiv.org/pdf/2206.03619">this preprint</a>
or at the <a href="https://www.pymc.io/projects/bart/en/latest/index.html">PyMC-BART homepage</a>.</p>

<h2 id="the-diamond-dataset">The diamond dataset</h2>

<p>We will use BART to fit the diamond dataset, which is dataset proposed
in <a href="https://www.tandfonline.com/doi/full/10.1080/10691898.2001.11910659">this article</a>
to show some of the main issues you will have to deal with when fitting
real-World datasets.
I strongly encourage you to read this article, as it is a very instructive example
of some of the issues most data scientist faced when working to real problems.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">pymc_bart</span> <span class="k">as</span> <span class="n">pmb</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Diamond.csv'</span><span class="p">)</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">rownames</th>
      <th style="text-align: right">carat</th>
      <th style="text-align: left">colour</th>
      <th style="text-align: left">clarity</th>
      <th style="text-align: left">certification</th>
      <th style="text-align: right">price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">0.3</td>
      <td style="text-align: left">D</td>
      <td style="text-align: left">VS2</td>
      <td style="text-align: left">GIA</td>
      <td style="text-align: right">1302</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">0.3</td>
      <td style="text-align: left">E</td>
      <td style="text-align: left">VS1</td>
      <td style="text-align: left">GIA</td>
      <td style="text-align: right">1510</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">3</td>
      <td style="text-align: right">0.3</td>
      <td style="text-align: left">G</td>
      <td style="text-align: left">VVS1</td>
      <td style="text-align: left">GIA</td>
      <td style="text-align: right">1510</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">4</td>
      <td style="text-align: right">0.3</td>
      <td style="text-align: left">G</td>
      <td style="text-align: left">VS1</td>
      <td style="text-align: left">GIA</td>
      <td style="text-align: right">1260</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">5</td>
      <td style="text-align: right">0.31</td>
      <td style="text-align: left">D</td>
      <td style="text-align: left">VS1</td>
      <td style="text-align: left">GIA</td>
      <td style="text-align: right">1641</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'carat'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'price'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bart/price.webp" alt="" /></p>

<p>As we can see, it appears that the relation between carat number and price
is non-linear, and the price also looks heteroscedastic with respect to the price.
We will use BART both the mean and the variance of a normal distribution.
First of all, let us convert the categorical variables into a meaningful way:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'colour'</span><span class="p">]).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span>
               <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'clarity'</span><span class="p">]).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span>
               <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'certification'</span><span class="p">]).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span>
               <span class="n">df</span><span class="p">[</span><span class="s">'carat'</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">yobs</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'price'</span><span class="p">]</span><span class="o">/</span><span class="mi">1000</span>
</code></pre></div></div>

<p>We also scaled the observations in order to simplify the work to the algorithms.
We can now implement the model as follows</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">coords</span><span class="o">=</span><span class="p">{</span><span class="s">'obs'</span><span class="p">:</span> <span class="n">X</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="s">'cols'</span><span class="p">:</span> <span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">})</span> <span class="k">as</span> <span class="n">model_carat</span><span class="p">:</span>
    <span class="n">Xv</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Data</span><span class="p">(</span><span class="s">'Xv'</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">pmb</span><span class="p">.</span><span class="n">BART</span><span class="p">(</span><span class="s">"w"</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">Xv</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">yobs</span><span class="p">),</span> <span class="n">m</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">yobs</span><span class="p">)))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">"y"</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">sigma</span><span class="o">=</span><span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">observed</span><span class="o">=</span><span class="n">yobs</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_carat</span><span class="p">:</span>
    <span class="n">idata</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bart/trace.webp" alt="The trace of the BART model" /></p>

<p>It is really hard to verify if there is any numerical issue with the sampling.
It is in fact generally recommended to only use it for the non-BART part of the
model, which is absent here.
PyMC-BART comes in fact with its own routines for the convergence assessment.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pmb</span><span class="p">.</span><span class="n">plot_convergence</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">var_name</span><span class="o">=</span><span class="s">'w'</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bart/pmb_trace.webp" alt="The trace of the BART model
using PyMC-BART" /></p>

<p>The curves in the left-hand plot are entirely above the dashed line,
while the ones in the right-hand figure are mostly below the corresponding
dashed line, and this tells us that our computation can be considered as reliable.</p>

<p>Notice that we haven’t used numpyro as usual, as we cannot use it together
with PyMC-BART.
This is however not a problem, since PyMC is fast enough.</p>

<p>We can now inspect the posterior predictive distribution</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">posterior_mean</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">"w"</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s">"chain"</span><span class="p">,</span> <span class="s">"draw"</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">w_hdi</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">hdi</span><span class="p">(</span><span class="n">ary</span><span class="o">=</span><span class="n">idata</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s">"posterior"</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">"w"</span><span class="p">],</span> <span class="n">hdi_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="k">with</span> <span class="n">model_carat</span><span class="p">:</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idata</span><span class="p">)</span>

<span class="n">pps</span> <span class="o">=</span> <span class="n">az</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span>
    <span class="n">ppc</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s">"posterior_predictive"</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">"y"</span><span class="p">]</span>
<span class="p">).</span><span class="n">T</span>

<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">Xv</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_hdi</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'carat'</span><span class="p">],</span>
    <span class="n">y</span><span class="o">=</span><span class="n">pps</span><span class="p">.</span><span class="n">values</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="n">hdi_prob</span><span class="o">=</span><span class="mf">0.90</span><span class="p">,</span>
    <span class="n">fill_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s">"alpha"</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span> <span class="s">"label"</span><span class="p">:</span> <span class="sa">r</span><span class="s">"Observations $90\%$ HDI"</span><span class="p">},</span>
<span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'carat'</span><span class="p">],</span>
    <span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">posterior_mean</span><span class="p">.</span><span class="n">values</span><span class="p">),</span>
    <span class="n">marker</span><span class="o">=</span><span class="s">'x'</span>
<span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'carat'</span><span class="p">],</span> <span class="n">yobs</span><span class="p">)</span>
<span class="c1"># ax.plot(df["youtube"], df["sales"], "o", c="C0", label="Raw Data")
</span><span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">"upper left"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bart/ppc.webp" alt="" /></p>

<p>Except from few extreme cases, our model seems appropriate to describe the observed price.
We can also assess the variable importance.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">pmb</span><span class="p">.</span><span class="n">plot_variable_importance</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">"VI"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bart/variable_importance.webp" alt="The variable importance plot" /></p>

<p>We can finally visualize the marginal dependence of the model on the single variables</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pmb</span><span class="p">.</span><span class="n">plot_pdp</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">yobs</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="n">grid</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
            <span class="n">var_discrete</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">14</span><span class="p">)))</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bart/plot_pdb.webp" alt="The marginal dependence plot" /></p>

<h2 id="conclusions">Conclusions</h2>
<p>We introduced BARTs, and we showed how to use them in PyMC by applying them
to the diamonds dataset.</p>

<h2 id="suggested-readings">Suggested readings</h2>
<ul>
  <li><cite>Quiroga, M., Garay, P.G., Alonso, J.M., Loyola, J.M., &amp; Martin, O.A. (2022). Bayesian additive regression trees for probabilistic programming.</cite></li>
  <li><cite>Chu, Singfat. (2001). Pricing the C’s of Diamond Stones. Journal of Statistics Education. 9. 10.1080/10691898.2001.11910659. </cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span>
</code></pre></div></div>

<div class="code">
Last updated: Wed Aug 21 2024
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.4
<br />
IPython version      : 8.24.0
<br />

<br />
xarray: 2024.5.0
<br />

<br />
arviz     : 0.18.0
<br />
numpy     : 1.26.4
<br />
pandas    : 2.2.2
<br />
pymc      : 5.15.0
<br />
seaborn   : 0.13.2
<br />
matplotlib: 3.9.0
<br />
pymc_bart : 0.5.14
<br />

<br />
Watermark: 2.4.3
<br />
</div>
