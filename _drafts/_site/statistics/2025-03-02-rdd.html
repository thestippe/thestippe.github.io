<p>Regression Discontinuity Design (RDD) can be applied when there is a threshold
above which some causal effect applies, and allows you to infer the impact of such an effect
on your population.
More precisely, you can determine the average treatment effect
on a neighborhood of the threshold.
In most countries, there is a retirement age, and you might analyze the impact of the
retirement on your lifestyle.
There are also countries where school classes has a maximum number of students,
and this has been used to assess the impact of the number of students on the students’ performances.
Here we will re-analyze, in a Bayesian way, the impact of alcohol on the mortality, as done in “Mastering Metrics”.
In the US, at 21, you are legally allowed to drink alcohol,
and we will use RDD to assess the impact on this on the probability of death in the US.</p>

<h2 id="implementation">Implementation</h2>

<p>Let us first of all take a look at the dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">bambi</span> <span class="k">as</span> <span class="n">pmb</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s">'nuts_sampler'</span><span class="p">:</span> <span class="s">'numpyro'</span><span class="p">,</span> <span class="s">'random_seed'</span><span class="p">:</span> <span class="n">rng</span><span class="p">,</span>
          <span class="s">'draws'</span><span class="p">:</span> <span class="mi">5000</span><span class="p">,</span> <span class="s">'tune'</span><span class="p">:</span> <span class="mi">5000</span><span class="p">,</span> <span class="s">'chains'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s">'target_accept'</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">}</span>

<span class="n">df_madd</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"https://raw.githubusercontent.com/seramirezruiz/stats-ii-lab/master/Session%206/data/mlda.csv"</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_madd</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="s">'forcing'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'outcome'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/rdd/data.webp" alt="" /></p>

<p>A linear model seems appropriate, and it seems quite clear that there is a jump when
the forcing variable (age-21) is zero.</p>

<p>While RDD can be both applied with a sharp cutoff and a fuzzy one, we will
limit our discussion to the sharp one.
We will take a simple linear model, as <a href="https://stat.columbia.edu/~gelman/research/published/2018_gelman_jbes.pdf">polynomial models should be generally avoided in RDD models</a>
as they tend to introduce artifacts.</p>

\[y \sim \mathcal{N}( \alpha + \beta x + \gamma \theta(x), \sigma)\]

<p>Here $x$ is the age minus 21, while $\theta(x)$ is the Heaviside theta</p>

\[\theta(x)
=
\begin{cases}
0 &amp; x\leq0 \\
1 &amp; x &gt; 0\\
\end{cases}\]

<p>As usual, we will assume a non-informative prior for all the parameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_red</span> <span class="o">=</span> <span class="n">df_madd</span><span class="p">[[</span><span class="s">'forcing'</span><span class="p">,</span> <span class="s">'outcome'</span><span class="p">]]</span>

<span class="n">df_red</span><span class="p">[</span><span class="s">'discontinuity'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_red</span><span class="p">[</span><span class="s">'forcing'</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">pmb</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="s">'outcome ~ forcing + discontinuity'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df_red</span><span class="p">)</span>

<span class="n">idata</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata</span><span class="p">)</span>
<span class="n">fig_trace</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig_trace</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/rdd/trace.webp" alt="" /></p>

<p>The trace looks fine, and it is clear that the value of the discontinuity is quite large.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">az</span><span class="p">.</span><span class="n">plot_posterior</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'discontinuity'</span><span class="p">])</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="/docs/assets/images/statistics/rdd/effect.webp" alt="" /></p>

<p>Let us now verify if our model is capable of reproducing the observed data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_pl</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
<span class="n">df_plot</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'forcing'</span><span class="p">:</span> <span class="n">x_pl</span><span class="p">,</span> <span class="s">'discontinuity'</span><span class="p">:</span> <span class="p">(</span><span class="n">x_pl</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)})</span>

<span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">idata</span><span class="o">=</span><span class="n">idata</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df_plot</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s">'response'</span><span class="p">)</span>

<span class="n">pp_madd</span> <span class="o">=</span> <span class="n">idata</span><span class="p">.</span><span class="n">posterior_predictive</span><span class="p">.</span><span class="n">outcome</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_pl</span><span class="p">)))</span>

<span class="n">madd_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pp_madd</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">az</span><span class="p">.</span><span class="n">plot_hdi</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">pp_madd</span><span class="p">,</span> <span class="n">smooth</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">hdi_prob</span><span class="o">=</span><span class="mf">0.94</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_red</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="s">'forcing'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'outcome'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pl</span><span class="p">,</span> <span class="n">madd_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/rdd/posterior_predictive.webp" alt="" /></p>

<h2 id="conclusions">Conclusions</h2>
<p>We re-analyzed the effect of the Minimum Legal Driving Age (MLDA)
on the mortality, and we discussed how to apply RDD to perform causal inference
in the presence of a threshold.</p>

<p>Before concluding, we would like to warn the reader that applying the
RDD design to time series might look appealing, but it’s rarely a good idea.
We won’t give you the details for this, and the interested reader
is invited to go through <a href="https://www.annualreviews.org/docserver/fulltext/resource/10/1/annurev-resource-121517-033306.pdf?expires=1743703371&amp;id=id&amp;accname=guest&amp;checksum=EF04497E18FC61428E3DD48DC29B58DD">this paper by Hausman and Rapson</a>
and references therein.</p>

<h2 id="suggested-readings">Suggested readings</h2>

<ul>
  <li><cite>Imbens, G. W., Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. US: Cambridge University Press.<cite></cite></cite></li>
  <li><cite><a href="https://arxiv.org/pdf/2206.15460.pdf">Li, Ding, Mealli (2022). Bayesian Causal Inference: A Critical Review</a></cite></li>
  <li><cite>Ding, P. (2024). A First Course in Causal Inference. CRC Press.</cite></li>
  <li><cite>Angrist, J. D., Pischke, J. (2014). Mastering ‘Metrics: The Path from Cause to Effect.   Princeton University Press.</cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">pytensor</span>
</code></pre></div></div>

<div class="code">
Last updated: Tue Mar 04 2025
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.8
<br />
IPython version      : 8.31.0
<br />

<br />
xarray  : 2024.11.0
<br />
pytensor: 2.26.4
<br />

<br />
pandas    : 2.2.3
<br />
bambi     : 0.15.0
<br />
seaborn   : 0.13.2
<br />
numpy     : 1.26.4
<br />
pymc      : 5.19.1
<br />
matplotlib: 3.10.0
<br />
arviz     : 0.20.0
<br />

<br />
Watermark: 2.5.0
<br />
</div>
