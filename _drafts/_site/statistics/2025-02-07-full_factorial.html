<p>In the <a href="/statistics/crd">previous post</a> we discussed how to perform
a completely randomized design experiment (CRD).
In a CRD there is only one factor of interest, and we generally want
to assess the effect of the different levels of that factor
on the outcome.</p>

<p>There are however situations when the factors of interest are more than
one. One common case is when you don’t know which factors
are relevant for the outcome, and your aim is to clarify which are
the factors that are relevant and which ones are irrelevant for the outcome.
Another common thing you might want to do is to find the optimal
setup for your experiment.</p>

<p>In a full factorial experiment, we have $k$ factors of interest,
and we run the experiment by combining all the levels of all the factors.
If we only perform one repetition, and assuming that we are
considering an equal number of levels $n$ for each factor,
then the number of runs is $n^k$.
This number grows a lot as soon as $n$ grows, so a common strategy
is to first fix $n=2$ for all the factor in order to discover
which are the relevant factors. Only then we either fix the irrelevant factors
or we randomize over them and increase the number of levels
of the relevant factors (if more than two levels are possible)
in order to optimize over them.
A more modern approach, which will be discussed in a future post,
is to dynamically look for the optimum value conditioning on the already
observed values by using the so-called Gaussian Process optimization,
but this is a rather advanced topic, and we will talk about this when
discussing non-parametric models.</p>

<p>Let us first look at how to run a $2^k$ experiment, we will discuss how to generalize
for larger $n$ in a later post.</p>

<h2 id="a-pipeline-for-ml-optimization">A pipeline for ML optimization</h2>

<p>Let us assume that we want to discover which is the optimal set of parameters
to train a ML algorith with a given dataset.
Let us assume that the algorithm allows for a large number of parameters.
We will use a gradient  boosting regressor for our problem, and we will
take the Abalone dataset.
As before, we will perform multiple train-test split, but this time
we will not match over them.
In order to show how  to perform and analyze a full factorial example,
we will instead randomize over the train-test split.</p>

<p>We will take consider the following combinations of factors:</p>
<ul>
  <li>loss: absolute error vs Huber</li>
  <li>criterion: Friedman MSE vs squared error</li>
  <li>learning rate: 0.25 and 0.75</li>
  <li>max features: number of features or its square root.</li>
</ul>

<p>Each combination will be tested with 50 different random train-test split.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">ucimlrepo</span> <span class="kn">import</span> <span class="n">fetch_ucirepo</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span><span class="p">,</span> <span class="n">mean_absolute_error</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">fetch_ucirepo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">Xs</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">features</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">Xs</span>
<span class="n">dummies</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">Xs</span><span class="p">[</span><span class="s">'Sex'</span><span class="p">])</span>
<span class="n">X</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s">'Sex'</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">dummies</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">targets</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">ravel</span><span class="p">()</span>


<span class="n">loss_list</span> <span class="o">=</span> <span class="p">[</span><span class="s">'absolute_error'</span><span class="p">,</span> <span class="s">'huber'</span><span class="p">]</span>
<span class="n">criterion_list</span> <span class="o">=</span> <span class="p">[</span><span class="s">'friedman_mse'</span><span class="p">,</span> <span class="s">'squared_error'</span><span class="p">]</span>
<span class="n">learning_rate_list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">]</span>
<span class="n">max_features_list</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="s">'sqrt'</span><span class="p">]</span>

<span class="n">combs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">reps</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">l</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">loss</span> <span class="ow">in</span> <span class="n">loss_list</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">criterion</span> <span class="ow">in</span> <span class="n">criterion_list</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">max_features</span> <span class="ow">in</span> <span class="n">max_features_list</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">learning_rate</span> <span class="ow">in</span> <span class="n">learning_rate_list</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">rep</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">reps</span><span class="p">):</span>
                    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">100000</span><span class="p">)</span>
                    <span class="n">combs</span> <span class="o">+=</span> <span class="p">[{</span><span class="s">'loss'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>
                               <span class="s">'criterion'</span><span class="p">:</span> <span class="n">criterion</span><span class="p">,</span>
                               <span class="s">'learning_rate'</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">,</span>
                               <span class="s">'max_features'</span><span class="p">:</span> <span class="n">max_features</span><span class="p">,</span>
                               <span class="s">'state'</span><span class="p">:</span> <span class="n">state</span><span class="p">,</span> <span class="s">'rep'</span><span class="p">:</span> <span class="n">rep</span><span class="p">,</span> <span class="s">'idx'</span><span class="p">:</span> <span class="n">l</span><span class="p">}]</span>
                    <span class="n">l</span> <span class="o">+=</span> <span class="mi">1</span>


<span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">random</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">combs</span><span class="p">)</span>
<span class="k">for</span> <span class="n">comb</span> <span class="ow">in</span> <span class="n">combs</span><span class="p">:</span>

    <span class="n">Xtrain</span><span class="p">,</span> <span class="n">Xtest</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">ytest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">state</span><span class="p">)</span>
    <span class="n">regr</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">comb</span><span class="p">[</span><span class="s">'loss'</span><span class="p">],</span>
                                     <span class="n">criterion</span><span class="o">=</span><span class="n">comb</span><span class="p">[</span><span class="s">'criterion'</span><span class="p">],</span>
                                     <span class="n">learning_rate</span><span class="o">=</span><span class="n">comb</span><span class="p">[</span><span class="s">'learning_rate'</span><span class="p">],</span>
                                     <span class="n">max_features</span><span class="o">=</span><span class="n">comb</span><span class="p">[</span><span class="s">'max_features'</span><span class="p">])</span>
    <span class="n">regr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>
    <span class="n">ypred</span> <span class="o">=</span> <span class="n">regr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">ypred</span><span class="p">)</span>
    <span class="n">pars</span> <span class="o">=</span> <span class="n">comb</span>
    <span class="n">pars</span><span class="p">.</span><span class="n">update</span><span class="p">({</span><span class="s">'score'</span><span class="p">:</span> <span class="n">score</span><span class="p">,</span> <span class="s">'ord'</span><span class="p">:</span> <span class="n">k</span><span class="p">})</span>
    <span class="n">out</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">pars</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">combs</span><span class="p">),</span> <span class="n">k</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">combs</span><span class="p">),</span> <span class="n">score</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">.</span><span class="n">from_records</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="n">df</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">'full_factorial.csv'</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="experiment-analysis">Experiment analysis</h2>

<p>Let us first of all import all the relevant libraries and the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">bambi</span> <span class="k">as</span> <span class="n">bmb</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s">'nuts_sampler'</span><span class="p">:</span> <span class="s">'numpyro'</span><span class="p">,</span> <span class="s">'random_seed'</span><span class="p">:</span> <span class="n">rng</span><span class="p">,</span>
          <span class="s">'draws'</span><span class="p">:</span> <span class="mi">2000</span><span class="p">,</span> <span class="s">'tune'</span><span class="p">:</span> <span class="mi">2000</span><span class="p">,</span> <span class="s">'chains'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s">'target_accept'</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">}</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'full_factorial.csv'</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s">'max_features'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'max_features'</span><span class="p">].</span><span class="n">fillna</span><span class="p">(</span><span class="s">'features'</span><span class="p">)</span>

<span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">Unnamed: 0</th>
      <th style="text-align: left">loss</th>
      <th style="text-align: left">criterion</th>
      <th style="text-align: right">learning_rate</th>
      <th style="text-align: left">max_features</th>
      <th style="text-align: right">state</th>
      <th style="text-align: right">rep</th>
      <th style="text-align: right">idx</th>
      <th style="text-align: right">score</th>
      <th style="text-align: right">ord</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">0</td>
      <td style="text-align: left">absolute_error</td>
      <td style="text-align: left">squared_error</td>
      <td style="text-align: right">0.75</td>
      <td style="text-align: left">features</td>
      <td style="text-align: right">99122</td>
      <td style="text-align: right">0</td>
      <td style="text-align: right">250</td>
      <td style="text-align: right">1.60952</td>
      <td style="text-align: right">0</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: left">huber</td>
      <td style="text-align: left">friedman_mse</td>
      <td style="text-align: right">0.75</td>
      <td style="text-align: left">sqrt</td>
      <td style="text-align: right">71432</td>
      <td style="text-align: right">42</td>
      <td style="text-align: right">592</td>
      <td style="text-align: right">1.68774</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: left">huber</td>
      <td style="text-align: left">squared_error</td>
      <td style="text-align: right">0.25</td>
      <td style="text-align: left">features</td>
      <td style="text-align: right">44912</td>
      <td style="text-align: right">31</td>
      <td style="text-align: right">631</td>
      <td style="text-align: right">1.56034</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: right">3</td>
      <td style="text-align: left">huber</td>
      <td style="text-align: left">squared_error</td>
      <td style="text-align: right">0.25</td>
      <td style="text-align: left">features</td>
      <td style="text-align: right">92911</td>
      <td style="text-align: right">20</td>
      <td style="text-align: right">620</td>
      <td style="text-align: right">1.55886</td>
      <td style="text-align: right">3</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">4</td>
      <td style="text-align: left">absolute_error</td>
      <td style="text-align: left">squared_error</td>
      <td style="text-align: right">0.25</td>
      <td style="text-align: left">features</td>
      <td style="text-align: right">49787</td>
      <td style="text-align: right">40</td>
      <td style="text-align: right">240</td>
      <td style="text-align: right">1.57739</td>
      <td style="text-align: right">4</td>
    </tr>
  </tbody>
</table>

<p>Let us now prepare the data for the analysis.
In order to have an idea of what are the relevant factors, we will first
use a non-interacting model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_ni</span> <span class="o">=</span> <span class="n">bmb</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="s">'score ~ criterion + loss + max_features + learning_rate'</span><span class="p">,</span>
                     <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="p">[</span><span class="s">'criterion'</span><span class="p">,</span> <span class="s">'loss'</span><span class="p">,</span> <span class="s">'max_features'</span><span class="p">])</span>

<span class="n">idata_ni</span> <span class="o">=</span> <span class="n">model_ni</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_ni</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/full_factorial/trace_ni.webp" alt="The trace of the non-interacting model" /></p>

<p>We immediately see that the only significant factors in this model
are the loss and the learning rate.
We have enough information in order to consider all the possible
interaction terms, and this is what we will do.
Building the fully interacting model is immediate with Bambi</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_int</span> <span class="o">=</span> <span class="n">bmb</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="s">'score ~ criterion * loss * max_features * learning_rate'</span><span class="p">,</span>
                      <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="p">[</span><span class="s">'criterion'</span><span class="p">,</span> <span class="s">'loss'</span><span class="p">,</span> <span class="s">'max_features'</span><span class="p">],</span> <span class="p">)</span>

<span class="n">idata_int</span> <span class="o">=</span> <span class="n">model_int</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_int</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/full_factorial/trace.webp" alt="The trace of the fully interacting model" /></p>

<p>It is clear that, by only keeping the non-interacting terms,
we were missing a lot of information.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s">':'</span><span class="p">)</span>
<span class="n">az</span><span class="p">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s">'beta'</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">combined</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/full_factorial/forest.webp" alt="" /></p>

<p>Finding the optimal combination within the one in the above dataset can
be done as follows</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_pred</span> <span class="o">=</span> <span class="n">df_ni</span><span class="p">.</span><span class="n">drop_duplicates</span><span class="p">()</span>

<span class="n">model_int</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">idata_int</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df_pred</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s">"response_params"</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df_pred</span><span class="p">[</span><span class="s">'mean_absolute_error'</span><span class="p">]</span> <span class="o">=</span> <span class="n">idata_int</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'mu'</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">)).</span><span class="n">values</span>

<span class="n">df_pred</span><span class="p">[</span><span class="s">'mean_absolute_error_std'</span><span class="p">]</span> <span class="o">=</span> <span class="n">idata_int</span><span class="p">.</span><span class="n">posterior</span><span class="p">[</span><span class="s">'mu'</span><span class="p">].</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="s">'draw'</span><span class="p">,</span> <span class="s">'chain'</span><span class="p">)).</span><span class="n">values</span>

<span class="n">df_pred</span><span class="p">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s">'mean_absolute_error'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: left">loss</th>
      <th style="text-align: left">criterion</th>
      <th style="text-align: left">max_features</th>
      <th style="text-align: right">learning_rate</th>
      <th style="text-align: right">mean_absolute_error</th>
      <th style="text-align: right">mean_absolute_error_std</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: left">huber</td>
      <td style="text-align: left">squared_error</td>
      <td style="text-align: left">features</td>
      <td style="text-align: right">0.25</td>
      <td style="text-align: right">1.56031</td>
      <td style="text-align: right">0.00203882</td>
    </tr>
    <tr>
      <td style="text-align: right">5</td>
      <td style="text-align: left">huber</td>
      <td style="text-align: left">friedman_mse</td>
      <td style="text-align: left">features</td>
      <td style="text-align: right">0.25</td>
      <td style="text-align: right">1.56084</td>
      <td style="text-align: right">0.00204077</td>
    </tr>
    <tr>
      <td style="text-align: right">15</td>
      <td style="text-align: left">huber</td>
      <td style="text-align: left">friedman_mse</td>
      <td style="text-align: left">sqrt</td>
      <td style="text-align: right">0.25</td>
      <td style="text-align: right">1.5655</td>
      <td style="text-align: right">0.00206221</td>
    </tr>
    <tr>
      <td style="text-align: right">10</td>
      <td style="text-align: left">huber</td>
      <td style="text-align: left">squared_error</td>
      <td style="text-align: left">sqrt</td>
      <td style="text-align: right">0.25</td>
      <td style="text-align: right">1.56654</td>
      <td style="text-align: right">0.00206114</td>
    </tr>
    <tr>
      <td style="text-align: right">7</td>
      <td style="text-align: left">absolute_error</td>
      <td style="text-align: left">friedman_mse</td>
      <td style="text-align: left">features</td>
      <td style="text-align: right">0.25</td>
      <td style="text-align: right">1.57463</td>
      <td style="text-align: right">0.00205604</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: left">absolute_error</td>
      <td style="text-align: left">squared_error</td>
      <td style="text-align: left">features</td>
      <td style="text-align: right">0.25</td>
      <td style="text-align: right">1.57605</td>
      <td style="text-align: right">0.00204244</td>
    </tr>
    <tr>
      <td style="text-align: right">60</td>
      <td style="text-align: left">absolute_error</td>
      <td style="text-align: left">friedman_mse</td>
      <td style="text-align: left">sqrt</td>
      <td style="text-align: right">0.25</td>
      <td style="text-align: right">1.5846</td>
      <td style="text-align: right">0.00202452</td>
    </tr>
    <tr>
      <td style="text-align: right">30</td>
      <td style="text-align: left">absolute_error</td>
      <td style="text-align: left">squared_error</td>
      <td style="text-align: left">sqrt</td>
      <td style="text-align: right">0.25</td>
      <td style="text-align: right">1.58915</td>
      <td style="text-align: right">0.00201078</td>
    </tr>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: left">absolute_error</td>
      <td style="text-align: left">squared_error</td>
      <td style="text-align: left">features</td>
      <td style="text-align: right">0.75</td>
      <td style="text-align: right">1.61377</td>
      <td style="text-align: right">0.00205295</td>
    </tr>
    <tr>
      <td style="text-align: right">11</td>
      <td style="text-align: left">absolute_error</td>
      <td style="text-align: left">friedman_mse</td>
      <td style="text-align: left">features</td>
      <td style="text-align: right">0.75</td>
      <td style="text-align: right">1.615</td>
      <td style="text-align: right">0.00205967</td>
    </tr>
    <tr>
      <td style="text-align: right">36</td>
      <td style="text-align: left">absolute_error</td>
      <td style="text-align: left">squared_error</td>
      <td style="text-align: left">sqrt</td>
      <td style="text-align: right">0.75</td>
      <td style="text-align: right">1.63457</td>
      <td style="text-align: right">0.00204341</td>
    </tr>
    <tr>
      <td style="text-align: right">18</td>
      <td style="text-align: left">absolute_error</td>
      <td style="text-align: left">friedman_mse</td>
      <td style="text-align: left">sqrt</td>
      <td style="text-align: right">0.75</td>
      <td style="text-align: right">1.63491</td>
      <td style="text-align: right">0.00204379</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: left">huber</td>
      <td style="text-align: left">friedman_mse</td>
      <td style="text-align: left">sqrt</td>
      <td style="text-align: right">0.75</td>
      <td style="text-align: right">1.69584</td>
      <td style="text-align: right">0.00206499</td>
    </tr>
    <tr>
      <td style="text-align: right">14</td>
      <td style="text-align: left">huber</td>
      <td style="text-align: left">squared_error</td>
      <td style="text-align: left">sqrt</td>
      <td style="text-align: right">0.75</td>
      <td style="text-align: right">1.70833</td>
      <td style="text-align: right">0.00205859</td>
    </tr>
    <tr>
      <td style="text-align: right">6</td>
      <td style="text-align: left">huber</td>
      <td style="text-align: left">squared_error</td>
      <td style="text-align: left">features</td>
      <td style="text-align: right">0.75</td>
      <td style="text-align: right">1.74248</td>
      <td style="text-align: right">0.00204959</td>
    </tr>
    <tr>
      <td style="text-align: right">17</td>
      <td style="text-align: left">huber</td>
      <td style="text-align: left">friedman_mse</td>
      <td style="text-align: left">features</td>
      <td style="text-align: right">0.75</td>
      <td style="text-align: right">1.74938</td>
      <td style="text-align: right">0.00203594</td>
    </tr>
  </tbody>
</table>

<p>The choice of the criterion has no impact on the error, and we can be quite
sure that the optimal setup has the Huber loss, probably with
the “features” number of max_features, and a small learning rate.</p>

<p>We stress that the learning rate is 
a continuous variables, so we cannot only use two values to find the
minimum score.
We will discuss this kind of problem in a future post.</p>

<p>We can finally take a look at the average predictions</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">bmb</span><span class="p">.</span><span class="n">interpret</span><span class="p">.</span><span class="n">plot_predictions</span><span class="p">(</span><span class="n">idata</span><span class="o">=</span><span class="n">idata_int</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_int</span><span class="p">,</span> <span class="n">conditional</span><span class="o">=</span><span class="s">'learning_rate'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">set_xlim</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">])</span>

<span class="n">bmb</span><span class="p">.</span><span class="n">interpret</span><span class="p">.</span><span class="n">plot_predictions</span><span class="p">(</span><span class="n">idata</span><span class="o">=</span><span class="n">idata_int</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_int</span><span class="p">,</span> <span class="n">conditional</span><span class="o">=</span><span class="s">'criterion'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="n">bmb</span><span class="p">.</span><span class="n">interpret</span><span class="p">.</span><span class="n">plot_predictions</span><span class="p">(</span><span class="n">idata</span><span class="o">=</span><span class="n">idata_int</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_int</span><span class="p">,</span> <span class="n">conditional</span><span class="o">=</span><span class="s">'loss'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="n">bmb</span><span class="p">.</span><span class="n">interpret</span><span class="p">.</span><span class="n">plot_predictions</span><span class="p">(</span><span class="n">idata</span><span class="o">=</span><span class="n">idata_int</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_int</span><span class="p">,</span> <span class="n">conditional</span><span class="o">=</span><span class="s">'max_features'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/full_factorial/slopes.webp" alt="The plot of the average predictions" /></p>

<p>Notice that, from the above plot, it looks like the score of the predictions for the Huber loss
are higher than the ones obtained by using the absolute error.
This only happens once we integrate over the remaining factors,
while we are interested in conditioning over them, since we are looking
for the optimal value.
We can convince ourselves of the above statement by looking at
the following figure:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">bmb</span><span class="p">.</span><span class="n">interpret</span><span class="p">.</span><span class="n">plot_predictions</span><span class="p">(</span><span class="n">idata</span><span class="o">=</span><span class="n">idata_int</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_int</span><span class="p">,</span>
                               <span class="n">conditional</span><span class="o">=</span><span class="p">{</span><span class="s">'learning_rate'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">],</span>
                                            <span class="s">'loss'</span><span class="p">:</span> <span class="p">[</span><span class="s">'absolute_error'</span><span class="p">,</span> <span class="s">'huber'</span><span class="p">]},</span>
                              <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">])</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/full_factorial/cond_predictions.webp" alt="The conditional predictions" /></p>

<h2 id="conclusions">Conclusions</h2>

<p>We have seen how to run and analyze a full factorial experiment
with binary outcomes.</p>

<h2 id="suggested-readings">Suggested readings</h2>
<ul>
  <li><cite>Lawson, J. (2014). Design and Analysis of Experiments with R. US: CRC Press.</cite></li>
  <li><cite>Hinkelmann, K., Kempthorne, O. (2008). Design and Analysis of Experiments Set. UK: Wiley.</cite></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>

<div class="code">
Last updated: Fri Feb 28 2025
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.8
<br />
IPython version      : 8.31.0
<br />

<br />
xarray : 2024.11.0
<br />
numpyro: 0.16.1
<br />
jax    : 0.4.38
<br />
jaxlib : 0.4.38
<br />

<br />
pandas    : 2.2.3
<br />
numpy     : 1.26.4
<br />
pymc      : 5.19.1
<br />
matplotlib: 3.10.0
<br />
arviz     : 0.20.0
<br />
seaborn   : 0.13.2
<br />
bambi     : 0.15.0
<br />

<br />
Watermark: 2.5.0
<br />
</div>

