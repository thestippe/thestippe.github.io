<p>In the last post we introduced multilevel models, in this post
we will dig deeper and try and use them at their best.
They are really powerful tools, but as usual from a great power
comes great responsibility.
We will also see how to use <a href="https://bambinos.github.io/bambi/">Bambi</a> to easily implement them.</p>

<h2 id="introduction-to-bambi">Introduction to Bambi</h2>

<p>Building mixed-effects models can become quite messy as soon as the
number of variables grows, but fortunately Bambi can help us.
I am generally skeptical when it comes to use interfaces to do stuff,
since I prefer having the full control of the underlying model.
Bambi is however powerful enough to give us a lot of freedom
in implementing the model.</p>

<p>Let us start by taking a look at the <a href="http://www.bodowinter.com/tutorial/politeness_data.csv">pointless dataset</a>
from the <a href="https://cran.r-project.org/web/packages/lme4/index.html">LME4 R library</a>, as explained in 
<a href="https://bodowinter.com/tutorial/bw_LME_tutorial2.pdf">this very nice introduction by Bodo Winter</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="n">az</span>
<span class="kn">import</span> <span class="nn">bambi</span> <span class="k">as</span> <span class="n">bmb</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s">'nuts_sampler'</span><span class="p">:</span> <span class="s">'numpyro'</span><span class="p">,</span> <span class="s">'random_seed'</span><span class="p">:</span> <span class="n">rng</span><span class="p">,</span>
          <span class="s">'draws'</span><span class="p">:</span> <span class="mi">2000</span><span class="p">,</span> <span class="s">'tune'</span><span class="p">:</span> <span class="mi">2000</span><span class="p">,</span> <span class="s">'chains'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
          <span class="s">'target_accept'</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">}</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'http://www.bodowinter.com/tutorial/politeness_data.csv'</span><span class="p">)</span>

<span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: left">subject</th>
      <th style="text-align: left">gender</th>
      <th style="text-align: right">scenario</th>
      <th style="text-align: left">attitude</th>
      <th style="text-align: right">frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: left">F1</td>
      <td style="text-align: left">F</td>
      <td style="text-align: right">1</td>
      <td style="text-align: left">pol</td>
      <td style="text-align: right">213.3</td>
    </tr>
    <tr>
      <td style="text-align: right">1</td>
      <td style="text-align: left">F1</td>
      <td style="text-align: left">F</td>
      <td style="text-align: right">1</td>
      <td style="text-align: left">inf</td>
      <td style="text-align: right">204.5</td>
    </tr>
    <tr>
      <td style="text-align: right">2</td>
      <td style="text-align: left">F1</td>
      <td style="text-align: left">F</td>
      <td style="text-align: right">2</td>
      <td style="text-align: left">pol</td>
      <td style="text-align: right">285.1</td>
    </tr>
    <tr>
      <td style="text-align: right">3</td>
      <td style="text-align: left">F1</td>
      <td style="text-align: left">F</td>
      <td style="text-align: right">2</td>
      <td style="text-align: left">inf</td>
      <td style="text-align: right">259.7</td>
    </tr>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: left">F1</td>
      <td style="text-align: left">F</td>
      <td style="text-align: right">3</td>
      <td style="text-align: left">pol</td>
      <td style="text-align: right">203.9</td>
    </tr>
  </tbody>
</table>

<p>As extensively described in <a href="https://arxiv.org/pdf/1308.5499">this preprint</a>,
the dataset measures the frequency of different individuals, both males
and females, with different attitudes (polite) and in different
scenarios.
The aim of the study is to determine the dependence of the frequency on the
attitude.</p>

<p>Before starting, let us clean the dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="n">isna</span><span class="p">().</span><span class="nb">any</span><span class="p">()</span>
</code></pre></div></div>

<div class="code">
subject      False<br />
gender       False<br />
scenario     False<br />
attitude     False<br />
frequency     True<br />
dtype: bool<br />
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_clean</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>


<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">sns</span><span class="p">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">df_clean</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'frequency'</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'gender'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>

</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bambi/violin.webp" alt="" /></p>

<p>It is not surprising that the frequency depends on the gender,
but it might also depend on the individual as well as on
the context.</p>

<h2 id="using-bambi-to-fit-linear-models">Using Bambi to fit linear models</h2>

<p>All of our independent variables are categorical,
and we will specify this to Bambi.</p>

<p>With Bambi you must pass a model as follows</p>

\[y \sim model\]

<p>where mode is a string where the dependence on each variable
is specified.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">categoricals</span> <span class="o">=</span> <span class="p">[</span><span class="s">'subject'</span><span class="p">,</span> <span class="s">'gender'</span><span class="p">,</span> <span class="s">'scenario'</span><span class="p">,</span> <span class="s">'attitude'</span><span class="p">]</span>

<span class="n">model_start</span> <span class="o">=</span> <span class="n">bmb</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="s">'frequency ~ gender + attitude'</span><span class="p">,</span>
                        <span class="n">data</span><span class="o">=</span><span class="n">df_clean</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="n">categoricals</span><span class="p">)</span>

<span class="n">model_start</span>
</code></pre></div></div>

<div class="code">
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Formula:&nbsp;frequency&nbsp;~&nbsp;gender&nbsp;+&nbsp;attitude
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Family:&nbsp;gaussian
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Link:&nbsp;mu&nbsp;=&nbsp;identity
<br />
&nbsp;&nbsp;Observations:&nbsp;83
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Priors:&nbsp;
<br />
&nbsp;&nbsp;&nbsp;&nbsp;target&nbsp;=&nbsp;mu
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Common-level&nbsp;effects
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Intercept&nbsp;~&nbsp;Normal(mu:&nbsp;193.5819,&nbsp;sigma:&nbsp;279.8369)
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gender&nbsp;~&nbsp;Normal(mu:&nbsp;0.0,&nbsp;sigma:&nbsp;325.7469)
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attitude&nbsp;~&nbsp;Normal(mu:&nbsp;0.0,&nbsp;sigma:&nbsp;325.7469)
<br />
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Auxiliary&nbsp;parameters
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sigma&nbsp;~&nbsp;HalfStudentT(nu:&nbsp;4.0,&nbsp;sigma:&nbsp;65.1447)
<br />
------
<br />
* To see a plot of the priors call the .plot_priors() method.
<br />
* To see a summary or plot of the posterior pass the object returned by .fit() to az.summary() or az.plot_trace()
</div>

<p>We can now fit our model as follows.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">idata_start</span> <span class="o">=</span> <span class="n">model_start</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_start</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bambi/trace_start.webp" alt="The trace of the first model" /></p>

<p>The result of the fit method is simply the inference data,
exactly as the one returned by the sample method in PyMC,
and the arguments of the fit method are also the arguments
of the PyMC sample method.</p>

<p>Once you run the fit, you can also access the PyMC model
by using the backend method:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pm</span><span class="p">.</span><span class="n">model_to_graphviz</span><span class="p">(</span><span class="n">model_start</span><span class="p">.</span><span class="n">backend</span><span class="p">.</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bambi/model_start.webp" alt="" /></p>

<p>The above model for the frequency is equivalent to</p>

\[y = \alpha + \beta_g X_g + \beta_a X_a\]

<p>where $X_g$ and $X_a$ are the indicator functions,
and the priors are specified in the above model.</p>

<p>The intercept is automatically included,
but can be explicitly included in the model with the $1$
variable as follows.</p>

\[frequency \sim 1  + gender + attitude\]

<p>We can also remove the intercept with the $0$ variable.
In this way, the first level of the first variable
is not dropped. In other words, the regression variable
without the $0$ is the same as</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_g</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df_clean</span><span class="p">[</span><span class="s">'gender'</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>while with the $0$ the model becomes</p>

\[frequency \sim 0  + gender + attitude\]

<p>which is equivalent to</p>

\[y =  \beta_g X_g + \beta_a X_a\]

<p>where this time</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_g</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df_clean</span><span class="p">[</span><span class="s">'gender'</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p>The degrees of freedom of the two models are the same,
but they are differently distributed.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_factor_explicit</span> <span class="o">=</span> <span class="n">bmb</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="s">'frequency ~  0  + attitude + gender'</span><span class="p">,</span>
                                  <span class="n">data</span><span class="o">=</span><span class="n">df_clean</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="n">categoricals</span><span class="p">)</span>

<span class="n">idata_factor_explicit</span> <span class="o">=</span> <span class="n">model_factor_explicit</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_factor_explicit</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bambi/trace_factor_explicit.webp" alt="The trace of the model without intercept" /></p>

<p>We can easily add an interaction term. The first way to do so is
as follows</p>

\[frequency \sim  gender + attitude + gender : attitude\]

<p>A shorter notation to include the full interaction</p>

\[frequency \sim  gender * attitude\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_int</span> <span class="o">=</span> <span class="n">bmb</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="s">'frequency ~ gender * attitude'</span><span class="p">,</span>
                      <span class="n">data</span><span class="o">=</span><span class="n">df_clean</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="n">categoricals</span><span class="p">)</span>

<span class="n">idata_int</span> <span class="o">=</span> <span class="n">model_int</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_int</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bambi/trace_int.webp" alt="The trace of the interacting model" /></p>

<h2 id="how-to-specify-a-mixed-effects-model">How to specify a mixed-effects model</h2>

<p>In Bambi, one can specify the grouping by using the pipe operator.
As an example, our first model with an additional
subject-level random intercept can be written as</p>

\[frequency \sim gender + attitude + (1 | subject)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_random_intercept</span> <span class="o">=</span> <span class="n">bmb</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="s">'frequency ~ gender + attitude + (1 | subject)'</span><span class="p">,</span>
                                   <span class="n">data</span><span class="o">=</span><span class="n">df_clean</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="n">categoricals</span><span class="p">)</span>

<span class="n">idata_ri</span> <span class="o">=</span> <span class="n">model_random_intercept</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_ri</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bambi/trace_ri.webp" alt="The subject-level random intercept" /></p>

<p>In the above model, we are allowing for the base frequency do be subject-dependence,
but we are not allowing for the context dependence to do so.
This does not seem logical, we should therefore include
a random slope too, and we can do this as follows</p>

\[frequency \sim gender + attitude + (1 | subject) + (attitude | subject)\]

<p>As we have previously seen, the intercept is automatically included
by Bambi once the variable is specified, and this is true for the random factors.
The above model can be simplified as</p>

\[frequency \sim gender + attitude + (attitude | subject)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_random_intercept_and_slope</span> <span class="o">=</span> <span class="n">bmb</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="s">'frequency ~ gender + attitude + (attitude | subject)'</span><span class="p">,</span>
                                             <span class="n">data</span><span class="o">=</span><span class="n">df_clean</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="n">categoricals</span><span class="p">)</span> 

<span class="n">idata_rias</span> <span class="o">=</span> <span class="n">model_random_intercept_and_slope</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_rias</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bambi/trace_rias.webp" alt="The trace of the random slope and random intercept model" /></p>

<p>We can immediately generalize to more than one grouping factor.
Let us assume we also want to quantify the scenario-dependent part
of the data, we can do this as</p>

\[frequency ~ gender + attitude + (attitude | subject) + (attitude | scenario)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_scenario</span> <span class="o">=</span> <span class="n">bmb</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="s">'frequency ~ gender + attitude + (attitude | subject) + (attitude | scenario)'</span><span class="p">,</span>
                           <span class="n">data</span><span class="o">=</span><span class="n">df_clean</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="n">categoricals</span><span class="p">)</span>

<span class="n">idata_scenario</span> <span class="o">=</span> <span class="n">model_scenario</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">az</span><span class="p">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">idata_scenario</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/docs/assets/images/statistics/bambi/trace_scenario.webp" alt="The trace of the scenario-dependant model" /></p>

<h2 id="choosing-a-good-model">Choosing a good model</h2>

<p>We have seen that implementing a mixed-effects model
in Bambi is straightforward. Since we have so much freedom,
it doesn’t look easy to choose <em>which</em> model we should implement.
Which factors should be included as random and which ones should
be modelled as fixed?</p>

<p>Let us first ask ourselves <em>what is</em> a random factor, and the answer
directly comes from our theory.
The levels of a random factor has not been fixed in the phase
of data collection, but they have been sampled from a larger population.</p>

<p>If we consider our dataset, we have that we have infinitely many
possible scenarios, and only tree of them have been studied in our
experiment.
The same is true for the attitude: in the experiment
the researcher asked the participants to try and
stick to two attitudes, but it doesn’t make much sense
to say that one can speak with only two attitudes.
One can in fact experience a great variety of emotions,
and express all of them when he is speaking.</p>

<p>Things are more tricky when we speak about the gender, since in our
context we might be both be interested in the gender and, probably
even more, in the biological sex.
It makes in fact a lot of sense to imagine that there is a genetic
component in the frequency of our voice, so we might consider
replacing the gender with the sex.
I personally cannot easily think about the sex of a person as sampled from a larger population.
<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<p>By the above considerations, I might consider more appropriate to
model the gender column of our dataset as a fixed factor,
while in my opinion attitude and scenario are more appropriately
modeled as random factors.</p>

<p>That said, which level of randomness should we allow in our model?
There is quite a general agreement that we should allow for the largest
possible level of randomness our data allows for.
Half of <a href="https://moodle2.units.it/pluginfile.php/290155/mod_resource/content/1/Gelman%20A.%2C%20Hill%20J.%20-%20Data%20Analysis%20Using%20Regression%20and%20Multilevel%20Hierarchical%20Models.pdf">Gelman’s textbook</a> 
has been devoted to answer to this question, so I strongly recommend
you to take a look it.
Another interesting reading is <a href="https://www.sciencedirect.com/science/article/abs/pii/S0749596X12001180">Barr’s article</a>
where the authors analyze the effect of random factors in the context of hypothesis testing.
In both cases, the answer is that if it makes sense to include a random factor, you should do so.
Generally, if you allow for random slopes, you should also consider using random intercepts,
and if it makes sense to include them, you should do so.</p>

<p>There are of course circumstances where it doesn’t make sense to do so.
As an example, a pre-post experiment where the pre-test condition is fixed but the post-test
is random might be modelled as a fixed-intercept random-slope.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Mixed-effect models can be a powerful tool for a data scientist,
and Bambi can be powerful too when it comes to implement them.
We have seen some of the main features of Bambi,
and we briefly discussed what degree of randomness one should allow
for in a mixed-effect model.</p>

<p><br />
<br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">watermark</span>

<span class="o">%</span><span class="n">watermark</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">u</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">iv</span> <span class="o">-</span><span class="n">w</span> <span class="o">-</span><span class="n">p</span> <span class="n">xarray</span><span class="p">,</span><span class="n">pytensor</span><span class="p">,</span><span class="n">numpyro</span><span class="p">,</span><span class="n">jax</span><span class="p">,</span><span class="n">jaxlib</span>
</code></pre></div></div>

<div class="code">
Last updated: Wed Feb 26 2025
<br />

<br />
Python implementation: CPython
<br />
Python version       : 3.12.8
<br />
IPython version      : 8.31.0
<br />

<br />
xarray  : 2024.11.0
<br />
pytensor: 2.26.4
<br />
numpyro : 0.16.1
<br />
jax     : 0.4.38
<br />
jaxlib  : 0.4.38
<br />

<br />
pymc      : 5.19.1
<br />
arviz     : 0.20.0
<br />
seaborn   : 0.13.2
<br />
numpy     : 1.26.4
<br />
bambi     : 0.15.0
<br />
pandas    : 2.2.3
<br />
matplotlib: 3.10.0
<br />

<br />
Watermark: 2.5.0
<br />
</div>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Please consider this as an illustrative example, do not consider this as an opinion in psycholinguistics, as I am not an expert in this field. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
